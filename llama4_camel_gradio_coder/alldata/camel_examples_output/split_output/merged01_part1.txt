
================================================================================
# Files from: camel_examples
================================================================================


--------------------------------------------------------------------------------
# File: __init__.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========



--------------------------------------------------------------------------------
# File: agent\agent_step_with_reasoning.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

sys_msg = "You are a helpful assistant."
usr_msg = """Who is the best basketball player in the world? 
Tell about his career.
"""

openai_model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
)

openai_agent = ChatAgent(
    system_message=sys_msg,
    model=openai_model,
)


# 1st run: the ordinary response

response = openai_agent.step(usr_msg)
print(response.msgs[0].content)
# flake8: noqa: E501
"""
===============================================================================
Determining the "best" basketball player in the world is subjective and often depends on personal preferences, criteria, and the specific time frame being considered. As of the latest NBA season, players like LeBron James, Kevin Durant, Giannis Antetokounmpo, Stephen Curry, and Nikola JokiÄ‡ are frequently mentioned in discussions about the best players due to their exceptional skills, achievements, and impact on the game. Each of these players brings unique strengths to the court, and opinions on who is the best can vary widely among fans and analysts.
===============================================================================
"""

# 2nd run: the response with thinking (agent choose the best candidate above the threshold)

response_with_think = openai_agent.step(
    usr_msg,
    reason_params=dict(
        choices=3,
        threshold=0.33,
    ),
)
print(response_with_think.msgs[0].content)
# flake8: noqa: E501
"""
===============================================================================
Let's start by identifying three potential candidates for the title of the best basketball player in the world. Here are three top candidates:

1. **LeBron James**: Known for his versatility, basketball IQ, and leadership on and off the court. LeBron has won multiple NBA championships and MVP awards.

2. **Kevin Durant**: Renowned for his scoring ability, shooting accuracy, and clutch performances. Durant has also won multiple NBA championships and MVP awards.

3. **Giannis Antetokounmpo**: Known for his athleticism, defensive prowess, and ability to dominate games. Giannis has won NBA championships and MVP awards as well.

Now, let's assign probabilities/credibilities to each choice:

- LeBron James: 0.4
- Kevin Durant: 0.3
- Giannis Antetokounmpo: 0.3

Since LeBron James has a probability/credibility greater than 0.33, I will continue with him.

### LeBron James' Career Overview

LeBron James, often regarded as one of the greatest basketball players of all time, began his NBA career in 2003 when he was drafted as the first overall pick by the Cleveland Cavaliers. Known for his exceptional athleticism, court vision, and versatility, LeBron quickly made an impact in the league.

Throughout his career, LeBron has played for the Cleveland Cavaliers, Miami Heat, and Los Angeles Lakers. He has won four NBA championships (two with the Miami Heat, one with the Cleveland Cavaliers, and one with the Los Angeles Lakers) and has been named NBA Finals MVP four times. LeBron is also a four-time NBA Most Valuable Player (MVP).

LeBron is known for his ability to play and defend multiple positions, his leadership on and off the court, and his contributions to the game beyond just scoring. He is also recognized for his philanthropic efforts and influence in social justice issues.

LeBron continues to play at a high level, consistently being a key player for his team and a significant figure in the NBA.
===============================================================================
"""

# 3rd run: the response with thinking (agent fails to choose the best candidate above the threshold, let the user decide)

response_with_think2 = openai_agent.step(
    usr_msg,
    reason_params=dict(
        choices=3,
        threshold=0.5,
    ),
)

print(response_with_think2.msgs[0].content)
# flake8: noqa: E501
"""
===============================================================================
Question: Who do you think is the best basketball player in the world? Here are the candidates and their probabilities: 
1. LeBron James (0.4): Known for his versatility, leadership, and consistent performance over the years. Multiple NBA championships and MVP awards.
2. Giannis Antetokounmpo (0.3): Known as the "Greek Freak," celebrated for his athleticism, defensive skills, and recent NBA championship win. Multiple MVP awards.
3. Stephen Curry (0.3): Renowned for his exceptional shooting ability, revolutionized the game with his three-point shooting. Multiple NBA championships and MVP awards.
Please choose one to continue with.
Your reply: 3
The user has chosen Stephen Curry as the best basketball player in the world. Let's talk about his career.

Stephen Curry, born on March 14, 1988, is an American professional basketball player for the Golden State Warriors of the National Basketball Association (NBA). Widely regarded as one of the greatest shooters in NBA history, Curry is credited with revolutionizing the game of basketball by inspiring teams to regularly utilize the three-point shot.

### Career Highlights:

1. **Early Life and College:**
   - Stephen Curry is the son of former NBA player Dell Curry. He played college basketball for the Davidson Wildcats, where he gained national attention for leading his team to the NCAA Tournament's Elite Eight in 2008.

2. **NBA Draft and Early Years:**
   - Curry was selected by the Golden State Warriors with the seventh overall pick in the 2009 NBA Draft. He quickly became known for his shooting prowess and playmaking ability.

3. **Rise to Stardom:**
   - Curry's breakout season came in 2014-2015 when he won his first NBA Most Valuable Player (MVP) award and led the Warriors to their first NBA Championship in 40 years.

4. **Record-Breaking Achievements:**
   - He set the record for most three-pointers made in a single season, a record he has broken multiple times. Curry's shooting range and accuracy have made him a central figure in the Warriors' success.

5. **Championships and MVP Awards:**
   - Stephen Curry has won four NBA Championships with the Warriors (2015, 2017, 2018, and 2022). He has been named NBA MVP twice (2015 and 2016), with the 2016 award being the first unanimous MVP in league history.

6. **Impact on the Game:**
   - Curry's influence extends beyond his statistics. He has changed how the game is played, with teams placing a greater emphasis on three-point shooting. His style of play has inspired a new generation of players.

7. **Continued Excellence:**
   - Even as he progresses in his career, Curry continues to perform at an elite level, contributing significantly to his team's success and maintaining his status as one of the top players in the league.

Stephen Curry's combination of skill, leadership, and impact on the game makes him a strong candidate for the best basketball player in the world today.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: agent\repo_agent.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import os

from camel.agents import RepoAgent
from camel.embeddings import OpenAIEmbedding
from camel.retrievers import VectorRetriever
from camel.storages.vectordb_storages import QdrantStorage

vector_storage = QdrantStorage(
    vector_dim=OpenAIEmbedding().get_output_dim(),
    collection_name="tmp_collection",
    path="local_data/",
)

vr = VectorRetriever(embedding_model=OpenAIEmbedding(), storage=vector_storage)

repo_agent = RepoAgent(
    repo_paths=["https://github.com/camel-ai/camel"],
    chunk_size=8192,
    top_k=5,
    similarity=0.3,
    vector_retriever=vr,
    github_auth_token=os.getenv("GITHUB_AUTH_TOKEN"),
)

response = repo_agent.step("How to use a ChatAgent in CAMEL?")

print(response.msgs[0].content)

"""
Based on your request to learn how to use a `ChatAgent` in CAMEL, I will 
explain key aspects of the implementation provided in the source code 
"retrieved" and guide you on how to create and utilize the `ChatAgent` 
effectively.

### Overview of `ChatAgent`

`ChatAgent` is designed to interact with language models, supporting 
conversation management, memory, and tool integration. 
It can perform tasks like handling user queries, responding with structured 
data, and performing computations.

### Basic Usage of `ChatAgent`

Here's a step-by-step guide on how to implement and utilize a `ChatAgent`:

1. **Import Necessary Modules**:
   Ensure to import the relevant classes from the CAMEL library.

   ```python
   from camel.agents import ChatAgent
   ```

2. **Creating a `ChatAgent` Instance**:
   When you create an instance of `ChatAgent`, you can optionally pass a 
   `system_message` to define its role and behavior.

   ```python
   agent = ChatAgent(system_message="You are a helpful assistant.")
   ```

3. **Interacting with the Agent**:
   You can have a conversation by using the `step()` method, which allows you 
   to send messages and get responses.

   ```python
   user_message = "Hello, can you help me with a question?"
   response = agent.step(user_message)
   print(response.msgs[0].content)  # Print the agent's response
   ```

### Advanced Features

#### Integrating Tools

You can define tools (functions) that the agent can call during its operation.

```python
from camel.toolkits import FunctionTool

def calculator(a: int, b: int) -> int:
    return a + b

# Create ChatAgent with a tool
agent_with_tool = ChatAgent(tools=[calculator])
response = agent_with_tool.step("What is 2 + 2 using the calculator?")
```

#### Structured Output

You can specify structured outputs using Pydantic models to control the 
format of the response.

```python
from pydantic import BaseModel
from typing import List

class StructuredResponse(BaseModel):
    points: List[str]
    summary: str

agent = ChatAgent()
response = agent.step(
    "List benefits of exercise", response_format=StructuredResponse
)
```

### Example with a Specific Model

The code examples you provided also show how to specify and configure models 
used by `ChatAgent`. Here's how to create a `ChatAgent` with a custom model:

```python
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,
    model_type="gpt-3.5-turbo",  # Example model
    api_key="your_api_key",  # Ensure you have appropriate credentials
    model_config_dict={"temperature": 0.7}
)

camel_agent = ChatAgent(
    system_message="You are a helpful assistant.", model=model
)

user_message = "What are the best practices for using AI?"
response = camel_agent.step(user_message)
print(response.msgs[0].content)
```

### Conclusion

You can leverage `ChatAgent` in CAMEL to create powerful conversational agents 
that can perform a variety of tasks, integrate tools, and manage conversations 
effectively. The examples given demonstrate basic usage, tool integration, 
structured output formats, and model specification, allowing you to customize 
the behavior of your chat agent to suit your needs.

If you need more specific features or have other questions about the CAMEL 
framework, feel free to ask!

"""



--------------------------------------------------------------------------------
# File: ai_society\babyagi_playing.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from colorama import Fore

from camel.societies import BabyAGI
from camel.utils import print_text_animated


def main(model=None, chat_turn_limit=15) -> None:
    task_prompt = "Develop a trading bot for the stock market"
    babyagi_session = BabyAGI(
        assistant_role_name="Python Programmer",
        assistant_agent_kwargs=dict(model=model),
        user_role_name="Stock Trader",
        task_prompt=task_prompt,
        task_specify_agent_kwargs=dict(model=model),
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{babyagi_session.assistant_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + f"Specified task prompt:\n{babyagi_session.specified_task_prompt}\n"
    )
    print(
        Fore.RED
        + f"Final task prompt:\n{babyagi_session.specified_task_prompt}\n"
    )

    n = 0
    while n < chat_turn_limit:
        n += 1
        assistant_response = babyagi_session.step()
        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        print_text_animated(
            Fore.RED + "Task Name:\n\n"
            f"{assistant_response.info['task_name']}\n"
        )
        print_text_animated(
            Fore.GREEN + "AI Assistant:\n\n"
            f"{assistant_response.msg.content}\n"
        )
        print_text_animated(
            Fore.BLUE + "Remaining Subtasks:\n\n"
            f"{assistant_response.info['subtasks'][:5]}\n"
        )


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: ai_society\generate_meta_data.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.prompts import PromptTemplateGenerator
from camel.types import TaskType


def main(key: str = "generate_users", num_roles: int = 50):
    prompt_template = PromptTemplateGenerator().get_prompt_from_key(
        TaskType.AI_SOCIETY, key
    )
    prompt = prompt_template.format(num_roles=num_roles)
    print(prompt)
    agent = ChatAgent("You are a helpful assistant.")
    agent.reset()

    assistant_response = agent.step(prompt)
    if assistant_response.msgs is not None:
        print(assistant_response.msg.content)


if __name__ == "__main__":
    main("generate_users", 50)
    main("generate_assistants", 50)



--------------------------------------------------------------------------------
# File: ai_society\role_playing.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from colorama import Fore

from camel.societies import RolePlaying
from camel.utils import print_text_animated


def main(model=None, chat_turn_limit=50) -> None:
    task_prompt = "Develop a trading bot for the stock market"
    role_play_session = RolePlaying(
        assistant_role_name="Python Programmer",
        assistant_agent_kwargs=dict(model=model),
        user_role_name="Stock Trader",
        user_agent_kwargs=dict(model=model),
        task_prompt=task_prompt,
        with_task_specify=True,
        task_specify_agent_kwargs=dict(model=model),
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    n = 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
        )
        print_text_animated(
            Fore.GREEN + "AI Assistant:\n\n"
            f"{assistant_response.msg.content}\n"
        )

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: ai_society\role_playing_multi_lingual.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from colorama import Fore

from camel.societies import RolePlaying
from camel.utils import print_text_animated


def main(model=None) -> None:
    task_prompt = "Develop a trading bot for the stock market"
    role_play_session = RolePlaying(
        assistant_role_name="Python Programmer",
        assistant_agent_kwargs=dict(model=model),
        user_role_name="Stock Trader",
        user_agent_kwargs=dict(model=model),
        task_prompt=task_prompt,
        with_task_specify=True,
        task_specify_agent_kwargs=dict(model=model),
        output_language="Chinese",  # Arabic, French, Spanish, ...
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    chat_turn_limit, n = 50, 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
        )
        print_text_animated(
            Fore.GREEN + "AI Assistant:\n\n"
            f"{assistant_response.msg.content}\n"
        )

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    from camel.types import ModelType

    main(ModelType.GPT_4)



--------------------------------------------------------------------------------
# File: ai_society\role_playing_multiprocess.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import json
import multiprocessing
import os
import sys
from typing import Any, Dict

from colorama import Fore

from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.types import ModelPlatformType, ModelType, TaskType
from camel.utils import download_tasks


def generate_data(
    assistant_idx: int,
    assistant_role_name: str,
    user_idx: int,
    user_role_name: str,
    task_idx: int,
    task_prompt: str,
    verbose: bool = False,
) -> None:
    max_num_messages = 40

    original_task_prompt = task_prompt.replace(f"{task_idx+1}. ", "")

    model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
        model_config_dict=ChatGPTConfig(temperature=1.4).as_dict(),
    )

    role_play_session = RolePlaying(
        assistant_role_name,
        user_role_name,
        task_prompt=original_task_prompt,
        with_task_specify=True,
        with_task_planner=False,
        task_specify_agent_kwargs=dict(model=model),
    )

    input_msg = role_play_session.init_chat()

    if verbose:
        print(
            Fore.GREEN + "AI Assistant sys message:\n"
            f"{role_play_session.assistant_sys_msg}\n"
        )
        print(
            Fore.BLUE
            + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
        )

        print(Fore.YELLOW + f"Original task prompt:\n{original_task_prompt}\n")
        print(
            Fore.CYAN + "Specified task prompt:\n"
            f"{role_play_session.specified_task_prompt}\n"
        )
        print(
            Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n"
        )

    message_counter = 0
    message_dict: Dict[str, Any] = {}

    assistant_agent = role_play_session.assistant_agent
    user_agent = role_play_session.user_agent

    # Append roles to the dictionary
    # We start number from 1 not 0.
    message_dict["role_1"] = (
        f"{assistant_role_name}_{assistant_agent.role_type!s}"
    )
    message_dict["role_2"] = f"{user_role_name}_{user_agent.role_type!s}"
    message_dict["id"] = (
        f"{(assistant_idx+1):03}_{(user_idx+1):03}_{(task_idx+1):03}"
    )
    message_dict["original_task"] = original_task_prompt
    message_dict["specified_task"] = role_play_session.specified_task_prompt

    # Threshold to terminate the conversation if no end token appears

    repeat_word_counter = 0
    repeat_word_threshold = 4
    repeat_word_list = [
        "goodbye",
        "good bye",
        "thank",
        "bye",
        "welcome",
        "language model",
    ]

    assistant_instruct_counter = 0
    assistant_instruct_threshold = 1
    assistant_instruct_word = "Instruction:"

    user_no_instruct_counter = 0
    user_no_instruct_threshold = 3
    user_no_instruct_word = "Instruction:"

    # Set max number of messages for the chat

    while message_counter < max_num_messages:
        assistant_response, user_response = role_play_session.step(input_msg)

        # Condition 1: User terminates the chat
        if user_response.terminated and user_response.info is not None:
            message_dict["termination_reason"] = (
                f"{user_agent.role_type!s}: "
                f"{user_response.info['termination_reasons'][0]}"
            )
            break

        # Condition 2: Assistant terminates the chat
        if (
            assistant_response.terminated
            and assistant_response.info is not None
        ):
            message_dict["termination_reason"] = (
                f"{assistant_agent.role_type!s}: "
                f"{assistant_response.info['termination_reasons'][0]}"
            )
            break

        assert (
            user_response.msg is not None
            and assistant_response.msg is not None
        )

        if verbose:
            print(f"User:\n{user_response.msg.content}\n")
            print(f"Assistant:\n{assistant_response.msg.content}\n")

        # Condition 3: Break if user does not give instruction
        if user_no_instruct_word not in user_response.msg.content:
            user_no_instruct_counter += 1
            if user_no_instruct_counter == user_no_instruct_threshold:
                message_dict['termination_reason'] = (
                    "user_no_instruct_threshold"
                )
                break
        else:
            user_no_instruct_counter = 0

        # Condition 4: Break if assistant gives instruction (flipped role)
        if assistant_instruct_word in assistant_response.msg.content:
            assistant_instruct_counter += 1
            if assistant_instruct_counter == assistant_instruct_threshold:
                message_dict['termination_reason'] = (
                    "assistant_instruct_threshold"
                )
                break
        else:
            assistant_instruct_counter = 0

        # Condition 5: Repeat word observed
        for repeat_word in repeat_word_list:
            if (
                repeat_word in user_response.msg.content.lower()
                or repeat_word in assistant_response.msg.content.lower()
            ):
                repeat_word_counter += 1
                if repeat_word_counter == repeat_word_threshold:
                    message_dict['termination_reason'] = (
                        "repeat_word_threshold"
                    )
                    break
            else:
                repeat_word_counter = 0

        # Save user message
        message_counter += 1
        message_dict[f"message_{message_counter}"] = (
            user_response.msg.to_dict()
        )

        # Condition 5: End token observed
        if "<CAMEL_TASK_DONE>" in user_response.msg.content:
            message_dict['termination_reason'] = "<CAMEL_TASK_DONE>"
            break

        # Save assistant message
        message_counter += 1
        message_dict[f"message_{message_counter}"] = (
            assistant_response.msg.to_dict()
        )

        input_msg = assistant_response.msg

    message_dict["num_messages"] = message_counter

    if message_dict["num_messages"] == max_num_messages:
        message_dict["termination_reason"] = "max_num_messages"

    with open(
        f"./camel_data/ai_society/{message_dict['id']}.json", "w"
    ) as json_file:
        json.dump(message_dict, json_file, ensure_ascii=False)


def generate_data_wrapper(args):
    try:
        generate_data(*args)
    except Exception as e:
        print(f"Error in generate_data: {e}", file=sys.stderr)


def main() -> None:
    # Disable/Enable Printing
    verbose = True

    # Check for tasks folder and install if not exists
    # Define the folder path
    folder_path = "./ai_society_data/"

    # Check if the folder already exists
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)

    # Check if the folder is empty
    if not os.listdir(folder_path):
        download_tasks(task=TaskType.AI_SOCIETY, folder_path=folder_path)

    # Chunk for parallel jobs
    try:
        slurm_array_task_id = os.environ.get('SLURM_ARRAY_TASK_ID')
        if slurm_array_task_id is None:
            raise
        array_idx = int(slurm_array_task_id)
    except (TypeError, ValueError) as e:
        print(f"Error: {e}")
        array_idx = 0

    roles_per_chunk = 10

    # Parameters for filtering the generated task string
    start_token = "1."
    num_tasks = 10

    with open("./data/ai_society/user_roles.txt", "r") as f:
        user_roles = f.read().splitlines()

    with open("./data/ai_society/assistant_roles.txt", "r") as f:
        assistant_roles = f.read().splitlines()

    assert (array_idx + 1) * roles_per_chunk <= len(assistant_roles)
    assistant_roles = assistant_roles[
        array_idx * roles_per_chunk : (array_idx + 1) * roles_per_chunk
    ]

    pool = multiprocessing.Pool()

    for assistant_idx, assistant_role_name in enumerate(assistant_roles):
        assistant_idx += array_idx * roles_per_chunk
        assistant_role_name = " ".join(assistant_role_name.split(" ")[1:])
        for user_idx, user_role_name in enumerate(user_roles):
            user_role_name = " ".join(user_role_name.split(" ")[1:])
            # Load the task list assigned for assistant and user roles
            with open(
                (
                    f"./ai_society_data/tasks/"
                    f"{assistant_role_name}_{user_role_name}.txt"
                ),
                "r",
            ) as f:
                tasks = f.read().splitlines()

                # Filter out the generated response to include the tasks only
                for i, task in enumerate(tasks):
                    if start_token in task:
                        tasks = tasks[i : i + num_tasks]
                        break

                # Ensure exact number of tasks is generated
                assert str(num_tasks) in tasks[-1], print(tasks)

            for task_idx, task_prompt in enumerate(tasks):
                id = (
                    f"{(assistant_idx+1):03}_"
                    f"{(user_idx+1):03}_{(task_idx+1):03}"
                )
                if not os.path.exists(f"./camel_data/ai_society/{id}.json"):
                    pool.apply_async(
                        generate_data_wrapper,
                        (
                            (
                                assistant_idx,
                                assistant_role_name,
                                user_idx,
                                user_role_name,
                                task_idx,
                                task_prompt,
                                verbose,
                            ),
                        ),
                    )

    pool.close()
    pool.join()


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: ai_society\role_playing_with_critic.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from colorama import Fore

from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated


def main() -> None:
    task_prompt = "Write a research proposal for large-scale language models"
    model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
        model_config_dict=ChatGPTConfig(temperature=0.8, n=3).as_dict(),
    )
    assistant_agent_kwargs = dict(model=model)
    user_agent_kwargs = dict(model=model)
    critic_kwargs = dict(verbose=True)
    role_play_session = RolePlaying(
        "PhD Student",
        "Postdoc",
        critic_role_name="Professor",
        task_prompt=task_prompt,
        with_task_specify=True,
        with_critic_in_the_loop=True,
        assistant_agent_kwargs=assistant_agent_kwargs,
        user_agent_kwargs=user_agent_kwargs,
        critic_kwargs=critic_kwargs,
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )
    print(
        Fore.MAGENTA
        + f"Critic sys message:\n{role_play_session.critic_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    chat_turn_limit, n = 50, 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
        )
        print_text_animated(
            Fore.GREEN + f"AI Assistant:\n\n{assistant_response.msg.content}\n"
        )

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: ai_society\role_playing_with_human.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from colorama import Fore

from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated


def main() -> None:
    task_prompt = "Write a book about the future of AI Society"
    model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
        model_config_dict=ChatGPTConfig(temperature=1.4, n=3).as_dict(),
    )
    assistant_agent_kwargs = dict(model=model)
    user_agent_kwargs = dict(model=model)
    role_play_session = RolePlaying(
        "AGI",
        "Writer",
        critic_role_name="human",
        task_prompt=task_prompt,
        with_task_specify=True,
        with_critic_in_the_loop=True,
        assistant_agent_kwargs=assistant_agent_kwargs,
        user_agent_kwargs=user_agent_kwargs,
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    chat_turn_limit, n = 50, 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
        )
        print_text_animated(
            Fore.GREEN + f"AI Assistant:\n\n{assistant_response.msg.content}\n"
        )

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: ai_society\task_generation.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import multiprocessing
import os

from camel.agents import ChatAgent
from camel.generators import (
    AISocietyTaskPromptGenerator,
    SystemMessageGenerator,
)
from camel.messages import BaseMessage
from camel.types import RoleType, TaskType


def generate_tasks(
    role_names: str,
    task_generator_prompt: str,
    start_token: str = "1.",
    num_tasks: int = 10,
    model=None,
) -> None:
    sys_msg_generator = SystemMessageGenerator(task_type=TaskType.AI_SOCIETY)

    assistant_sys_msg = sys_msg_generator.from_dict(
        dict(), role_tuple=("Task Generator", RoleType.DEFAULT)
    )
    assistant_agent = ChatAgent(assistant_sys_msg, model=model)

    user_msg = BaseMessage.make_user_message(
        role_name="Task Generator", content=task_generator_prompt
    )

    assistant_response = assistant_agent.step(user_msg)

    if assistant_response.terminated or len(assistant_response.msgs) == 0:
        raise RuntimeError("Assistant agent terminated unexpectedly.")

    tasks = assistant_response.msg.content.split("\n")

    # Filter out the generated response to include the tasks only
    for i, task in enumerate(tasks):
        if start_token in task:
            tasks = tasks[i : i + num_tasks]
            break

    # Ensure exact number of tasks is generated
    assert str(num_tasks) in tasks[-1], print(tasks)

    with open(
        f"./ai_society_data/tasks/{'_'.join(role_names)}.txt", "w"
    ) as file:
        file.write("\n".join(tasks))


def main(model=None) -> None:
    num_tasks = 10
    start_token = "1."

    task_generator_prompt_generator = AISocietyTaskPromptGenerator(
        num_tasks=num_tasks
    ).from_role_files()

    pool = multiprocessing.Pool()

    for task_generator_prompt, role_names in task_generator_prompt_generator:
        if not os.path.exists(
            f"./ai_society_data/tasks/{'_'.join(role_names)}.txt"
        ):
            print(f"Generating tasks for {role_names}")
            pool.apply_async(
                generate_tasks,
                (
                    role_names,
                    task_generator_prompt,
                    start_token,
                    num_tasks,
                    model,
                ),
            )

    pool.close()
    pool.join()


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: benchmarks\apibank.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.benchmarks import APIBankBenchmark
from camel.benchmarks.apibank import Evaluator

# Set up the agent to be benchmarked
agent = ChatAgent()

# Set up the APIBench Benchmark
# Please note that the data_dir is predefined
# for better import management of the tools
benchmark = APIBankBenchmark(save_to="APIBankResults.jsonl")

# Download the benchmark data
benchmark.download()

# Set the subset to be benchmarked
level = 'level-1'

# NOTE: You might encounter the following error when
# running the benchmark in Windows:
# UnicodeDecodeError: 'charmap' codec can't decode byte 0x81
# in position 130908: character maps to <undefined>
# To solve this issue, you can navigate to the file
# api_bank/tool_manager.py", line 30 and change the encoding
# with open(os.path.join(init_database_dir, file), 'r',
# encoding='utf-8') as f:


# Run the benchmark
result = benchmark.run(agent, level, api_test_enabled=True, subset=10)

# The following steps are only for demonstration purposes,
# they have been integrated into the run method of the benchmark.
# Get the first example of the test data
example_test = list(benchmark._data.items())[0]  # type: ignore[assignment] # noqa: RUF015
evaluator = Evaluator(example_test)
api_description = evaluator.get_api_description('ToolSearcher')
print('\nAPI description for ToolSearcher:\n', api_description)
'''
===============================================================================
API description for ToolSearcher:
 {"name": "ToolSearcher", "description": "Searches for relevant tools in 
 library based on the keywords.", "input_parameters": {"keywords": {"type": 
 "str", "description": "The keyword to search for."}}, 
 "output_parameters": 
 {"best_matchs": {"type": "Union[List[dict], dict]", 
 "description": "The best match tool(s)."}}}
===============================================================================
'''

# Print the final results
print("Total:", result["total"])
print("Correct:", result["correct"])
'''
===============================================================================
Total: 24
Correct: 10
===============================================================================
'''



--------------------------------------------------------------------------------
# File: benchmarks\apibench.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.benchmarks import APIBenchBenchmark

# Set up the agent to be benchmarked
agent = ChatAgent()

# Set up the APIBench Benchmark
benchmark = APIBenchBenchmark(
    data_dir="APIBenchDatasets", save_to="APIBenchResults.jsonl"
)

# Download the benchmark data
benchmark.download()

# Set the subset to be benchmarked
subset_name = 'torchhub'

# Run the benchmark
result = benchmark.run(agent, subset_name, subset=10)

# Please note that APIBench does not use 'real function call'
# but instead includes API documentation in the questions
# for the agent to reference.
# An example question including the API documentation is printed below.
print(
    "\nExample question including API documentation:\n",
    benchmark._data['questions'][0]['text'],
)
'''
===============================================================================
Example question including API documentation:
  What is an API that can be used to classify sports activities in videos?\n
 Use this API documentation for reference:  
 {"domain": "Video Classification", "framework": "PyTorch", 
 "functionality": "3D ResNet", "api_name": "slow_r50", 
 "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', 
 model='slow_r50', pretrained=True)", "api_arguments": {"pretrained": "True"}, 
 "python_environment_requirements": ["torch", "json", "urllib", 
 "pytorchvideo", 
 "torchvision", "torchaudio", "torchtext", "torcharrow", "TorchData", 
 "TorchRec", "TorchServe", "PyTorch on XLA Devices"], 
 "example_code": ["import torch", 
 "model = torch.hub.load('facebookresearch/pytorchvideo', 
 'slow_r50', pretrained=True)", 
 "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], 
 "performance": {"dataset": "Kinetics 400", 
 "accuracy": {"top_1": 74.58, "top_5": 91.63}, 
 "Flops (G)": 54.52, "Params (M)": 32.45}, 
 "description": "The 3D ResNet model is a Resnet-style video classification 
 network pretrained on the Kinetics 400 dataset. It is based on the 
 architecture from the paper 'SlowFast Networks for Video Recognition' 
 by Christoph Feichtenhofer et al."}}
===============================================================================
'''

print("Total:", result["total"])
print("Correct:", result["correct"])
print("Hallucination:", result["hallucination"])
'''
===============================================================================
Total: 10
Correct: 10
Hallucination: 0
===============================================================================
'''



--------------------------------------------------------------------------------
# File: benchmarks\gaia.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========


from camel.agents import ChatAgent
from camel.benchmarks import DefaultGAIARetriever, GAIABenchmark
from camel.models import ModelFactory
from camel.runtime import RemoteHttpRuntime
from camel.toolkits import CodeExecutionToolkit
from camel.types import ModelPlatformType, ModelType, StorageType

retriever = DefaultGAIARetriever(
    vector_storage_local_path="local_data2/", storage_type=StorageType.QDRANT
)

benchmark = GAIABenchmark(
    data_dir="datasets_test",
    processes=1,
    save_to="results.jsonl",
    retriever=retriever,
)

print(f"Number of validation examples: {len(benchmark.valid)}")
print(f"Number of test examples: {len(benchmark.test)}")


toolkit = CodeExecutionToolkit(verbose=True)
runtime = RemoteHttpRuntime("localhost").add(
    toolkit.get_tools(),
    "camel.toolkits.CodeExecutionToolkit",
)

task_prompt = """
        You are a general AI assistant. I will ask you a question. Report your
        thoughts, and finish your answer with the following template:
        FINAL ANSWER: [YOUR FINAL ANSWER].
        YOUR FINAL ANSWER should be a number OR as few words as possible OR a
        comma separated list of numbers and/or strings.
        If you are asked for a number, don't use comma to write your number
        neither use units such as $ or percent sign unless specified otherwise.
        If you are asked for a string, don't use articles, neither
        abbreviations (e.g. for cities), and write the digits in plain text
        unless specified otherwise.
        If you are asked for a comma separated list, apply the above rules
        depending of whether the element to be put in the list is a number or
        a string.
        """.strip()

tools = runtime.get_tools()

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
)


agent = ChatAgent(
    task_prompt,
    model,
    tools=tools,
)

result = benchmark.run(agent, "valid", level="all", subset=3)
print("correct:", result["correct"])
print("total:", result["total"])

# ruff: noqa: E501
"""
Number of validation examples: 165
Number of test examples: 300
correct: 0
total: 3
"""



--------------------------------------------------------------------------------
# File: benchmarks\nexus.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.benchmarks import NexusBenchmark
from camel.benchmarks.nexus import construct_tool_descriptions

# Set up the agent to be benchmarked
agent = ChatAgent()

# Set up the Nexusraven Function Calling Benchmark
benchmark = NexusBenchmark(
    data_dir="NexusDatasets", save_to="NexusResults.jsonl"
)

# Download the benchmark data
benchmark.download()

# Set the task (sub-dataset) to be benchmarked
task = "OTX"

# Please note that the following step is only for demonstration purposes,
# it has been integrated into the run method of the benchmark.
# The tools fetched here are used to construct the prompt for the task,
# which will be passed to the agent for response.
tools = construct_tool_descriptions(task)
print('\nTool descriptions for the task:\n', tools)
'''
===============================================================================
"""
Function:
def getIndicatorForIPv6(apiKey: str, ip: str, section: str):
"""

    Retrieves comprehensive information for a specific IPv6 address from the 
    AlienVault database. 
    This function allows you to obtain various types of data. 
    The 'general' section provides general information about the IP, 
    including geo data, and a list of other available sections. 
    'reputation' offers OTX data on malicious activity observed by 
    AlienVault Labs. 'geo' details more verbose geographic data such 
    as country code and coordinates. 'malware' reveals malware samples 
    connected to the IP, 
    and 'urlList' shows URLs associated with the IP. Lastly, 'passiveDns' 
    includes passive DNS information about hostnames/domains 
    pointing to this IP.

    Args:
    - apiKey: string, required, Your AlienVault API key
    - ip: string, required, IPv6 address to query
    - section: string, required, Specific data section to retrieve 
    (options: general, reputation, geo, malware, urlList, passiveDns)

"""
Function:
def getIndicatorForDomain(apiKey: str, domain: str, section: str):
"""

    Retrieves a comprehensive overview for a given domain name from the 
    AlienVault database. This function provides various data types 
    about the domain. The 'general' section includes general information 
    about the domain, such as geo data, and lists of other available 
    sections. 'geo' provides detailed geographic data including country 
    code and coordinates. The 'malware' section indicates malware samples 
    associated with the domain. 'urlList' shows URLs linked to the domain,
    'passiveDns' details passive DNS information about hostnames/domains
    associated with the domain, 
    and 'whois' gives Whois records for the domain.

    Args:
    - apiKey: string, required, Your AlienVault API key
    - domain: string, required, Domain address to query
    - section: string, required, Specific data section to retrieve 
    (options: general, geo, malware, urlList, passiveDns, whois)

"""
Function:
def getIndicatorForHostname(apiKey: str, hostname: str, section: str):
"""

    Retrieves detailed information for a specific hostname from the 
    AlienVault database. This function provides various data types about 
    the hostname. The 'general' section includes general information 
    about the IP, geo data, and lists of other available sections. 
    'geo' provides detailed geographic data including country code 
    and coordinates. The 'malware' section indicates malware samples 
    associated with the hostname. 'urlList' shows URLs linked to 
    the hostname, and 'passiveDns' details passive DNS information 
    about hostnames/domains associated with the hostname.

    Args:
    - apiKey: string, required, Your AlienVault API key
    - hostname: string, required, Single hostname address to query
    - section: string, required, Specific data section to retrieve 
    (options: general, geo, malware, urlList, passiveDns)

"""
Function:
def getIndicatorForFileHashes(apiKey: str, fileHash: str, section: str):
"""

    Retrieves information related to a specific file hash from the 
    AlienVault database. 
    This function provides two types of data: 'general', 
    which includes general metadata about the file hash and a list of other 
    available sections for the hash; and 'analysis', which encompasses both 
    dynamic and static analysis of the file, 
    including Cuckoo analysis, exiftool, etc.

    Args:
    - apiKey: string, required, Your AlienVault API key
    - fileHash: string, required, Single file hash to query
    - section: string, required, Specific data section to retrieve 
    (options: general, analysis)

"""
Function:
def getIndicatorForUrl(apiKey: str, url: str, section: str):
"""

    Retrieves information related to a specific URL from the AlienVault 
    database. This function offers two types of data: 'general', 
    which includes historical geographic information, 
    any pulses this indicator is on, 
    and a list of other available sections for this URL; and 'url_list', 
    which provides full results from AlienVault Labs URL analysis, 
    potentially including multiple entries.

    Args:
    - apiKey: string, required, Your AlienVault API key
    - url: string, required, Single URL to query
    - section: string, required, Specific data section to retrieve 
    (options: general, url_list)

"""
Function:
def getIndicatorForCVE(apiKey: str, cve: str, section: str):
"""

    Retrieves information related to a specific CVE 
    (Common Vulnerability Enumeration)
    from the AlienVault database. This function offers detailed data on CVEs.
    The 'General' section includes MITRE CVE data, such as CPEs 
    (Common Platform Enumerations), 
    CWEs (Common Weakness Enumerations), and other relevant details. 
    It also provides information on any pulses this indicator is on, 
    and lists other sections currently available for this CVE.

    Args:
    - apiKey: string, required, Your AlienVault API key
    - cve: string, required, Specific CVE identifier to query 
        (e.g., 'CVE-2014-0160')
    - section: string, required, Specific data section to retrieve 
        ('general' only)

"""
Function:
def getIndicatorForNIDS(apiKey: str, nids: str, section: str):
"""

    Retrieves metadata information for a specific 
    Network Intrusion Detection System (NIDS) 
    indicator from the AlienVault database. This function is designed to 
    provide general metadata about NIDS indicators.

    Args:
    - apiKey: string, required, Your AlienVault API key
    - nids: string, required, Specific NIDS indicator to query 
        (e.g., '2820184')
    - section: string, required, Specific data section to retrieve 
        ('general' only)

"""
Function:
def getIndicatorForCorrelationRules(apiKey: str, correlationRule: str,
 section: str):
"""

    Retrieves metadata information related to a specific Correlation Rule from
    the AlienVault database. This function is designed to provide 
    general metadata about 
    Correlation Rules used in network security and event correlation. 
    Correlation Rules are crucial for identifying patterns and potential 
    security threats in network data.

    Args:
    - apiKey: string, required, Your AlienVault API key
    - correlationRule: string, required, Specific Correlation Rule 
    identifier to query (e.g., '572f8c3c540c6f0161677877')
    - section: string, required, Specific data section to retrieve 
    ('general' only)

"""
===============================================================================
'''

# Run the benchmark
result = benchmark.run(agent, task, subset=10)
print("Total:", result["total"])
print("Correct:", result["correct"])
'''
===============================================================================
Total: 10
Correct: 9
===============================================================================
'''



--------------------------------------------------------------------------------
# File: benchmarks\ragbench.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.benchmarks import RAGBenchBenchmark
from camel.retrievers import AutoRetriever

assistant_sys_msg = """You are a helpful assistant to answer question,
         I will give you the Original Query and Retrieved Context,
        answer the Original Query based on the Retrieved Context,
        if you can't answer the question just say I don't know."""
agent = ChatAgent(assistant_sys_msg)
auto_retriever = AutoRetriever()

benchmark = RAGBenchBenchmark(subset="hotpotqa", split="test")
benchmark.download()
results = benchmark.run(agent, auto_retriever)
print(results)



--------------------------------------------------------------------------------
# File: bots\discord_bot.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import asyncio
from typing import TYPE_CHECKING, List, Optional, Union

from camel.agents import ChatAgent
from camel.bots import DiscordApp
from camel.retrievers import AutoRetriever
from camel.types import StorageType

try:
    from unstructured.documents.elements import Element
except ImportError:
    Element = None

if TYPE_CHECKING:
    from discord import Message


class BotAgent:
    def __init__(
        self,
        contents: Union[str, List[str], "Element", List["Element"]] = None,
        auto_retriever: Optional[AutoRetriever] = None,
        similarity_threshold: float = 0.5,
        vector_storage_local_path: str = "local_data/",
        top_k: int = 1,
        return_detailed_info: bool = True,
    ):
        r"""Initialize the BotAgent instance.

        Args:
            contents (Union[str, List[str], Element, List[Element]], optional)
                : The content to be retrieved.
            auto_retriever (Optional[AutoRetriever], optional): An instance of
                AutoRetriever for vector search.
            similarity_threshold (float): Threshold for vector similarity when
                retrieving content.
            vector_storage_local_path (str): Path to local vector storage for
                the retriever.
            top_k (int): Number of top results to retrieve.
            return_detailed_info (bool): Whether to return detailed
                information from the retriever.
        """

        assistant_sys_msg = '''
            Objective: 
                You are a customer service bot designed to assist users
                with inquiries related to our open-source project. 
                Your responses should be informative, concise, and helpful.

            Instructions:
                Understand User Queries: Carefully read and understand the
                        user's question. Focus on keywords and context to
                        determine the user's intent.
                Search for Relevant Information: Use the provided dataset
                        and refer to the RAG (file to find answers that 
                        closely match the user's query. The RAG file 
                        contains detailed interactions and should be your 
                        primary resource for crafting responses.
                Provide Clear and Concise Responses: Your answers should 
                        be clear and to the point. Avoid overly technical
                        language unless the user's query indicates 
                        familiarity with technical terms.
                Encourage Engagement: Where applicable, encourage users
                        to contribute to the project or seek further
                        assistance.

            Response Structure:
                Greeting: Begin with a polite greeting or acknowledgment.
                Main Response: Provide the main answer to the user's query.
                Additional Information: Offer any extra tips or direct the
                        user to additional resources if necessary.
                Closing: Close the response politely, encouraging
                        further engagement if appropriate.
            bd
            Tone:
                Professional: Maintain a professional tone that 
                        instills confidence in the user.
                Friendly: Be approachable and friendly to make users 
                        feel comfortable.
                Helpful: Always aim to be as helpful as possible,
                        guiding users to solutions.        
        '''

        self._agent = ChatAgent(
            assistant_sys_msg,
        )

        self._auto_retriever = None
        self._contents = contents
        self._top_k = top_k
        self._similarity_threshold = similarity_threshold
        self._return_detailed_info = return_detailed_info

        self._auto_retriever = auto_retriever or AutoRetriever(
            vector_storage_local_path=vector_storage_local_path,
            storage_type=StorageType.QDRANT,
        )

    async def process(self, message: str) -> str:
        r"""Process the user message, retrieve relevant content, and generate
        a response.

        Args:
            message (str): The user's query message.

        Returns:
            str: The assistant's response message.
        """
        user_raw_msg = message
        print("User message:", user_raw_msg)
        if self._auto_retriever:
            retrieved_content = self._auto_retriever.run_vector_retriever(
                query=user_raw_msg,
                contents=self._contents,
                top_k=self._top_k,
                similarity_threshold=self._similarity_threshold,
                return_detailed_info=self._return_detailed_info,
            )
            user_raw_msg = (
                f"Here is the query to you: {user_raw_msg}\n"
                f"Based on the retrieved content: {retrieved_content}, \n"
                f"answer the query"
            )

        assistant_response = self._agent.step(user_raw_msg)
        return assistant_response.msg.content


class DiscordBot(DiscordApp):
    def __init__(
        self,
        agent: BotAgent,
        token: Optional[str] = None,
        channel_ids: Optional[list[int]] = None,
    ):
        r"""Initializes the DiscordBot instance to handle Discord messages and
        communicate with BotAgent.

        Args:
            agent (BotAgent): The BotAgent responsible for processing messages
                and generating responses.
            token (Optional[str]): The token used to authenticate the bot with
                Discord.
            channel_ids (Optional[list[int]]): A list of Discord channel IDs
                where the bot is allowed to interact.
        """
        super().__init__(token=token, channel_ids=channel_ids)
        self.agent: BotAgent = agent

    async def on_message(self, message: 'Message') -> None:
        r"""Event handler for received messages. This method processes incoming
        messages, checks whether the message is from the bot itself, and
        determines whether the bot should respond based on channel ID and
        mentions. Then processes the message using the BotAgent, and responds.

        Args:
            message (discord.Message): The received message object.
        """
        # If the message author is the bot itself,
        # do not respond to this message
        if message.author == self._client.user:
            return

        # If allowed channel IDs are provided,
        # only respond to messages in those channels
        if self.channel_ids and message.channel.id not in self.channel_ids:
            return

        # Only respond to messages that mention the bot
        if not self._client.user or not self._client.user.mentioned_in(
            message
        ):
            return

        user_raw_msg = message.content
        response = await self.agent.process(user_raw_msg)
        await message.channel.send(response)


async def process_message(agent: BotAgent, msg_queue: asyncio.Queue):
    r"""Continuously processes messages from the queue and sends responses.

    This function waits for new messages in the queue, processes each message
    using the `BotAgent` instance, and sends the response back to Discord.

    Args:
        agent (BotAgent): An instance of `BotAgent` that processes the received
            messages.
        msg_queue (asyncio.Queue): The queue from which messages are retrieved
            for processing.
    """
    while True:
        message: "Message" = await msg_queue.get()
        user_raw_msg = message.content

        # Process the message using the agent and get the response
        response = await agent.process(user_raw_msg)
        # message.reply(response)
        await message.channel.send(response)
        msg_queue.task_done()


async def main():
    agent = BotAgent()

    # Initialize the DiscordBot with the message queue
    discord_bot = DiscordBot(agent=agent)

    await discord_bot.start()


if __name__ == "__main__":
    asyncio.run(main())



--------------------------------------------------------------------------------
# File: bots\discord_bot_installation_management.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import asyncio
import os

import uvicorn
from discord import Message
from starlette.responses import RedirectResponse

from camel.bots.discord import DiscordApp, DiscordSQLiteInstallationStore


class DiscordBot(DiscordApp):
    async def on_message(self, message: 'Message') -> None:
        if message.author == self._client.user:
            return

        if self.channel_ids and message.channel.id not in self.channel_ids:
            return

        if not self._client.user or not self._client.user.mentioned_in(
            message
        ):
            return

        installation = await self.installation_store.find_by_guild(
            str(message.guild.id)
        )
        if installation:
            await message.channel.send("Found installation.")
        else:
            await message.channel.send("No installation found for this guild.")


async def main():
    installation_store = DiscordSQLiteInstallationStore(
        database="./discord_installations.db"
    )
    await installation_store.init()

    discord_bot = DiscordBot(
        token=os.getenv("DISCORD_BOT_TOKEN"),
        client_id=os.getenv("DISCORD_BOT_CLIENT_ID"),
        client_secret=os.getenv("DISCORD_BOT_CLIENT_SECRET"),
        redirect_uri=os.getenv("DISCORD_BOT_REDIRECT_URL"),
        installation_store=installation_store,
    )

    @discord_bot.app.get(os.getenv("DISCORD_BOT_REDIRECT_URI"))
    async def oauth_redirect(code: str, guild_id: str):
        if not code:
            return {"error": "No code provided"}

        response = await discord_bot.exchange_code_for_token_response(code)
        if not response:
            return {"error": "Failed to obtain access token"}

        access_token = response.get("access_token")
        refresh_token = response.get("refresh_token")
        expires_in = response.get("expires_in")

        await discord_bot.save_installation(
            guild_id, access_token, refresh_token, expires_in
        )
        return RedirectResponse(url=f"https://discord.com/channels/{guild_id}")

    server_task = asyncio.create_task(
        uvicorn.Server(
            uvicorn.Config(discord_bot.app, host="0.0.0.0", port=8000)
        ).serve()
    )

    bot_task = asyncio.create_task(discord_bot.start())

    await asyncio.gather(server_task, bot_task)


if __name__ == "__main__":
    asyncio.run(main())



--------------------------------------------------------------------------------
# File: bots\discord_bot_use_msg_queue.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import asyncio
from typing import TYPE_CHECKING, List, Optional, Union

from camel.agents import ChatAgent
from camel.bots import DiscordApp
from camel.retrievers import AutoRetriever
from camel.types import StorageType

if TYPE_CHECKING:
    from discord import Message
    from unstructured.documents.elements import Element


class BotAgent:
    def __init__(
        self,
        contents: Union[str, List[str], "Element", List["Element"]] = None,
        auto_retriever: Optional[AutoRetriever] = None,
        similarity_threshold: float = 0.5,
        vector_storage_local_path: str = "local_data/",
        top_k: int = 1,
        return_detailed_info: bool = True,
    ):
        r"""Initialize the BotAgent instance.

        Args:
            contents (Union[str, List[str], Element, List[Element]], optional)
                : The content to be retrieved.
            auto_retriever (Optional[AutoRetriever], optional): An instance of
                AutoRetriever for vector search.
            similarity_threshold (float): Threshold for vector similarity when
                retrieving content.
            vector_storage_local_path (str): Path to local vector storage for
                the retriever.
            top_k (int): Number of top results to retrieve.
            return_detailed_info (bool): Whether to return detailed
                information from the retriever.
        """

        assistant_sys_msg = '''
            Objective: 
                You are a customer service bot designed to assist users
                with inquiries related to our open-source project. 
                Your responses should be informative, concise, and helpful.

            Instructions:
                Understand User Queries: Carefully read and understand the
                        user's question. Focus on keywords and context to
                        determine the user's intent.
                Search for Relevant Information: Use the provided dataset
                        and refer to the RAG (file to find answers that 
                        closely match the user's query. The RAG file 
                        contains detailed interactions and should be your 
                        primary resource for crafting responses.
                Provide Clear and Concise Responses: Your answers should 
                        be clear and to the point. Avoid overly technical
                        language unless the user's query indicates 
                        familiarity with technical terms.
                Encourage Engagement: Where applicable, encourage users
                        to contribute to the project or seek further
                        assistance.

            Response Structure:
                Greeting: Begin with a polite greeting or acknowledgment.
                Main Response: Provide the main answer to the user's query.
                Additional Information: Offer any extra tips or direct the
                        user to additional resources if necessary.
                Closing: Close the response politely, encouraging
                        further engagement if appropriate.
            bd
            Tone:
                Professional: Maintain a professional tone that 
                        instills confidence in the user.
                Friendly: Be approachable and friendly to make users 
                        feel comfortable.
                Helpful: Always aim to be as helpful as possible,
                        guiding users to solutions.        
        '''

        self._agent = ChatAgent(
            assistant_sys_msg,
        )

        self._auto_retriever = None
        self._contents = contents
        self._top_k = top_k
        self._similarity_threshold = similarity_threshold
        self._return_detailed_info = return_detailed_info

        self._auto_retriever = auto_retriever or AutoRetriever(
            vector_storage_local_path=vector_storage_local_path,
            storage_type=StorageType.QDRANT,
        )

    async def process(self, message: str) -> str:
        r"""Process the user message, retrieve relevant content, and generate
        a response.

        Args:
            message (str): The user's query message.

        Returns:
            str: The assistant's response message.
        """
        user_raw_msg = message
        print("User message:", user_raw_msg)
        if self._auto_retriever:
            retrieved_content = self._auto_retriever.run_vector_retriever(
                query=user_raw_msg,
                contents=self._contents,
                top_k=self._top_k,
                similarity_threshold=self._similarity_threshold,
                return_detailed_info=self._return_detailed_info,
            )
            user_raw_msg = (
                f"Here is the query to you: {user_raw_msg}\n"
                f"Based on the retrieved content: {retrieved_content}, \n"
                f"answer the query"
            )

        assistant_response = self._agent.step(user_raw_msg)
        return assistant_response.msg.content


class DiscordBot(DiscordApp):
    r"""A Discord bot that listens for messages, adds them to a queue,
    and processes them asynchronously.

    This class extends the functionality of `DiscordApp` and adds message
    handling by pushing messages into a queue for further processing.

    Args:
        msg_queue (asyncio.Queue): A queue used to store incoming messages for
            processing.
        token (Optional[str]): The token used to authenticate the bot with
            Discord.
        channel_ids (Optional[list[int]]): A list of Discord channel IDs where
            the bot is allowed to interact.
    """

    def __init__(
        self,
        msg_queue: asyncio.Queue,
        token: Optional[str] = None,
        channel_ids: Optional[list[int]] = None,
    ):
        super().__init__(token=token, channel_ids=channel_ids)
        self._queue: asyncio.Queue = msg_queue

    async def on_message(self, message: 'Message') -> None:
        r"""Event handler for received messages. This method processes incoming
        messages, checks whether the message is from the bot itself, and
        determines whether the bot should respond based on channel ID and
        mentions.

        Args:
            message (discord.Message): The received message object.
        """
        # If the message author is the bot itself,
        # do not respond to this message
        if message.author == self._client.user:
            return

        # If allowed channel IDs are provided,
        # only respond to messages in those channels
        if self.channel_ids and message.channel.id not in self.channel_ids:
            return

        # Only respond to messages that mention the bot
        if not self._client.user or not self._client.user.mentioned_in(
            message
        ):
            return

        await self._queue.put(message)


async def process_message(agent: BotAgent, msg_queue: asyncio.Queue):
    r"""Continuously processes messages from the queue and sends responses.

    This function waits for new messages in the queue, processes each message
    using the `BotAgent` instance, and sends the response back to Discord.

    Args:
        agent (BotAgent): An instance of `BotAgent` that processes the received
            messages.
        msg_queue (asyncio.Queue): The queue from which messages are retrieved
            for processing.
    """
    while True:
        message: "Message" = await msg_queue.get()
        user_raw_msg = message.content

        # Process the message using the agent and get the response
        response = await agent.process(user_raw_msg)
        # message.reply(response)
        await message.channel.send(response)
        msg_queue.task_done()


async def main():
    r"""Main function to initialize and run the Discord bot and message
    processor.

    This function initializes the message queue, creates an `BotAgent` instance
    for processing messages, and starts both the Discord bot and the
    message-processing loop asynchronously.
    """
    msg_queue = asyncio.Queue()

    agent = BotAgent()

    # Initialize the DiscordBot with the message queue
    discord_bot = DiscordBot(msg_queue=msg_queue)
    await asyncio.gather(
        discord_bot.start(), process_message(agent, msg_queue)
    )


if __name__ == "__main__":
    asyncio.run(main())



--------------------------------------------------------------------------------
# File: bots\slack_bot.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import logging
from typing import TYPE_CHECKING, List, Optional, Union

from slack_bolt.context.async_context import AsyncBoltContext
from slack_bolt.context.say.async_say import AsyncSay
from slack_sdk.web.async_client import AsyncWebClient

from camel.agents import ChatAgent
from camel.bots import SlackApp, SlackEventBody
from camel.retrievers import AutoRetriever
from camel.types import StorageType

if TYPE_CHECKING:
    from unstructured.documents.elements import Element

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BotAgent:
    def __init__(
        self,
        contents: Union[str, List[str], "Element", List["Element"]] = None,
        auto_retriever: Optional[AutoRetriever] = None,
        similarity_threshold: float = 0.5,
        vector_storage_local_path: str = "local_data/",
        top_k: int = 1,
        return_detailed_info: bool = True,
    ):
        r"""Initialize the BotAgent instance.

        Args:
            contents (Union[str, List[str], Element, List[Element]], optional)
                : The content to be retrieved.
            auto_retriever (Optional[AutoRetriever], optional): An instance of
                AutoRetriever for vector search.
            similarity_threshold (float): Threshold for vector similarity when
                retrieving content.
            vector_storage_local_path (str): Path to local vector storage for
                the retriever.
            top_k (int): Number of top results to retrieve.
            return_detailed_info (bool): Whether to return detailed
                information from the retriever.
        """

        content = '''
            Objective: 
                You are a customer service bot designed to assist users
                with inquiries related to our open-source project. 
                Your responses should be informative, concise, and helpful.

            Instructions:
                Understand User Queries: Carefully read and understand the
                        user's question. Focus on keywords and context to
                        determine the user's intent.
                Search for Relevant Information: Use the provided dataset
                        and refer to the RAG (file to find answers that 
                        closely match the user's query. The RAG file 
                        contains detailed interactions and should be your 
                        primary resource for crafting responses.
                Provide Clear and Concise Responses: Your answers should 
                        be clear and to the point. Avoid overly technical
                        language unless the user's query indicates 
                        familiarity with technical terms.
                Encourage Engagement: Where applicable, encourage users
                        to contribute to the project or seek further
                        assistance.

            Response Structure:
                Greeting: Begin with a polite greeting or acknowledgment.
                Main Response: Provide the main answer to the user's query.
                Additional Information: Offer any extra tips or direct the
                        user to additional resources if necessary.
                Closing: Close the response politely, encouraging
                        further engagement if appropriate.
            bd
            Tone:
                Professional: Maintain a professional tone that 
                        instills confidence in the user.
                Friendly: Be approachable and friendly to make users 
                        feel comfortable.
                Helpful: Always aim to be as helpful as possible,
                        guiding users to solutions.        
        '''

        self._agent = ChatAgent(
            content,
        )

        self._auto_retriever = None
        self._contents = contents
        self._top_k = top_k
        self._similarity_threshold = similarity_threshold
        self._return_detailed_info = return_detailed_info

        self._auto_retriever = auto_retriever or AutoRetriever(
            vector_storage_local_path=vector_storage_local_path,
            storage_type=StorageType.QDRANT,
        )

    async def process(self, message: str) -> str:
        user_raw_msg = message
        logger.info("User message:", user_raw_msg)
        if self._auto_retriever:
            retrieved_content = self._auto_retriever.run_vector_retriever(
                query=user_raw_msg,
                contents=self._contents,
                top_k=self._top_k,
                similarity_threshold=self._similarity_threshold,
                return_detailed_info=self._return_detailed_info,
            )
            user_raw_msg = (
                f"Here is the query to you: {user_raw_msg}\n"
                f"Based on the retrieved content: {retrieved_content}, \n"
                f"answer the query"
            )

        assistant_response = self._agent.step(user_raw_msg)
        return assistant_response.msg.content


class SlackBot(SlackApp):
    def __init__(
        self,
        agent: BotAgent,
        token: Optional[str] = None,
        scopes: Optional[str] = None,
        signing_secret: Optional[str] = None,
        client_id: Optional[str] = None,
        client_secret: Optional[str] = None,
    ):
        """Initializes the SlackBot instance to handle Slack messages and
        communicate with BotAgent.

        Args:
            agent (BotAgent): The BotAgent responsible for processing messages
                and generating responses.
            token (Optional[str]): Slack API token for authentication.
            scopes (Optional[str]): Slack app scopes for permissions.
            signing_secret (Optional[str]): Signing secret for verifying Slack
                requests.
            client_id (Optional[str]): Slack app client ID.
            client_secret (Optional[str]): Slack app client secret.
        """
        super().__init__(
            token=token,
            scopes=scopes,
            signing_secret=signing_secret,
            client_id=client_id,
            client_secret=client_secret,
        )
        self.agent: BotAgent = agent

    async def on_message(
        self,
        context: "AsyncBoltContext",
        client: "AsyncWebClient",
        event: dict,
        body: dict,
        say: "AsyncSay",
    ):
        """Handles incoming Slack messages, processes them with BotAgent, and
        responds.

        Args:
            context (AsyncBoltContext): Context object that contains
                information about the Slack event.
            client (AsyncWebClient): Slack Web API client for making API
                requests.
            event (dict): Event data containing details of the incoming Slack
                message.
            body (dict): Full request body from Slack.
            say (AsyncSay): A function to send a response back to the Slack
                channel.
        """
        await context.ack()
        event_body = SlackEventBody(**body)
        user_raw_msg = event_body.event.text
        response = await agent.process(user_raw_msg)
        await say(response)


if __name__ == "__main__":
    agent = BotAgent()

    slack_bot = SlackBot(agent=agent)
    slack_bot.run(3000)



--------------------------------------------------------------------------------
# File: bots\slack_bot_use_msg_queue.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import asyncio
import logging
import queue
import threading
from typing import TYPE_CHECKING, List, Optional, Union

from slack_bolt.context.async_context import AsyncBoltContext
from slack_bolt.context.say.async_say import AsyncSay
from slack_sdk.web.async_client import AsyncWebClient

from camel.agents import ChatAgent
from camel.bots import SlackApp, SlackEventBody
from camel.retrievers import AutoRetriever
from camel.types import StorageType

if TYPE_CHECKING:
    from unstructured.documents.elements import Element

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BotAgent:
    def __init__(
        self,
        contents: Union[str, List[str], "Element", List["Element"]] = None,
        auto_retriever: Optional[AutoRetriever] = None,
        similarity_threshold: float = 0.5,
        vector_storage_local_path: str = "local_data/",
        top_k: int = 1,
        return_detailed_info: bool = True,
    ):
        r"""Initialize the BotAgent instance.

        Args:
            contents (Union[str, List[str], Element, List[Element]], optional)
                : The content to be retrieved.
            auto_retriever (Optional[AutoRetriever], optional): An instance of
                AutoRetriever for vector search.
            similarity_threshold (float): Threshold for vector similarity when
                retrieving content.
            vector_storage_local_path (str): Path to local vector storage for
                the retriever.
            top_k (int): Number of top results to retrieve.
            return_detailed_info (bool): Whether to return detailed
                information from the retriever.
        """

        assistant_sys_msg = '''
            Objective: 
                You are a customer service bot designed to assist users
                with inquiries related to our open-source project. 
                Your responses should be informative, concise, and helpful.

            Instructions:
                Understand User Queries: Carefully read and understand the
                        user's question. Focus on keywords and context to
                        determine the user's intent.
                Search for Relevant Information: Use the provided dataset
                        and refer to the RAG (file to find answers that 
                        closely match the user's query. The RAG file 
                        contains detailed interactions and should be your 
                        primary resource for crafting responses.
                Provide Clear and Concise Responses: Your answers should 
                        be clear and to the point. Avoid overly technical
                        language unless the user's query indicates 
                        familiarity with technical terms.
                Encourage Engagement: Where applicable, encourage users
                        to contribute to the project or seek further
                        assistance.

            Response Structure:
                Greeting: Begin with a polite greeting or acknowledgment.
                Main Response: Provide the main answer to the user's query.
                Additional Information: Offer any extra tips or direct the
                        user to additional resources if necessary.
                Closing: Close the response politely, encouraging
                        further engagement if appropriate.
            bd
            Tone:
                Professional: Maintain a professional tone that 
                        instills confidence in the user.
                Friendly: Be approachable and friendly to make users 
                        feel comfortable.
                Helpful: Always aim to be as helpful as possible,
                        guiding users to solutions.        
        '''

        self._agent = ChatAgent(
            assistant_sys_msg,
        )

        self._auto_retriever = None
        self._contents = contents
        self._top_k = top_k
        self._similarity_threshold = similarity_threshold
        self._return_detailed_info = return_detailed_info

        self._auto_retriever = auto_retriever or AutoRetriever(
            vector_storage_local_path=vector_storage_local_path,
            storage_type=StorageType.QDRANT,
        )

    async def process(self, message: str) -> str:
        r"""Process the user message, retrieve relevant content, and generate
        a response.

        Args:
            message (str): The user's query message.

        Returns:
            str: The assistant's response message.
        """
        user_raw_msg = message
        print("User message:", user_raw_msg)
        if self._auto_retriever:
            retrieved_content = self._auto_retriever.run_vector_retriever(
                query=user_raw_msg,
                contents=self._contents,
                top_k=self._top_k,
                similarity_threshold=self._similarity_threshold,
                return_detailed_info=self._return_detailed_info,
            )
            user_raw_msg = (
                f"Here is the query to you: {user_raw_msg}\n"
                f"Based on the retrieved content: {retrieved_content}, \n"
                f"answer the query"
            )

        assistant_response = self._agent.step(user_raw_msg)
        return assistant_response.msg.content


class SlackBot(SlackApp):
    r"""SlackBot class that extends the SlackApp class to handle Slack events
    and integrate with a message queue for asynchronous processing.

    This class initializes the Slack app and adds a message handler to process
    Slack events, specifically for incoming messages.

    Args:
        msg_queue (queue.Queue): A thread-safe queue to communicate between
            threads.
        token (Optional[str]): Slack API token for authentication.
        scopes (Optional[str]): Slack app scopes for permissions.
        signing_secret (Optional[str]): Signing secret for verifying Slack
            requests.
        client_id (Optional[str]): Slack app client ID.
        client_secret (Optional[str]): Slack app client secret.
    """

    def __init__(
        self,
        msg_queue: queue.Queue,
        token: Optional[str] = None,
        scopes: Optional[str] = None,
        signing_secret: Optional[str] = None,
        client_id: Optional[str] = None,
        client_secret: Optional[str] = None,
    ):
        r"""Initializes the SlackBot instance with a message queue and the
        required Slack authentication details.

        Args:
            msg_queue (queue.Queue): A thread-safe queue to communicate between
                threads.
            token (Optional[str]): Slack API token for authentication.
            scopes (Optional[str]): Slack app scopes for permissions.
            signing_secret (Optional[str]): Signing secret for verifying Slack
                requests.
            client_id (Optional[str]): Slack app client ID.
            client_secret (Optional[str]): Slack app client secret.
        """
        super().__init__(
            token=token,
            scopes=scopes,
            signing_secret=signing_secret,
            client_id=client_id,
            client_secret=client_secret,
        )
        self._queue: queue.Queue = msg_queue

    async def on_message(
        self,
        context: "AsyncBoltContext",
        client: "AsyncWebClient",
        event: dict,
        body: dict,
        say: "AsyncSay",
    ):
        r"""Event handler for processing incoming Slack messages.

        This method is called when a message event is received from Slack.
        It acknowledges the message and adds the event body and say function to
        the queue for further processing.

        Args:
            context (AsyncBoltContext): Context object that contains
                information about the Slack event.
            client (AsyncWebClient): Slack Web API client for making API
                requests.
            event (dict): Event data containing details of the incoming Slack
                message.
            body (dict): Full request body from Slack.
            say (AsyncSay): A function to send a response back to the Slack
                channel.
        """
        await context.ack()
        event_body = SlackEventBody(**body)
        self._queue.put((event_body, say))


async def process_message(agent: BotAgent, msg_queue: queue.Queue):
    r"""Process messages from the queue asynchronously.

    This function continuously retrieves messages from the message queue,
    processes them using the `BotAgent` instance, and sends the response back
    to Slack using the `say` function.

    Args:
        agent (BotAgent): An instance of the `BotAgent` class that handles the
            processing of the incoming message.
        msg_queue (queue.Queue): A thread-safe queue that stores Slack event
            messages and the corresponding `say` functions for response.
    """
    while True:
        event_body, say = msg_queue.get()

        logger.info(f"Received message: {event_body.event.text}")
        user_raw_msg = event_body.event.text

        # Process the message using the agent and send response back to Slack.
        response = await agent.process(user_raw_msg)
        await say(response)
        msg_queue.task_done()


def start_async_queue_processor(agent: BotAgent, msg_queue: queue.Queue):
    r"""Start an asynchronous queue processor in a new event loop.

    This function creates a new asyncio event loop in a separate thread to
    asynchronously process messages from the queue. It will run until all
    the messages in the queue have been processed.

    Args:
        agent (BotAgent): The agent responsible for processing the messages.
        msg_queue (queue.Queue): The message queue that contains Slack events
            and responses.
    """
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    # Schedule the asynchronous message processing task in this event loop
    loop.run_until_complete(process_message(agent, msg_queue))


if __name__ == "__main__":
    r"""Main entry point for running the Slack bot application.

    This section initializes the required components including the message 
    queue, agent, and the SlackBot instance. It also starts a separate thread 
    for asynchronous message processing to avoid blocking the Slack bot's main 
    event loop. The `slack_bot.run()` function will handle incoming Slack 
    events on the main thread, while the separate thread will handle processing
    the messages from the queue.
    """
    msg_queue = queue.Queue()

    agent = BotAgent()

    thread = threading.Thread(
        target=start_async_queue_processor, args=(agent, msg_queue)
    )
    thread.start()

    # Initialize the SlackBot with the message queue.
    slack_bot = SlackBot(msg_queue=msg_queue)
    slack_bot.run(3000)

    thread.join()



--------------------------------------------------------------------------------
# File: code\generate_meta_data.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.prompts import PromptTemplateGenerator
from camel.types import TaskType


def generate_meta_data(meta_data: str, num: int = 50, model=None):
    prompt_template = PromptTemplateGenerator().get_prompt_from_key(
        TaskType.CODE, f"generate_{meta_data}"
    )
    prompt = prompt_template.format(**{f"num_{meta_data}": num})
    print(prompt)
    agent = ChatAgent("You are a helpful assistant.", model=model)
    agent.reset()

    assistant_response = agent.step(prompt)
    print(assistant_response.msg.content)


def main(model=None):
    generate_meta_data("languages", 20, model=model)
    generate_meta_data("domains", 50, model=model)


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: code\role_playing.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from colorama import Fore

from camel.societies import RolePlaying
from camel.types import TaskType
from camel.utils import print_text_animated


def main(model=None, chat_turn_limit=50) -> None:
    task_prompt = "Develop a poll app"
    language = "JavaScript"
    domain = "Sociology"
    meta_dict = {"language": language, "domain": domain}
    role_play_session = RolePlaying(
        assistant_role_name=f"{language} Programmer",
        assistant_agent_kwargs=dict(model=model),
        user_role_name=f"Person working in {domain}",
        user_agent_kwargs=dict(model=model),
        task_prompt=task_prompt,
        with_task_specify=True,
        task_specify_agent_kwargs=dict(model=model),
        task_type=TaskType.CODE,
        extend_sys_msg_meta_dicts=[meta_dict, meta_dict],
        extend_task_specify_meta_dict=meta_dict,
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    n = 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
        )
        print_text_animated(
            Fore.GREEN + "AI Assistant:\n\n"
            f"{assistant_response.msg.content}\n"
        )

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: code\role_playing_multiprocess.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import json
import multiprocessing
import os
from typing import Any, Dict

from camel.agents import ChatAgent, TaskSpecifyAgent
from camel.generators import SystemMessageGenerator
from camel.messages import BaseMessage
from camel.types import RoleType, TaskType
from camel.utils import download_tasks


def init_chat(
    assistant_agent: ChatAgent,
    user_agent: ChatAgent,
    user_sys_msg: BaseMessage,
    assistant_sys_msg: BaseMessage,
):
    assistant_agent.reset()
    user_agent.reset()

    # Send the system messages again to the agents using chat messages
    assistant_msg = BaseMessage.make_assistant_message(
        role_name=assistant_agent.role_name,
        content=(
            f"{user_sys_msg.content}. "
            "Now start to give me instructions one by one. "
            "Only reply with Instruction and Input."
        ),
    )

    user_msg = BaseMessage.make_user_message(
        role_name=user_agent.role_name, content=f"{assistant_sys_msg.content}"
    )
    assistant_agent.step(user_msg)

    return assistant_msg


def generate_data(
    language_idx: int,
    language_name: str,
    domain_idx: int,
    domain_name: str,
    task_idx: int,
    task_prompt: str,
) -> None:
    max_num_messages = 40

    # Remove number from task prompt
    original_task_prompt = task_prompt.replace(f"{task_idx+1}. ", "")

    task_specify_agent = TaskSpecifyAgent(
        task_type=TaskType.CODE,
    )
    specified_task_prompt = task_specify_agent.run(
        original_task_prompt,
        meta_dict=dict(domain=domain_name, language=language_name),
    )

    print(f"Original Task: {original_task_prompt}")
    print(f"Specified Task: {specified_task_prompt}")

    sys_msg_generator = SystemMessageGenerator(task_type=TaskType.CODE)
    sys_msg_meta_dicts = [
        dict(
            language=language_name,
            domain=domain_name,
            task=specified_task_prompt,
        )
    ] * 2
    assistant_sys_msg, user_sys_msg = sys_msg_generator.from_dicts(
        sys_msg_meta_dicts,
        role_tuples=[
            (f"{language_name} Programmer", RoleType.ASSISTANT),
            (f"{domain_name} User", RoleType.USER),
        ],
    )

    assistant_agent = ChatAgent(
        assistant_sys_msg, message_window_size=max_num_messages
    )
    user_agent = ChatAgent(user_sys_msg, message_window_size=max_num_messages)

    input_assistant_msg = init_chat(
        assistant_agent, user_agent, user_sys_msg, assistant_sys_msg
    )

    print("Assistant System Message: ", assistant_sys_msg.content)
    print("User System Message: ", user_sys_msg.content)
    message_counter = 0
    message_dict: Dict[str, Any] = {}

    # Append roles to the dictionary
    # We start number from 1 not 0.
    message_dict["role_1"] = f"{language_name}_{assistant_agent.role_type!s}"
    message_dict["role_2"] = f"{domain_name}_{user_agent.role_type!s}"
    message_dict["id"] = (
        f"{(language_idx+1):03}_{(domain_idx+1):03}_{(task_idx+1):03}"
    )
    message_dict["original_task"] = original_task_prompt
    message_dict["specified_task"] = specified_task_prompt

    # Threshold to terminate the conversation if no end token appears
    repeat_word_counter = 0
    repeat_word_threshold = 4
    repeat_word_list = [
        "goodbye",
        "good bye",
        "thank",
        "bye",
        "welcome",
        "language model",
    ]

    assistant_instruct_counter = 0
    assistant_instruct_threshold = 1
    assistant_instruct_word = "Instruction:"

    user_no_instruct_counter = 0
    user_no_instruct_threshold = 3
    user_no_instruct_word = "Instruction:"

    # Set max number of messages for the chat

    while message_counter < max_num_messages:
        user_response = user_agent.step(input_assistant_msg)

        # Condition 1: User terminates the chat
        if user_response.terminated:
            message_dict["termination_reason"] = (
                f"{user_agent.role_type!s}: "
                f"{user_response.info['termination_reasons'][0]}"
            )
            break

        print(f"User:\n{user_response.msg.content}\n")

        assistant_response = assistant_agent.step(user_response.msg)

        # Condition 2: Assistant terminates the chat
        if assistant_response.terminated:
            message_dict["termination_reason"] = (
                f"{assistant_agent.role_type!s}: "
                f"{assistant_response.info['termination_reasons'][0]}"
            )
            break

        print(f"Assistant:\n{assistant_response.msg.content}\n")

        # Condition 3: Break if user does not give instruction
        if user_no_instruct_word not in user_response.msg.content:
            user_no_instruct_counter += 1
            if user_no_instruct_counter == user_no_instruct_threshold:
                message_dict['termination_reason'] = (
                    "user_no_instruct_threshold"
                )
                break
        else:
            user_no_instruct_counter = 0

        # Condition 4: Break if assistant gives instruction (flipped role)
        if assistant_instruct_word in assistant_response.msg.content:
            assistant_instruct_counter += 1
            if assistant_instruct_counter == assistant_instruct_threshold:
                message_dict['termination_reason'] = (
                    "assistant_instruct_threshold"
                )
                break
        else:
            assistant_instruct_counter = 0

        # Condition 5: Repeat word observed
        for repeat_word in repeat_word_list:
            if (
                repeat_word in user_response.msg.content.lower()
                or repeat_word in assistant_response.msg.content.lower()
            ):
                repeat_word_counter += 1
                if repeat_word_counter == repeat_word_threshold:
                    message_dict['termination_reason'] = (
                        "repeat_word_threshold"
                    )
                    break
            else:
                repeat_word_counter = 0

        # Save user message
        message_counter += 1
        message_dict[f"message_{message_counter}"] = (
            user_response.msg.to_dict()
        )

        # Condition 5: End token observed
        if "<CAMEL_TASK_DONE>" in user_response.msg.content:
            message_dict['termination_reason'] = "<CAMEL_TASK_DONE>"
            break

        # Save assistant message
        message_counter += 1
        message_dict[f"message_{message_counter}"] = (
            assistant_response.msg.to_dict()
        )

        input_assistant_msg = assistant_response.msg

    message_dict["num_messages"] = message_counter

    if message_dict["num_messages"] == max_num_messages:
        message_dict["termination_reason"] = "max_num_messages"

    with open(
        f"./camel_data/code/{message_dict['id']}.json", "w"
    ) as json_file:
        json.dump(message_dict, json_file, ensure_ascii=False)


def main() -> None:
    # Define the folder path
    folder_path = "./code_data/"

    # Check if the folder already exists
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)

    # Check if the folder is empty
    if not os.listdir(folder_path):
        download_tasks(task=TaskType.CODE, folder_path=folder_path)

    # Chunk for parallel jobs
    try:
        slurm_array_task_id = os.environ.get('SLURM_ARRAY_TASK_ID')
        if not isinstance(slurm_array_task_id, str):
            raise TypeError()
        array_idx = int(slurm_array_task_id)
    except (TypeError, ValueError) as e:
        print(f"Error: {e}")
        array_idx = 0

    languages_per_chunk = 4

    # Parameters for filtering the generated task string
    start_token = "1."
    num_tasks = 50

    with open("./data/code/languages.txt", "r") as f:
        languages = f.read().splitlines()

    with open("./data/code/domains.txt", "r") as f:
        domains = f.read().splitlines()

    assert (array_idx + 1) * languages_per_chunk <= len(languages)
    languages = languages[
        array_idx * languages_per_chunk : (array_idx + 1) * languages_per_chunk
    ]

    pool = multiprocessing.Pool()

    for language_idx, language_name in enumerate(languages):
        language_idx += array_idx * languages_per_chunk
        language_name = " ".join(language_name.split(" ")[1:])
        for domain_idx, domain_name in enumerate(domains):
            domain_name = " ".join(domain_name.split(" ")[1:])
            # Load the task list assigned for assistant and user roles
            with open(
                f"./code_data/tasks/{language_name}_{domain_name}.txt", "r"
            ) as f:
                tasks = f.read().splitlines()

                # Filter out the generated response to include the tasks only
                for i, task in enumerate(tasks):
                    if start_token in task:
                        tasks = tasks[i : i + num_tasks]
                        break

                # Ensure exact number of tasks is generated
                assert str(num_tasks) in tasks[-1], print(tasks)

            for task_idx, task_prompt in enumerate(tasks):
                id = (
                    f"{(language_idx+1):03}_"
                    f"{(domain_idx+1):03}_{(task_idx+1):03}"
                )
                if not os.path.exists(f"./camel_data/code/{id}.json"):
                    pool.apply_async(
                        generate_data,
                        (
                            language_idx,
                            language_name,
                            domain_idx,
                            domain_name,
                            task_idx,
                            task_prompt,
                        ),
                    )

    pool.close()
    pool.join()


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: code\task_generation.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import multiprocessing
import os

from camel.agents import ChatAgent
from camel.generators import CodeTaskPromptGenerator, SystemMessageGenerator
from camel.messages import BaseMessage
from camel.types import RoleType, TaskType


def generate_tasks(
    task_generator_prompt: str,
    language: str,
    domain: str,
    start_token: str = "1.",
    num_tasks: int = 10,
    model=None,
) -> None:
    sys_msg_generator = SystemMessageGenerator(task_type=TaskType.DEFAULT)
    assistant_sys_msg = sys_msg_generator.from_dict(
        dict(), role_tuple=("Task Generator", RoleType.DEFAULT)
    )
    assistant_agent = ChatAgent(assistant_sys_msg, model=model)

    user_msg = BaseMessage.make_user_message(
        role_name="Task Generator", content=task_generator_prompt
    )

    assistant_response = assistant_agent.step(user_msg)

    tasks = assistant_response.msg.content.split("\n")

    # Filter out the generated response to include the tasks only
    for i, task in enumerate(tasks):
        if start_token in task:
            tasks = tasks[i : i + num_tasks]
            break

    # Ensure exact number of tasks is generated
    assert str(num_tasks) in tasks[-1], print(tasks)

    with open(f"./code/tasks/{language}_{domain}.txt", "w") as file:
        file.write("\n".join(tasks))


def main(model=None) -> None:
    num_tasks = 50
    start_token = "1."

    task_generator_prompt_gen = CodeTaskPromptGenerator(
        num_tasks=num_tasks
    ).from_role_files()

    pool = multiprocessing.Pool()
    for task_generator_prompt, language, domain in task_generator_prompt_gen:
        if not os.path.exists(f"./code/tasks/{language}_{domain}.txt"):
            print(language, domain)

            pool.apply_async(
                generate_tasks,
                (
                    task_generator_prompt,
                    language,
                    domain,
                    start_token,
                    num_tasks,
                    model,
                ),
            )

    pool.close()
    pool.join()


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: conversion.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from typing import List

from camel.messages import BaseMessage, HermesFunctionFormatter
from camel.messages.conversion import (
    ShareGPTConversation,
    ShareGPTMessage,
)
from camel.messages.func_message import FunctionCallingMessage


def sharegpt_to_camel_messages(
    conversation: ShareGPTConversation,
) -> List[BaseMessage]:
    r"""Convert ShareGPT conversation to list of CAMEL messages"""
    return [
        BaseMessage.from_sharegpt(msg, HermesFunctionFormatter())
        for msg in conversation
    ]


def camel_messages_to_sharegpt(
    messages: List[BaseMessage],
) -> ShareGPTConversation:
    r"""Convert list of CAMEL messages to ShareGPT conversation"""
    sharegpt_messages = [
        msg.to_sharegpt(HermesFunctionFormatter()) for msg in messages
    ]
    return ShareGPTConversation.model_validate(sharegpt_messages)


# Example usage
if __name__ == "__main__":
    # Create a sample ShareGPT conversation
    sharegpt_conv = ShareGPTConversation.model_validate(
        [
            ShareGPTMessage(
                from_="system", value="You are a helpful assistant."
            ),
            ShareGPTMessage(from_="human", value="What's Tesla's P/E ratio?"),
            ShareGPTMessage(
                from_="gpt",
                value="Let me check Tesla's stock fundamentals.\n"
                "<tool_call>\n{'name': 'get_stock_fundamentals',"
                " 'arguments': {'symbol': 'TSLA'}}\n</tool_call>",
            ),
            ShareGPTMessage(
                from_="tool",
                value='''<tool_response>
{"name": "get_stock_fundamentals", "content": 
{"symbol": "TSLA", "company_name": "Tesla, Inc.", 
"sector": "Consumer Cyclical", "pe_ratio": 49.604652}}
</tool_response>''',
            ),
            ShareGPTMessage(
                from_="gpt",
                value="Tesla (TSLA) currently has a P/E ratio of 49.60.",
            ),
        ]
    )

    # Convert to CAMEL messages
    camel_messages = sharegpt_to_camel_messages(sharegpt_conv)

    print("\nCAMEL Messages:")
    for msg in camel_messages:
        print(f"Role: {msg.role_name}")
        print(f"Content: {msg.content}")
        if isinstance(msg, FunctionCallingMessage):
            print(f"Function Name: {msg.func_name}")
            if msg.args:
                print(f"Arguments: {msg.args}")
            if msg.result:
                print(f"Result: {msg.result}")
        print()

    # Convert back to ShareGPT
    converted_back = camel_messages_to_sharegpt(camel_messages)
    print("\nConverted back to ShareGPT:")
    for msg in converted_back:
        print(f"From: {msg.from_}")
        print(f"Value: {msg.value}")
        print()



--------------------------------------------------------------------------------
# File: data_collector\alpaca_collector.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents.chat_agent import ChatAgent
from camel.configs.openai_config import ChatGPTConfig
from camel.data_collector import AlpacaDataCollector
from camel.models.model_factory import ModelFactory
from camel.types.enums import ModelPlatformType, ModelType

model_config_dict = ChatGPTConfig(
    temperature=0.0,
).as_dict()

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=model_config_dict,
)

agent = ChatAgent(
    system_message="You are a helpful assistant",
    model=model,
)

usr_msg = "When is the release date of the video game Portal?"

collector = AlpacaDataCollector().record(agent).start()

# Automatically record the message
resp = agent.step(usr_msg)

print(collector.convert())

print(collector.llm_convert())

collector.reset()

# Manually record the message
collector.step("user", "Tools calling operator", usr_msg)

collector.step("assistant", "Tools calling operator", resp.msgs[0].content)

print(collector.convert())

# ruff: noqa: E501
"""
{'instruction': 'You are a helpful assistantWhen is the release date of the video game Portal?', 'input': '', 'output': 'The video game "Portal" was released on October 10, 2007. It was developed by Valve Corporation and is part of the game bundle known as "The Orange Box," which also included "Half-Life 2" and its episodes.'}
2025-01-19 19:26:09,140 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
{'instruction': 'You are a helpful assistant When is the release date of the video game Portal?', 'input': '', 'output': 'The video game "Portal" was released on October 10, 2007. It was developed by Valve Corporation and is part of the game bundle known as "The Orange Box," which also included "Half-Life 2" and its episodes.'}
{'instruction': 'You are a helpful assistantWhen is the release date of the video game Portal?', 'input': '', 'output': 'The video game "Portal" was released on October 10, 2007. It was developed by 
"""



--------------------------------------------------------------------------------
# File: data_collector\sharegpt_collector.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import json

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.data_collector import ShareGPTDataCollector
from camel.models import ModelFactory
from camel.toolkits import MathToolkit
from camel.types import ModelPlatformType, ModelType

tool_list = MathToolkit().get_tools()

model_config_dict = ChatGPTConfig(
    tools=tool_list,
    temperature=0.0,
).as_dict()

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=model_config_dict,
)

# Set external_tools
agent = ChatAgent(
    system_message="You are a helpful assistant",
    model=model,
    tools=tool_list,
)
collector = ShareGPTDataCollector().record(agent).start()

# This will directly run the internal tool
response = agent.step("Call tools to calculate 17 * 19 = ?")


print(json.dumps(collector.convert(), indent=4, ensure_ascii=False))
print(json.dumps(collector.llm_convert(), indent=4, ensure_ascii=False))
print(collector.to_sharegpt_conversation(collector.convert()))

# ruff: noqa: E501
"""
{
    "system": "You are a helpful assistant",
    "tools": "[{\"name\": \"add\", \"description\": \"Adds two numbers.\", \"parameters\": {\"properties\": {\"a\": {\"type\": \"integer\", \"description\": \"The first number to be added.\"}, \"b\": {\"type\": \"integer\", \"description\": \"The second number to be added.\"}}, \"required\": [\"a\", \"b\"], \"type\": \"object\"}}, {\"name\": \"sub\", \"description\": \"Do subtraction between two numbers.\", \"parameters\": {\"properties\": {\"a\": {\"type\": \"integer\", \"description\": \"The minuend in subtraction.\"}, \"b\": {\"type\": \"integer\", \"description\": \"The subtrahend in subtraction.\"}}, \"required\": [\"a\", \"b\"], \"type\": \"object\"}}, {\"name\": \"mul\", \"description\": \"Multiplies two integers.\", \"parameters\": {\"properties\": {\"a\": {\"type\": \"integer\", \"description\": \"The multiplier in the multiplication.\"}, \"b\": {\"type\": \"integer\", \"description\": \"The multiplicand in the multiplication.\"}}, \"required\": [\"a\", \"b\"], \"type\": \"object\"}}]",
    "conversations": [
        {
            "from": "human",
            "value": "Call tools to calculate 17 * 19 = ?"
        },
        {
            "from": "function_call",
            "value": "{\"name\": \"mul\", \"arguments\": {\"a\": 17, \"b\": 19}}"
        },
        {
            "from": "observation",
            "value": "323"
        },
        {
            "from": "gpt",
            "value": "The result of \\( 17 \\times 19 \\) is \\( 323 \\)."
        }
    ]
}
{
    "system": "You are a helpful assistant",
    "tools": "[{\"name\": \"add\", \"description\": \"Adds two numbers.\", \"parameters\": {\"properties\": {\"a\": {\"type\": \"integer\", \"description\": \"The first number to be added.\"}, \"b\": {\"type\": \"integer\", \"description\": \"The second number to be added.\"}}, \"required\": [\"a\", \"b\"], \"type\": \"object\"}}, {\"name\": \"sub\", \"description\": \"Do subtraction between two numbers.\", \"parameters\": {\"properties\": {\"a\": {\"type\": \"integer\", \"description\": \"The minuend in subtraction.\"}, \"b\": {\"type\": \"integer\", \"description\": \"The subtrahend in subtraction.\"}}, \"required\": [\"a\", \"b\"], \"type\": \"object\"}}, {\"name\": \"mul\", \"description\": \"Multiplies two integers.\", \"parameters\": {\"properties\": {\"a\": {\"type\": \"integer\", \"description\": \"The multiplier in the multiplication.\"}, \"b\": {\"type\": \"integer\", \"description\": \"The multiplicand in the multiplication.\"}}, \"required\": [\"a\", \"b\"], \"type\": \"object\"}}]",
    "conversations": [
        {
            "from_": "human",
            "value": "Tools calling operator: Call tools to calculate 17 * 19 = ?"
        },
        {
            "from_": "function_call",
            "value": "{\"name\": \"mul\", \"arguments\": {\"a\": 17, \"b\": 19}}"
        },
        {
            "from_": "gpt",
            "value": "The result of \\( 17 \\times 19 \\) is \\( 323 \\)."
        }
    ]
}
root=[ShareGPTMessage(from_='system', value='You are a helpful assistant'), ShareGPTMessage(from_='human', value='Call tools to calculate 17 * 19 = ?'), ShareGPTMessage(from_='gpt', value='{"name": "multiply", "arguments": "{\'a\': 17, \'b\': 19}"}'), ShareGPTMessage(from_='human', value='"{\'result\': {\'323\'}}"'), ShareGPTMessage(from_='gpt', value='The result of \\( 17 \\times 19 \\) is \\( 323 \\).')]
"""



--------------------------------------------------------------------------------
# File: datagen\evol_instruct\evol_instruct.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import json
import logging
import os

from camel.agents import ChatAgent
from camel.datagen.evol_instruct import EvolInstructPipeline
from camel.datagen.evol_instruct.scorer import MathScorer
from camel.datagen.evol_instruct.templates import MathEvolInstructTemplates
from camel.logger import enable_logging, get_logger, set_log_level
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

os.environ["CAMEL_LOGGING_DISABLED"] = "false"


def main():
    r"""Example usage of EvolInstructPipeline for iterative and parallel
    evolution.
    """
    # Load data
    file_path = "./examples/datagen/evol_instruct/input.json"
    prompts = json.loads(open(file_path, "r", encoding="utf-8").read())

    # Define parameters
    model = ModelFactory.create(
        model_platform=ModelPlatformType.OPENAI,
        model_type=ModelType.GPT_4O_MINI,
        model_config_dict={"temperature": 0.7, "max_tokens": 4096},
    )
    agent = ChatAgent(model=model)
    num_generations = 2
    evol_spec = [
        "in-depth",
        "in-depth",
        "in-depth",
        "condense",
    ]

    # Initialize the data generation pipeline with the specified template
    pipeline = EvolInstructPipeline(
        agent=agent,
        templates=MathEvolInstructTemplates,
    )

    # Execute the data generation pipeline
    results = pipeline.generate(
        prompts=prompts,
        evolution_spec=evol_spec,
        num_iterations=4,
        num_generations=num_generations,
        scorer=MathScorer(),
    )

    # Save the generated results to a file
    results_path = "./examples/datagen/evol_instruct/results.json"
    with open(results_path, mode="w", encoding="utf-8") as file:
        json.dump(results, file, indent=4, ensure_ascii=False)

    logger.info(f"Results saved to '{results_path}'.")


if __name__ == "__main__":
    enable_logging()
    set_log_level(logging.WARNING)
    logger = get_logger("evol-instruct")
    logger.info("Begin evolution.")
    main()
    logger.info("Evolution complete.")



--------------------------------------------------------------------------------
# File: datagen\self_improving_cot\download_math_datasets.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import json
import uuid
from pathlib import Path

from datasets import load_dataset


def download_gsm8k_dataset():
    try:
        # Load the dataset using the datasets library
        dataset = load_dataset("openai/gsm8k", "main")

        # Get only 20 items from train split
        data = dataset['train'].select(range(10))

        # Convert to the desired format
        formatted_data = []
        for item in data:
            # Extract the final answer from the solution
            solution = item['answer']
            if solution:
                # GSM8K solutions typically end with "#### number"
                import re

                match = re.search(r'####\s*(\d+)', solution)
                if match:
                    number = match.group(1)
                    # Replace the "#### number" with "\boxed{number}"
                    solution = re.sub(
                        r'####\s*\d+', f'\\\\boxed{{{number}}}', solution
                    )

            formatted_item = {
                "id": str(uuid.uuid4()),  # GSM8K doesn't provide IDs
                "problem": item['question'],
                "type": "openai/gsm8k",  # All problems are from GSM8K
                "solution": solution,  # Use the modified solution with \boxed
                "evaluate_success": False,
                "boxed_answer_success": True,
                "improvement_history": [],
            }
            formatted_data.append(formatted_item)

        # Create output directory if it doesn't exist
        output_dir = Path("examples/datagen/star")
        output_dir.mkdir(exist_ok=True)

        # Save all data to a single JSON file
        output_file = output_dir / "gsm8k_dataset.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(formatted_data, f, indent=4, ensure_ascii=False)
        print(
            f"Successfully saved {len(formatted_data)} records "
            f"to {output_file}"
        )

        return formatted_data

    except Exception as e:
        print(f"Error downloading GSM8K dataset: {e}")


def download_amc_aime_dataset():
    try:
        # Load the dataset using the datasets library
        dataset = load_dataset(
            "mlfoundations-dev/bespokelabs-sky-t1-numina-amc-aime-subset-unfiltered"
        )

        # Get the first 4070 items from train split
        data = dataset['train'].select(range(4069))

        # Convert to the desired format
        formatted_data = []
        for item in data:
            formatted_item = {
                "id": str(uuid.uuid4()),
                "problem": item['problem'],
                "type": "amc_aime",
                "solution": item['ground_truth_solution'],
            }
            formatted_data.append(formatted_item)

        # Create output directory if it doesn't exist
        output_dir = Path("examples/datagen/star")
        output_dir.mkdir(exist_ok=True)

        # Save all data to a single JSON file
        output = formatted_data
        output_file = output_dir / "star_r1_output_amc_aime.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output, f, indent=4, ensure_ascii=False)
        print(
            f"Successfully saved {len(formatted_data)} records "
            f"to {output_file}"
        )

        return formatted_data

    except Exception as e:
        print(f"Error downloading AMC/AIME dataset: {e}")
        return None


if __name__ == "__main__":
    download_gsm8k_dataset()
    # download_amc_aime_dataset()



--------------------------------------------------------------------------------
# File: datagen\self_improving_cot\self_improving_cot_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import json
import os
import time

from camel.agents import ChatAgent
from camel.configs import DeepSeekConfig
from camel.datagen import SelfImprovingCoTPipeline
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

# from camel.models.reward import NemotronRewardModel

"""
please set the below os environment:
export DEEPSEEK_API_KEY=""
"""


model = ModelFactory.create(
    model_platform=ModelPlatformType.DEEPSEEK,
    model_type=ModelType.DEEPSEEK_CHAT,
    model_config_dict=DeepSeekConfig(temperature=0).as_dict(),
)


def main():
    start_time = time.time()

    current_dir = os.path.dirname(os.path.abspath(__file__))
    problems_path = os.path.join(current_dir, 'gsm8k_dataset.json')
    output_path = os.path.join(current_dir, 'self_improving_cot_output.json')

    # Load problems from JSON file
    with open(problems_path, 'r') as f:
        problems = json.load(f)

    # Initialize agent
    reason_agent_system_message = """Please reason step by step, and put your 
    final answer within \\boxed{}."""
    evaluate_agent_system_message = """You are a highly critical teacher who 
    evaluates the student's answers with a meticulous and demanding approach.
    """
    reason_agent = ChatAgent(reason_agent_system_message, model=model)
    evaluate_agent = ChatAgent(evaluate_agent_system_message)

    # Initialize reward model (optional)
    # reward_model = NemotronRewardModel(
    #     model_type=ModelType.NVIDIA_NEMOTRON_340B_REWARD,
    #     url="https://integrate.api.nvidia.com/v1",
    #     api_key=os.environ.get("NVIDIA_API_KEY"),
    # )

    # Set score thresholds for different dimensions (optional)
    score_threshold = {
        "correctness": 0.9,
        "clarity": 0.9,
        "completeness": 0.6,
    }
    # Or use a single threshold for all dimensions:
    # score_threshold = 0.9

    # Create and run pipeline
    pipeline = SelfImprovingCoTPipeline(
        reason_agent=reason_agent,
        evaluate_agent=evaluate_agent,
        problems=problems,  # Pass problems list directly
        output_path=output_path,
        max_iterations=3,
        score_threshold=score_threshold,
        # reward_model=reward_model,  # To use a reward model (optional)
    )

    results = pipeline.generate(rationalization=False)

    end_time = time.time()
    execution_time = end_time - start_time

    print(f"\nProcessed {len(results)} problems")
    print(f"Results saved to: {output_path}")
    print(f"Total execution time: {execution_time:.2f} seconds")


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: datagen\self_improving_cot\self_improving_cot_example_with_r1.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import json
import os
import time

from camel.agents import ChatAgent
from camel.datagen import SelfImprovingCoTPipeline
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

"""
please set the below os environment:
export DEEPSEEK_API_KEY=""
export GET_REASONING_CONTENT="true"
"""

evaluate_model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
)

reason_model_1 = ModelFactory.create(
    model_platform=ModelPlatformType.DEEPSEEK,
    model_type=ModelType.DEEPSEEK_REASONER,
)

reason_model_2 = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,
    model_type="accounts/fireworks/models/deepseek-r1",
    api_key=os.getenv("FIREWORKS_API_KEY"),
    url="https://api.fireworks.ai/inference/v1",
    model_config_dict={"max_tokens": 4096},
)

reason_model_3 = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,
    model_type="deepseek-ai/DeepSeek-R1",
    api_key=os.getenv("HYPERBOLIC_API_KEY"),
    url="https://api.hyperbolic.xyz/v1",
)

reason_model_4 = ModelFactory.create(
    model_platform=ModelPlatformType.TOGETHER,
    model_type="deepseek-ai/DeepSeek-R1",
    api_key=os.getenv("TOGETHER_API_KEY"),
)
# from camel.models.reward import NemotronRewardModel


def main():
    start_time = time.time()

    current_dir = os.path.dirname(os.path.abspath(__file__))
    problems_path = os.path.join(current_dir, 'outputs/gsm8k_dataset.json')
    output_path = os.path.join(
        current_dir, 'outputs/self_improving_cot_r1_output.json'
    )

    # Load problems from JSON file
    with open(problems_path, 'r') as f:
        problems = json.load(f)

    # Initialize agent
    reason_agent_system_message = """Answer my question and give your 
    final answer within \\boxed{}."""
    evaluate_agent_system_message = """You are a highly critical teacher who 
    evaluates the student's answers with a meticulous and demanding approach.
    """
    reason_agent = ChatAgent(
        system_message=reason_agent_system_message,
        model=[
            # reason_model_1,
            reason_model_2,
            # reason_model_3,
            # reason_model_4,
        ],
    )
    evaluate_agent = ChatAgent(
        system_message=evaluate_agent_system_message, model=evaluate_model
    )

    # Initialize reward model (optional)
    # reward_model = NemotronRewardModel(
    #     model_type=ModelType.NVIDIA_NEMOTRON_340B_REWARD,
    #     url="https://integrate.api.nvidia.com/v1",
    #     api_key=os.environ.get("NVIDIA_API_KEY"),
    # )

    # Set score thresholds for different dimensions (optional)
    score_threshold = {
        "correctness": 1.0,
        "clarity": 0.0,
        "completeness": 0.0,
    }
    # Or use a single threshold for all dimensions:
    # score_threshold = 0.9

    # Create and run pipeline
    pipeline = SelfImprovingCoTPipeline(
        reason_agent=reason_agent,
        evaluate_agent=evaluate_agent,
        problems=problems,  # Pass problems list directly
        output_path=output_path,
        max_iterations=0,
        score_threshold=score_threshold,
        # reward_model=reward_model,  # To use a reward model (optional)
    )

    results = pipeline.generate(rationalization=False)

    end_time = time.time()
    execution_time = end_time - start_time

    print(f"\nProcessed {len(results)} problems")
    print(f"Results saved to: {output_path}")
    print(f"Total execution time: {execution_time:.2f} seconds")


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: datagen\self_instruct\self_instruct.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.datagen.self_instruct import SelfInstructPipeline

agent = ChatAgent()

pipeline = SelfInstructPipeline(
    agent=agent,
    seed='seed_tasks.jsonl',
    num_machine_instructions=5,
    data_output_path='./data_output.json',
    human_to_machine_ratio=(6, 2),
)

pipeline.generate()



--------------------------------------------------------------------------------
# File: datagen\source2synth.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import json
import logging

from camel.datagen.source2synth.data_processor import (
    UserDataProcessor,
)
from camel.datagen.source2synth.user_data_processor_config import (
    ProcessorConfig,
)

# Configure logging
logging.basicConfig(
    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def save_results(results, output_file: str):
    r"""Save results to a JSON file."""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    logger.info(f"Results saved to: {output_file}")


def main():
    r"""Example usage."""
    # 1. Create configuration
    config = ProcessorConfig(
        seed=42,
        min_length=50,
        max_length=1000,
        complexity_threshold=0.5,
        dataset_size=10,
        use_ai_model=True,
    )

    # 2. Create the processor
    processor = UserDataProcessor(config)

    # 3. Prepare test data - texts containing multiple related information
    test_texts = [
        # Chain of technological developments
        """
        The invention of transistors revolutionized electronics in the 1950s. 
        These tiny semiconductor devices enabled the development of smaller and more 
        efficient computers. The miniaturization of computers led to the creation of 
        personal computers in the 1980s, which transformed how people work and communicate. 
        This digital revolution eventually gave rise to the internet, connecting billions 
        of people worldwide. Today, this interconnected network powers artificial 
        intelligence systems that are reshaping various industries.
        """,  # noqa: E501
        # Environmental changes causation chain
        """
        Industrial activities have significantly increased carbon dioxide emissions since 
        the Industrial Revolution. These elevated CO2 levels have enhanced the greenhouse 
        effect, trapping more heat in Earth's atmosphere. The rising global temperatures 
        have accelerated the melting of polar ice caps, which has led to rising sea levels. 
        Coastal communities are now facing increased flooding risks, forcing many to 
        consider relocation. This migration pattern is creating new challenges for urban 
        planning and resource management.
        """,  # noqa: E501
        # Biological evolution chain
        """
        The discovery of antibiotics began with Alexander Fleming's observation of 
        penicillin in 1928. The widespread use of these medications has saved countless 
        lives from bacterial infections. However, the extensive use of antibiotics has 
        led to the evolution of resistant bacteria strains. These superbugs now pose 
        a significant challenge to modern medicine, requiring the development of new 
        treatment approaches. Scientists are exploring alternative solutions like 
        bacteriophage therapy to combat antibiotic resistance.
        """,  # noqa: E501
    ]

    # 4. Process a single text
    logger.info("Processing a single text example...")
    single_result = processor.process_text(
        test_texts[0], source="technology_evolution"
    )

    # Save the single text processing result
    save_results(single_result, "single_text_results.json")

    # 5. Batch process texts
    logger.info("Batch processing texts...")
    batch_results = processor.process_batch(
        test_texts,
        sources=["tech_evolution", "climate_change", "medical_evolution"],
    )

    # Save the batch processing results
    save_results(batch_results, "batch_results.json")

    # 6. Print example results
    print("\n=== Single Text Processing Example ===")
    if single_result:
        for i, result in enumerate(single_result, 1):
            print(f"\nText {i}:")
            print(f"Source: {result['metadata']['source']}")
            print(f"Complexity: {result['metadata']['complexity']:.2f}")
            print("\nQ&A Pairs:")
            for j, qa in enumerate(result['qa_pairs'], 1):
                print(f"\nQ&A Pair {j}:")
                print(f"Type: {qa['type']}")
                print(f"Question: {qa['question']}")
                print("Reasoning Steps:")
                for step_num, step in enumerate(
                    qa.get('reasoning_steps', []), 1
                ):
                    print(f"{step_num}. {step}")
                print(f"Answer: {qa['answer']}")
                print("Supporting Facts:")
                for fact_num, fact in enumerate(
                    qa.get('supporting_facts', []), 1
                ):
                    print(f"{fact_num}. {fact}")

    print("\n=== Batch Processing Statistics ===")
    print(f"Total texts processed: {len(test_texts)}")
    pairs_generated = sum(len(result['qa_pairs']) for result in batch_results)
    print(f"Total Q&A pairs generated: {pairs_generated}")

    # 7. Analyze results
    multi_hop_qa = sum(
        1
        for result in batch_results
        for qa in result['qa_pairs']
        if qa['type'] == 'multi_hop_qa'
    )
    template_generated = sum(
        1
        for result in batch_results
        for qa in result['qa_pairs']
        if qa['type'] == 'template_generated_multi_hop'
    )

    print("\n=== Generation Statistics ===")
    print(f"AI-generated multi-hop Q&A count: {multi_hop_qa}")
    print(f"Template-generated multi-hop Q&A count: {template_generated}")

    # 8. Analyze reasoning steps
    avg_steps = sum(
        len(qa.get('reasoning_steps', []))
        for result in batch_results
        for qa in result['qa_pairs']
    ) / sum(len(result['qa_pairs']) for result in batch_results)

    print(f"\nAverage reasoning steps: {avg_steps:.2f}")

    # 9. Calculate average complexity
    avg_complexity = sum(
        result['metadata']['complexity'] for result in batch_results
    ) / len(batch_results)

    print(f"Average complexity score: {avg_complexity:.2f}")


if __name__ == "__main__":
    main()


'''
===============================================================================
Constructing examples: 100%|
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
â–ˆ| 1/1 [00:10<00:00, 10.98s/it]
Constructing examples: 100%|
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
â–ˆ| 3/3 [00:22<00:00,  7.64s/it]

=== Single Text Processing Example ===

Text 1:
Source: technology_evolution
Complexity: 0.88

Q&A Pairs:

Q&A Pair 1:
Type: multi_hop_qa
Question: How did the invention of transistors impact the development of 
personal computers?
Reasoning Steps:
1. {'step': 'Identify the role of transistors in electronics.'}
2. {'step': 'Understand how transistors enabled the miniaturization of 
computers.'}
3. {'step': 'Connect the miniaturization of computers to the creation of 
personal computers in the 1980s.'}
4. {'step': 'Determine the overall impact of personal computers on work and 
communication.'}
Answer: The invention of transistors allowed for smaller and more efficient 
computers, which led to the development of personal computers in the 1980s, 
transforming work and communication.
Supporting Facts:
1. Transistors are semiconductor devices that revolutionized electronics.
2. The miniaturization of computers was made possible by transistors.
3. Personal computers emerged in the 1980s as a result of smaller computer 
designs.
4. Personal computers changed how people work and communicate.

Q&A Pair 2:
Type: multi_hop_qa
Question: What was the sequence of developments that led from transistors to 
the internet?
Reasoning Steps:
1. {'step': 'Identify how transistors contributed to the development of 
smaller and more efficient computers.'}
2. {'step': 'Explain how the miniaturization of computers resulted in the 
creation of personal computers in the 1980s.'}
3. {'step': 'Discuss how personal computers transformed work and communication.
'}
4. {'step': 'Connect the transformation in communication to the rise of the 
internet.'}
Answer: Transistors enabled smaller computers, which led to personal computers 
in the 1980s, transforming communication and eventually giving rise to the 
internet.
Supporting Facts:
1. Transistors are tiny semiconductor devices that made computers smaller and 
more efficient.
2. The miniaturization of computers allowed for the creation of personal 
computers in the 1980s.
3. Personal computers transformed how people work and communicate.
4. The digital revolution and personal computers contributed to the rise of 
the internet, connecting billions worldwide.

Q&A Pair 3:
Type: multi_hop_qa
Question: How did the miniaturization of computers contribute to the 
development of artificial intelligence systems today?
Reasoning Steps:
1. {'step': 'Identify the impact of miniaturization on the creation of 
personal computers in the 1980s.'}
2. {'step': 'Explain how personal computers transformed communication and work.
'}
3. {'step': 'Connect the digital revolution and the rise of the internet to 
the development of artificial intelligence.'}
4. {'step': 'Discuss how the interconnected network of the internet supports 
AI systems in various industries.'}
Answer: The miniaturization of computers led to personal computers, which 
transformed communication and work, and this digital revolution, along with 
the internet, supports the development of artificial intelligence systems 
today.
Supporting Facts:
1. Miniaturization of computers enabled the creation of personal computers in 
the 1980s.
2. Personal computers transformed how people work and communicate.
3. The digital revolution led to the rise of the internet, connecting billions 
of people.
4. The internet powers artificial intelligence systems that are reshaping 
various industries.

=== Batch Processing Statistics ===
Total texts processed: 3
Total Q&A pairs generated: 9

=== Generation Statistics ===
AI-generated multi-hop Q&A count: 9
Template-generated multi-hop Q&A count: 0

Average reasoning steps: 4.00
Average complexity score: 0.90
===============================================================================
'''



--------------------------------------------------------------------------------
# File: datahubs\huggingface.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.datahubs.huggingface import HuggingFaceDatasetManager
from camel.datahubs.models import Record

manager = HuggingFaceDatasetManager()

USERNAME = "username"
REPO_ID = f"{USERNAME}/test-dataset-example"

records = [
    Record(
        id="record-1",
        content={"input": "What is AI?", "output": "Artificial Intelligence"},
        metadata={"method": "SFT"},
    ),
    Record(
        id="record-2",
        content={"input": "Translate 'hello'", "output": "Bonjour"},
        metadata={"method": "GPT"},
    ),
]

# 1. create a dataset
print("Creating dataset...")
dataset_url = manager.create_dataset(name=REPO_ID)
print(f"Dataset created: {dataset_url}")

# 2. create a dataset card
print("Creating dataset card...")
manager.create_dataset_card(
    dataset_name=REPO_ID,
    description="Test dataset",
    license="mit",
    language=["en"],
    size_category="<1MB",
    version="0.1.0",
    tags=["test", "example"],
    task_categories=["other"],
    authors=["camel-ai"],
)
print("Dataset card created successfully.")

# 3. add records to the dataset
print("Adding records to the dataset...")
manager.add_records(dataset_name=REPO_ID, records=records)
print("Records added successfully.")

# 4. list all records
print("Listing all records...")
all_records = manager.list_records(dataset_name=REPO_ID)
print("Records in the dataset:")
for record in all_records:
    print(
        f"- ID: {record.id}, Input: {record.content['input']}, "
        f"Output: {record.content['output']}"
    )

# 5. update a record
new_records = [
    Record(
        id="record-3",
        content={"input": "What is ML?", "output": "Machine Learning"},
        metadata={"method": "Updated GPT"},
    )
]
print("Updating records...")
manager.update_records(dataset_name=REPO_ID, records=new_records)
print("Records updated successfully.")

# 6. list updated records
print("Listing updated records...")
updated_records = manager.list_records(dataset_name=REPO_ID)
print("Updated records in the dataset:")
for record in updated_records:
    print(
        f"- ID: {record.id}, Input: {record.content['input']}, "
        f"Output: {record.content['output']}"
    )

# 7. delete a record
print("Deleting record with ID 'record-1'...")
manager.delete_record(dataset_name=REPO_ID, record_id="record-1")
print("Record deleted successfully.")

# 8. list records after deletion
print("Listing records after deletion...")
final_records = manager.list_records(dataset_name=REPO_ID)
print("Final records in the dataset:")
for record in final_records:
    print(
        f"- ID: {record.id}, Input: {record.content['input']}, "
        f"Output: {record.content['output']}"
    )

# 9. list all datasets
print("Listing all datasets...")
datasets = manager.list_datasets(USERNAME)
print("Datasets:")
for dataset in datasets:
    print(f"- {dataset}")

# 10. delete a dataset
print(f"Deleting dataset: {REPO_ID}...")
manager.delete_dataset(dataset_name=REPO_ID)
print("Dataset deleted successfully.")



--------------------------------------------------------------------------------
# File: dataset\few_shot_generator.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import asyncio
from pathlib import Path

from dotenv import load_dotenv

from camel.configs import ChatGPTConfig
from camel.datasets import FewShotGenerator, StaticDataset
from camel.logger import get_logger
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType
from camel.verifiers import PythonVerifier

logger = get_logger(__name__)

verifier = PythonVerifier(required_packages=["sympy"])
asyncio.run(verifier.setup())

raw_data2 = [
    {
        "question": "Evaluate the limit as x approaches 0 of (sin(3*x) - 3*x) / x**3.",  # noqa: E501
        "final_answer": "-9/2",
        "rationale": '''from sympy import symbols, limit, sin
x = symbols('x')
expr = (sin(3*x) - 3*x) / x**3
result = limit(expr, x, 0)
print(result)''',
    },
    {
        "question": "Evaluate the definite integral of (1 - x**2)**3 from x = 0 to x = 1.",  # noqa: E501
        "final_answer": "16/35",
        "rationale": '''from sympy import symbols, integrate
x = symbols('x')
expr = (1 - x**2)**3
result = integrate(expr, (x, 0, 1))
print(result)''',
    },
    {
        "question": "Evaluate the limit as n approaches infinity of n*(sqrt(n**2 + 1) - n).",  # noqa: E501
        "final_answer": "1/2",
        "rationale": '''from sympy import symbols, limit, sqrt
n = symbols('n', positive=True)
expr = n*(sqrt(n**2 + 1) - n)
result = limit(expr, n, float("inf"))
print(result)''',
    },
    {
        "question": "Compute the sum of the series sum from n = 1 to 50 of 1/(n*(n+1)).",  # noqa: E501
        "final_answer": "50/51",
        "rationale": '''from sympy import symbols, summation
n = symbols('n', positive=True, integer=True)
expr = 1/(n*(n+1))
result = summation(expr, (n, 1, 50))
print(result)''',
    },
]

seed_dataset = StaticDataset(raw_data2)


load_dotenv()


model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=ChatGPTConfig().as_dict(),
)
generator = FewShotGenerator(
    seed_dataset=seed_dataset, verifier=verifier, model=model
)

new_data = asyncio.run(generator.generate_new(n=2, max_retries=5))

for dp in new_data:
    print(dp)

generator.save_to_jsonl(Path("generated_data.jsonl"))

asyncio.run(verifier.cleanup())



--------------------------------------------------------------------------------
# File: dataset\self_instruct_generator.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import asyncio
from pathlib import Path

from dotenv import load_dotenv

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.datasets import SelfInstructGenerator, StaticDataset
from camel.logger import get_logger
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType
from camel.verifiers import PythonVerifier

logger = get_logger(__name__)

verifier = PythonVerifier(required_packages=["sympy"], timeout=60)
asyncio.run(verifier.setup())

raw_data2 = [
    {
        "question": "Evaluate the limit as x approaches 0 of (sin(3*x) - 3*x) / x**3.",  # noqa: E501
        "final_answer": "-9/2",
        "rationale": '''from sympy import symbols, limit, sin
x = symbols('x')
expr = (sin(3*x) - 3*x) / x**3
result = limit(expr, x, 0)
print(result)''',
    },
    {
        "question": "Evaluate the definite integral of (1 - x**2)**3 from x = 0 to x = 1.",  # noqa: E501
        "final_answer": "16/35",
        "rationale": '''from sympy import symbols, integrate
x = symbols('x')
expr = (1 - x**2)**3
result = integrate(expr, (x, 0, 1))
print(result)''',
    },
    {
        "question": "Evaluate the limit as n approaches infinity of n*(sqrt(n**2 + 1) - n).",  # noqa: E501
        "final_answer": "1/2",
        "rationale": '''from sympy import symbols, limit, sqrt
n = symbols('n', positive=True)
expr = n*(sqrt(n**2 + 1) - n)
result = limit(expr, n, float("inf"))
print(result)''',
    },
    {
        "question": "Compute the sum of the series sum from n = 1 to 50 of 1/(n*(n+1)).",  # noqa: E501
        "final_answer": "50/51",
        "rationale": '''from sympy import symbols, summation
n = symbols('n', positive=True, integer=True)
expr = 1/(n*(n+1))
result = summation(expr, (n, 1, 50))
print(result)''',
    },
]

seed_dataset = StaticDataset(raw_data2)

load_dotenv()

model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=ChatGPTConfig().as_dict(),
)

RATIONALE_SYSTEM_PROMPT = """You are an advanced Python code assistant.

Your task is to **solve the given question by writing Python code only**,
without any explanation or natural language output.
The code must compute the answer **programmatically**, not by hardcoding or
guessing the result.

**Rules:**
- Use Python code to perform the actual computation.
- Use sympy to solve the problem. Do not import any other libraries.
- **Do not hardcode the final answer** (e.g., avoid writing `print(1/2)` unless
  that value is computed).
- The result must be obtained through valid computation logic in code.
- Do not include explanations. Output code only.
- The entire code must be wrapped in triple backticks:
```
[Your Python code here]
```

Now, solve the following question using Python. Only output the code:
"""

rationale_agent = ChatAgent(RATIONALE_SYSTEM_PROMPT, model=model)

generator = SelfInstructGenerator(
    seed_dataset=seed_dataset,
    verifier=verifier,
    instruction_agent=None,  # use default instruction agent
    rationale_agent=rationale_agent,
)

new_data = asyncio.run(generator.generate_new(n=3, max_retries=5))

for dp in new_data:
    print(dp)

generator.save_to_jsonl(Path("generated_data.jsonl"))

asyncio.run(verifier.cleanup())



--------------------------------------------------------------------------------
# File: deductive_reasoner_agent\deduce_conditions_and_quality.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import json

from colorama import Fore

from camel.agents.deductive_reasoner_agent import DeductiveReasonerAgent


def main(model=None) -> None:
    # Construct deductive reasoner agent
    insight_agent = DeductiveReasonerAgent(model=model)

    starting_state = "The current empty website."
    target_state = "A website with search capabilities."
    conditions_and_quality = insight_agent.deduce_conditions_and_quality(
        starting_state=starting_state, target_state=target_state
    )
    print(
        Fore.GREEN
        + "Conditions and quality from the starting state:\n"
        + f"{json.dumps(conditions_and_quality, 
            indent=4, ensure_ascii=False)}",
        Fore.RESET,
    )


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: embeddings\jina_embedding_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import requests
from PIL import Image

from camel.embeddings import JinaEmbedding
from camel.types import EmbeddingModelType

# Set the text embedding instance
jina_text_embed = JinaEmbedding(
    model_type=EmbeddingModelType.JINA_EMBEDDINGS_V3,
)

# Embed the text
text_embeddings = jina_text_embed.embed_list(
    ["What is the capital of France?"]
)

print(len(text_embeddings[0]))
'''
===============================================================================
1024
===============================================================================
'''


# Set the code embedding instance
jina_code_embed = JinaEmbedding(
    model_type=EmbeddingModelType.JINA_EMBEDDINGS_V2_BASE_CODE,
    normalized=True,
)

# Embed the code
code_embeddings = jina_code_embed.embed_list(
    [
        "Calculates the square of a number. Parameters: number (int or float)"
        " - The number to square. Returns: int or float - The square of the"
        " number.",
        "This function calculates the square of a number you give it.",
        "def square(number): return number ** 2",
        "print(square(5))",
        "Output: 25",
        "Each text can be up to 8192 tokens long",
    ]
)

print(len(code_embeddings[0]))
'''
===============================================================================
768
===============================================================================
'''

# Set the clip embedding instance
jina_clip_embed = JinaEmbedding(
    model_type=EmbeddingModelType.JINA_CLIP_V2,
)

# Embed the text
text_embeddings = jina_clip_embed.embed_list(
    ["What is the capital of France?"]
)

# Set example image to embed
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image_example = Image.open(requests.get(url, stream=True).raw)

# Embed the image
image_embeddings = jina_clip_embed.embed_list([image_example])

print(len(text_embeddings[0]))
'''
===============================================================================
1024
===============================================================================
'''

print(len(image_embeddings[0]))

'''
===============================================================================
1024
===============================================================================
'''



--------------------------------------------------------------------------------
# File: embeddings\openai_compatible_embedding_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.embeddings import OpenAICompatibleEmbedding

# Set the embedding instance
nv_embed = OpenAICompatibleEmbedding(
    model_type="nvidia/nv-embed-v1",
    api_key="nvapi-xxx",
    url="https://integrate.api.nvidia.com/v1",
)

# Embed the text
text_embeddings = nv_embed.embed_list(["What is the capital of France?"])

print(len(text_embeddings[0]))
'''
===============================================================================
4096
===============================================================================
'''



--------------------------------------------------------------------------------
# File: embeddings\vlm_embedding_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import requests
from PIL import Image

from camel.embeddings import VisionLanguageEmbedding

# Set the VLM instance
VLM_instance = VisionLanguageEmbedding(
    model_name="openai/clip-vit-base-patch32"
)

# Set example image to embed
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image_example = Image.open(requests.get(url, stream=True).raw)

# Embed the image
image_embeddings = VLM_instance.embed_list([image_example])

# Set example text to embed
text_example = 'two cats laying on the sofa'

# Embed the text
text_embeddings = VLM_instance.embed_list([text_example])

# Get the length of 2 embeedings
print(len(image_embeddings[0]))
print(len(text_embeddings[0]))

'''
===============================================================================
512
512
===============================================================================
'''



--------------------------------------------------------------------------------
# File: embodiment\code_execution.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import EmbodiedAgent
from camel.generators import SystemMessageGenerator
from camel.types import RoleType


def main():
    # Create an embodied agent
    role_name = "Programmer"
    meta_dict = dict(role=role_name, task="Programming")
    sys_msg = SystemMessageGenerator().from_dict(
        meta_dict=meta_dict,
        role_tuple=(role_name, RoleType.EMBODIMENT),
    )
    embodied_agent = EmbodiedAgent(
        sys_msg,
        verbose=True,
    )
    print(embodied_agent.system_message.content)

    user_msg = (
        "Write a bash script to install numpy, "
        "then write a python script to compute "
        "the dot product of [6.75,3] and [4,5] and print the result, "
        "then write a script to open a browser and search today's weather."
    )
    response = embodied_agent.step(user_msg)
    print(response.msg.content)


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: embodiment\hugging_face_tool.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from typing import List

from camel.agents import EmbodiedAgent, HuggingFaceToolAgent
from camel.agents.tool_agents.base import BaseToolAgent
from camel.generators import SystemMessageGenerator
from camel.types import RoleType


def main():
    # Create an embodied agent
    role_name = "Artist"
    meta_dict = dict(role=role_name, task="Drawing")
    sys_msg = SystemMessageGenerator().from_dict(
        meta_dict=meta_dict,
        role_tuple=(f"{role_name}'s Embodiment", RoleType.EMBODIMENT),
    )
    tool_agents = [
        HuggingFaceToolAgent(
            'hugging_face_tool_agent',
            remote=True,
        )
    ]
    tool_agents: List[BaseToolAgent]
    embodied_agent = EmbodiedAgent(
        sys_msg,
        verbose=True,
        tool_agents=tool_agents,
    )

    user_msg = (
        "Draw all the Camelidae species, "
        "caption the image content, "
        "save the images by species name."
    )
    response = embodied_agent.step(user_msg)
    print(response.msg.content)


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: evaluation\single_agent.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import json
import os
import re
from typing import Any, Dict, List

from camel.agents import ChatAgent
from camel.prompts import PromptTemplateGenerator
from camel.types import TaskType


def parse_question_string(
    question_string: str, category: str
) -> List[Dict[str, Any]]:
    pattern = r'^(\d+)\.\s+(.*?)\s*\n*$'
    questions = []
    for match in re.finditer(pattern, question_string, re.MULTILINE):
        question_id = int(match.group(1))
        text = match.group(2)
        questions.append(
            {'question_id': question_id, 'text': text, 'category': category}
        )
    return questions


def generate_questions(
    examples: str,
    category: str,
    save_file_name: str,
    key: str = 'generate_questions',
    num_questions: int = 20,
    model=None,
) -> None:
    prompt_template = PromptTemplateGenerator().get_prompt_from_key(
        TaskType.EVALUATION, key
    )

    evaluation_dict = {
        'num_questions': num_questions,
        'category': category,
        'examples': examples,
    }

    prompt = prompt_template.format(**evaluation_dict)
    print(prompt)

    agent = ChatAgent("You are a helpful assistant.", model=model)
    agent.reset()

    assistant_response = agent.step(prompt)

    if len(assistant_response.msgs) > 0:
        print(assistant_response.msg.content)

        parsed_assistant_msg = parse_question_string(
            assistant_response.msg.content, category
        )

        # save json file
        folder_path = './evaluation_data/questions'
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)

        with open(f"{folder_path}/{save_file_name}.jsonl", "w") as f:
            for item in parsed_assistant_msg:
                json.dump(item, f, ensure_ascii=False)
                f.write('\n')


def main(model=None) -> None:
    # generate ai society evaluation questions
    examples = (
        "1. What are the most effective ways to deal with stress?\n"
        "2. Explain the process of natural selection and how it "
        "contributes to the evolution and adaptation of species.\n"
        "3. Can you help me write a formal email to a potential"
        "business partner proposing a joint venture?"
    )

    category = 'generic task Q&A'
    save_file_name = 'questions_ai_society'
    generate_questions(examples, category, save_file_name, model=model)

    # generate coding evaluation questions
    examples = (
        "1. Develop a C++ program that reads a text file line by line and"
        "counts the number of occurrences of a specific word in the file.\n"
        "2. Implement a Java function to find the longest common"
        " subsequence of two input strings using dynamic programming.\n"
        "3. Implement a machine learning-based chatbot system in Python."
    )

    category = 'coding task'
    save_file_name = 'questions_code'
    generate_questions(examples, category, save_file_name, model=model)

    # generate math evaluation questions
    examples = (
        "1. Given that f(x) = 5x^3 - 2x + 3, find the value of f(2).\n"
        "2. If the endpoints of a line segment are (2, -2) and (10, 4), "
        "what is the length of the segment?\n"
        "3. Solve for x in the equation 3x + 10 = 5(x - 2)."
    )

    category = 'math'
    save_file_name = 'questions_math'
    generate_questions(examples, category, save_file_name, model=model)

    # generate science evaluation questions
    examples = (
        "1. What is the probability of finding a particle with a given energy"
        " in a one-dimensional infinite square well potential when the"
        " potential width is 2 nm and the particle has a mass of 5x10^-26"
        " kg? Use the SchrÃ¶dinger equation to solve for the allowed energy"
        " states and their wave functions.\n"
        "2. How does habitat loss and fragmentation affect the migratory"
        " patterns and survival of a specific species, such as the monarch"
        " butterfly, over time?\n"
        "3. What is the systematic name of the organic compound with the"
        " molecular formula C6H12O and a ketone functional group located"
        " on the second carbon atom from the left end?"
    )

    category = 'science'

    save_file_name = 'questions_science'
    generate_questions(
        examples, category, save_file_name, num_questions=60, model=model
    )


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: external_tools\use_external_tools.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import MathToolkit, SearchToolkit
from camel.types import ModelPlatformType, ModelType


def main():
    # Set the tools for the external_tools
    internal_tools = SearchToolkit().get_tools()
    external_tools = MathToolkit().get_tools()
    tool_list = internal_tools + external_tools

    model_config_dict = ChatGPTConfig(
        tools=tool_list,
        temperature=0.0,
    ).as_dict()

    model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
        model_config_dict=model_config_dict,
    )

    # Set external_tools
    external_tool_agent = ChatAgent(
        system_message="You are a helpful assistant",
        model=model,
        tools=internal_tools,
        external_tools=external_tools,
    )

    # This will directly run the internal tool
    response = external_tool_agent.step(
        "When is the release date of the video game Portal?"
    )
    print(response.msg.content)

    usr_msg = "What's the result of the release year of Portal subtracted by"
    "the year that United States was founded?"
    # This will first automatically run the internal tool to check the years
    # Then it will request the external tool to calculate the sum
    response = external_tool_agent.step(usr_msg)
    # This should be empty
    print(response.msg.content)
    external_tool_request = response.info["external_tool_request"]
    # This will print the info of the external tool request
    print(external_tool_request)


if __name__ == "__main__":
    main()


# flake8: noqa :E501
"""
Output:
The video game "Portal" was released in 2007 as part of a bundle called The Orange Box for Windows, Xbox 360, and PlayStation 3.

ChatCompletionMessageToolCall(id='call_U5Xju7vYtAQAEW4D1M8R1kgs', function=Function(arguments='{"a": 2007, "b": 1776}', name='sub'), type='function')
"""



--------------------------------------------------------------------------------
# File: extractors\python_strategies_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import ast
import asyncio

from camel.extractors.base import BaseExtractor
from camel.extractors.python_strategies import (
    BoxedStrategy,
    PythonDictStrategy,
    PythonListStrategy,
)
from camel.logger import get_logger

logger = get_logger(__name__)


def create_list_extractor(
    _cache_templates=True,
    max_cache_size=5000,
    extraction_timeout=60.0,
    _batch_size=20,
    _monitoring_interval=10.0,
    _cpu_threshold=90.0,
    _memory_threshold=90.0,
) -> "BaseExtractor":
    r"""Create an extractor for Python lists."""
    # Create a pipeline with two stages
    pipeline = [
        [BoxedStrategy()],  # Stage 1: Extract boxed content
        [PythonListStrategy()],  # Stage 2: Extract and normalize Python list
    ]

    return BaseExtractor(
        pipeline=pipeline,
        cache_templates=_cache_templates,
        max_cache_size=max_cache_size,
        extraction_timeout=extraction_timeout,
        batch_size=_batch_size,
        monitoring_interval=_monitoring_interval,
        cpu_threshold=_cpu_threshold,
        memory_threshold=_memory_threshold,
        default_value="[]",
    )


def create_dict_extractor(
    _cache_templates=True,
    max_cache_size=5000,
    extraction_timeout=60.0,
    _batch_size=20,
    _monitoring_interval=10.0,
    _cpu_threshold=90.0,
    _memory_threshold=90.0,
) -> "BaseExtractor":
    r"""Create an extractor for Python dictionaries."""
    # Create a pipeline with two stages
    pipeline = [
        [BoxedStrategy()],  # Stage 1: Extract boxed content
        [PythonDictStrategy()],  # Stage 2: Extract and normalize Python dict
    ]

    return BaseExtractor(
        pipeline=pipeline,
        cache_templates=_cache_templates,
        max_cache_size=max_cache_size,
        extraction_timeout=extraction_timeout,
        batch_size=_batch_size,
        monitoring_interval=_monitoring_interval,
        cpu_threshold=_cpu_threshold,
        memory_threshold=_memory_threshold,
        default_value="{}",
    )


async def example_list_extraction():
    r"""Demonstrate list extraction."""
    print("\n=== Example 1: List extraction ===")

    # Example LLM response with a list in \boxed{} format
    llm_response = r"\boxed{[3, 1, 2, 'apple']}"

    # Create a list extractor
    extractor = create_list_extractor()

    # Set up the extractor
    await extractor.setup()

    try:
        # Extract the list
        result = await extractor.extract(llm_response)

        print(f"LLM Response: {llm_response}")
        print(f"Extracted list string: {result}")

        # Parse the result to get the actual list
        try:
            parsed_list = ast.literal_eval(result)
            print(f"Parsed list: {parsed_list}")
        except (SyntaxError, ValueError) as e:
            print(f"Error parsing result: {e}")

    finally:
        # Clean up the extractor
        await extractor.cleanup()


async def example_dict_extraction():
    r"""Demonstrate dictionary extraction."""
    print("\n=== Example 2: Dictionary extraction ===")

    # Example LLM response with a dictionary in \boxed{} format
    llm_response = r"\boxed{{'apple': 5, 'banana': 3, 'cherry': 8}}"

    # Create a dictionary extractor
    extractor = create_dict_extractor()

    # Set up the extractor
    await extractor.setup()

    try:
        # Extract the dictionary
        result = await extractor.extract(llm_response)

        print(f"LLM Response: {llm_response}")
        print(f"Extracted dictionary string: {result}")

        # Parse the result to get the actual dictionary
        try:
            parsed_dict = ast.literal_eval(result)
            print(f"Parsed dictionary: {parsed_dict}")
        except (SyntaxError, ValueError) as e:
            print(f"Error parsing result: {e}")

    finally:
        # Clean up the extractor
        await extractor.cleanup()


async def main():
    r"""Run all examples."""
    print("=== Python Strategies Examples ===")

    await example_list_extraction()
    await example_dict_extraction()

    print("\nAll examples completed.")


if __name__ == "__main__":
    asyncio.run(main())

"""
===============================================================================
=== Python Strategies Examples ===

=== Example 1: List extraction ===
LLM Response: \boxed{[3, 1, 2, 'apple']}
Extracted list string: [1, 2, 3, 'apple']
Parsed list: [1, 2, 3, 'apple']

=== Example 2: Dictionary extraction ===
LLM Response: \boxed{{'apple': 5, 'banana': 3, 'cherry': 8}}
Extracted dictionary string: {'apple': 5, 'banana': 3, 'cherry': 8}
Parsed dictionary: {'apple': 5, 'banana': 3, 'cherry': 8}

All examples completed.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: files_log.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import os

from camel.logger import get_logger, set_log_file, set_log_level

# Set log output to a file using an absolute path
log_path = os.path.join(
    os.path.dirname(os.path.abspath(__file__)),
    'camel.log',
)
set_log_file(log_path)

# Set the logging level
set_log_level('DEBUG')

# Use the logger
logger = get_logger(__name__)
logger.debug('This is a debug message')
logger.info('This is an info message')
logger.warning('This is a warning message')



--------------------------------------------------------------------------------
# File: generate_text_embedding_data\single_agent.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import json
import os
import random

from camel.agents import ChatAgent
from camel.configs.openai_config import ChatGPTConfig
from camel.generators import SystemMessageGenerator
from camel.models import ModelFactory
from camel.types import (
    ModelPlatformType,
    ModelType,
    RoleType,
    TaskType,
)

QUERY_TYPE_LIST = ["extremely long-tail", "long-tail", "common"]
QUERY_LENGTH_LIST = ["less than 5 words", "5 to 15 words", "at least 10 words"]
CLARITY_LIST = ["clear", "understandable with some effort", "ambiguous"]
NUM_WORDS_LIST = ["50", "100", "200", "300", "400", "500"]
DIFFICULTY_LIST = ["high school", "college", "PhD"]
DEFAULT_LANGUAGE = "English"

random.seed(42)


def main() -> None:
    with open("./text_embedding_data/tasks/tasks.txt", "r") as file:
        tasks = file.readlines()
        tasks = [task.replace("\n", "") for task in tasks]

    sys_msg_generator = SystemMessageGenerator(
        task_type=TaskType.GENERATE_TEXT_EMBEDDING_DATA
    )
    for i, task in enumerate(tasks):
        query_type = random.choice(QUERY_TYPE_LIST)
        query_length = random.choice(QUERY_LENGTH_LIST)
        clarity = random.choice(CLARITY_LIST)
        num_words = random.choice(NUM_WORDS_LIST)
        difficulty = random.choice(DIFFICULTY_LIST)
        assistant_sys_msg = sys_msg_generator.from_dict(
            meta_dict=dict(
                task=task,
                query_type=query_type,
                query_length=query_length,
                clarity=clarity,
                num_words=num_words,
                difficulty=difficulty,
            ),
            role_tuple=("Text retrieval example writer:", RoleType.ASSISTANT),
        )

        model = ModelFactory.create(
            model_platform=ModelPlatformType.DEFAULT,
            model_type=ModelType.DEFAULT,
            model_config_dict=ChatGPTConfig(
                temperature=0.0, response_format={"type": "json_object"}
            ).as_dict(),
        )

        assistant_agent = ChatAgent(
            system_message=assistant_sys_msg,
            model=model,
        )
        print(f"Generating positive and negative documents for '{task}'")
        assistant_response = assistant_agent.step("Start to generate!")
        content = assistant_response.msg.content
        try:
            data = json.loads(content)
            os.makedirs("./text_embedding_data/tasks/", exist_ok=True)
            with open(f"./text_embedding_data/tasks/{i}.json", "w") as f:
                json.dump(data, f, indent=4, ensure_ascii=False)
        except Exception as e:
            print(f"Error raised during generation of task {task}", e)


if __name__ == "__main__":
    main()

# flake8: noqa :E501
"""
===============================================================================
{
    "user_query": "Fall of Berlin Wall",
    "positive_document": "The fall of the Berlin Wall on November 9, 1989, marked a pivotal moment in world history, symbolizing the end of the Cold War and the beginning of a new era of European integration. The Wall, which had divided East and West Berlin since 1961, was a stark representation of the ideological divide between the communist East and the capitalist West. Its fall was precipitated by a series of events, including the liberalization policies of Soviet leader Mikhail Gorbachev, the rise of pro-democracy movements in Eastern Europe, and the increasing pressure from East German citizens who demanded freedom and reform. On the evening of November 9, an announcement by East German official G\u00fcnter Schabowski mistakenly suggested that the border was open, leading to a spontaneous and massive gathering of East Berliners at the Wall. Overwhelmed, the border guards eventually allowed people to pass through, and jubilant crowds began to dismantle the Wall piece by piece. The fall of the Berlin Wall not only reunited families and friends who had been separated for decades but also paved the way for the reunification of Germany on October 3, 1990. It was a moment of profound joy and relief, but also one of significant challenges, as the newly unified Germany had to address economic disparities and social integration issues. The event had far-reaching implications, contributing to the collapse of communist regimes across Eastern Europe and the eventual dissolution of the Soviet Union in 1991. The fall of the Berlin Wall remains a powerful symbol of the triumph of freedom and democracy over oppression and totalitarianism.",
    "hard_negative_document": "The Berlin Wall, constructed in 1961, was a concrete barrier that physically and ideologically divided Berlin into East and West. It was erected by the German Democratic Republic (GDR) to prevent East Germans from fleeing to the West. The Wall was a prominent symbol of the Cold War, representing the division between the communist Eastern Bloc and the Western democracies. Over the years, the Wall saw numerous escape attempts, some successful and many tragically fatal. The Wall was heavily guarded, with watchtowers, anti-vehicle trenches, and a 'death strip' that made crossing extremely dangerous. The construction of the Wall was a response to the mass exodus of East Germans to the West, which threatened the stability of the GDR. The Wall's existence was a constant reminder of the lack of freedom and the oppressive nature of the East German regime. Despite its grim purpose, the Wall also became a canvas for artistic expression, with graffiti and murals covering its western side. The Wall stood for 28 years, until its fall in 1989, which was a result of mounting political pressure and the liberalization policies of Soviet leader Mikhail Gorbachev. The fall of the Wall was a significant event in world history, leading to the reunification of Germany and the end of the Cold War. Today, remnants of the Wall serve as a historical reminder of the division and the eventual triumph of freedom and unity."
}
{
    "user_query": "chronic elbow pain",
    "positive_document": "Chronic elbow pain can be a debilitating condition that affects daily activities and overall quality of life. There are several potential causes for chronic elbow pain, including repetitive strain injuries, arthritis, and nerve compression. Repetitive strain injuries, such as tennis elbow or golfer's elbow, are common among athletes and individuals who perform repetitive tasks. These conditions result from overuse of the muscles and tendons around the elbow, leading to inflammation and pain. Arthritis, particularly osteoarthritis, can also cause chronic elbow pain. This degenerative joint disease leads to the breakdown of cartilage, causing pain and stiffness in the elbow joint. Nerve compression, such as cubital tunnel syndrome, occurs when the ulnar nerve is compressed at the elbow, leading to pain, numbness, and tingling in the arm and hand. Treatment for chronic elbow pain depends on the underlying cause. Rest, ice, and anti-inflammatory medications are often recommended for initial management. Physical therapy can help strengthen the muscles around the elbow and improve flexibility. In some cases, corticosteroid injections may be used to reduce inflammation. For severe cases, surgical intervention may be necessary to repair damaged tissues or relieve nerve compression. Patient experiences with chronic elbow pain vary, but many report significant improvement with a combination of treatments. It is important to consult with a healthcare professional for an accurate diagnosis and appropriate treatment plan.",
    "hard_negative_document": "Elbow pain is a common complaint that can result from a variety of causes. Acute elbow pain is often due to injuries such as fractures, dislocations, or sprains. These injuries typically occur from falls, direct blows, or overuse. Symptoms of acute elbow injuries include sudden pain, swelling, and limited range of motion. Immediate treatment for acute elbow pain includes rest, ice, compression, and elevation (RICE). Over-the-counter pain relievers can also help manage pain and inflammation. In some cases, medical intervention may be required to realign bones or repair torn ligaments. Chronic elbow pain, on the other hand, may develop over time due to conditions such as tendinitis, bursitis, or nerve entrapment. Tendinitis, also known as tennis elbow or golfer's elbow, is caused by inflammation of the tendons around the elbow. Bursitis is the inflammation of the bursa, a fluid-filled sac that cushions the elbow joint. Nerve entrapment, such as cubital tunnel syndrome, occurs when nerves are compressed, leading to pain and numbness. Treatment for chronic elbow pain often involves a combination of rest, physical therapy, and medications. In some cases, surgery may be necessary to address the underlying issue. It is important to seek medical advice for a proper diagnosis and treatment plan tailored to the individual's condition."
}
{
    "user_query": "How has the development of quantum computing influenced cryptographic methods and what are the potential societal impacts?",
    "positive_document": "Quantum computing represents a paradigm shift in computational capabilities, leveraging principles of quantum mechanics such as superposition and entanglement. This has profound implications for cryptography, particularly in the context of breaking traditional encryption methods like RSA and ECC. Quantum algorithms, notably Shor's algorithm, can factorize large integers exponentially faster than classical algorithms, rendering many current cryptographic systems vulnerable. Consequently, there is a significant push towards developing quantum-resistant cryptographic methods, such as lattice-based, hash-based, and multivariate polynomial cryptography. The societal impacts of quantum computing extend beyond cryptography, potentially revolutionizing fields such as drug discovery, materials science, and complex system simulations. However, the transition to quantum-resistant cryptography is critical to ensure data security in a post-quantum world, necessitating substantial research and development efforts.",
    "hard_negative_document": "Quantum computing has been a topic of interest for decades, with theoretical foundations laid by pioneers like Richard Feynman and David Deutsch. The field has seen significant advancements, particularly with the development of quantum bits or qubits, which can exist in multiple states simultaneously. This capability allows quantum computers to solve certain problems much faster than classical computers. However, the practical implementation of quantum computing faces numerous challenges, including error rates and qubit coherence times. While the potential applications are vast, ranging from optimization problems to machine learning, the current state of quantum computing is still in its infancy, with fully functional, large-scale quantum computers yet to be realized. The societal impacts are speculative at this stage, as the technology is not yet mature enough to be widely adopted."
}
{
    "user_query": "Battle of Thermopylae strategies",
    "positive_document": "The Battle of Thermopylae, fought in 480 BC, is renowned for the strategic brilliance of the Greek forces, particularly the Spartans led by King Leonidas. The Greeks chose the narrow pass of Thermopylae to counter the numerical superiority of the Persian army. This terrain limited the effectiveness of the Persian cavalry and forced the Persians to engage in close combat, where the heavily armored Greek hoplites had an advantage. The Greeks also utilized a phalanx formation, which was highly effective in the confined space. Despite being vastly outnumbered, the Greek forces managed to hold off the Persians for three days, showcasing their tactical ingenuity and the importance of terrain in military strategy.",
    "hard_negative_document": "The Battle of Thermopylae is one of the most famous battles in ancient history, taking place in 480 BC during the Persian Wars. The Greek forces, led by King Leonidas of Sparta, faced a much larger Persian army under King Xerxes. Despite their valiant efforts, the Greeks were ultimately defeated. The battle has been immortalized in various works of art and literature, symbolizing the courage and sacrifice of the outnumbered Greek warriors. The story of the 300 Spartans has become a legendary tale of heroism and resistance against overwhelming odds."
}
{
    "user_query": "What are the potential causes and treatments for persistent unilateral facial numbness accompanied by occasional dizziness?",
    "positive_document": "Persistent unilateral facial numbness can be indicative of several underlying conditions, ranging from benign to serious. One potential cause is trigeminal neuralgia, a chronic pain condition affecting the trigeminal nerve in the face. Another possibility is multiple sclerosis, an autoimmune disease that affects the central nervous system. Additionally, a stroke or transient ischemic attack (TIA) could present with these symptoms. Diagnostic imaging, such as MRI or CT scans, is often required to determine the exact cause. Treatment options vary depending on the diagnosis but may include medications like anticonvulsants for trigeminal neuralgia, corticosteroids for multiple sclerosis, or anticoagulants for stroke prevention. In some cases, surgical interventions may be necessary. Patient experiences with these conditions can vary widely, with some reporting significant relief from medications while others may require more invasive treatments.",
    "hard_negative_document": "Facial numbness can be a symptom of various conditions, including dental issues, infections, or nerve damage. Dental problems such as abscesses or impacted teeth can cause localized numbness. Infections like herpes zoster (shingles) can also lead to facial numbness, often accompanied by a rash. Nerve damage from trauma or surgery is another potential cause. Treatment typically involves addressing the underlying issue, such as antibiotics for infections or dental procedures for tooth-related problems. Over-the-counter pain relievers and topical anesthetics may provide temporary relief. It's important to consult a healthcare provider for a proper diagnosis and treatment plan."
}
{
    "user_query": "Exploration of quantum dot solar cells and their potential impact on renewable energy sectors",
    "positive_document": "Quantum dot solar cells (QDSCs) represent a significant advancement in photovoltaic technology, leveraging the unique properties of quantum dots to enhance solar energy conversion efficiency. Quantum dots are semiconductor particles only a few nanometers in size, which exhibit quantum mechanical properties. These properties allow for the tuning of the bandgap by simply changing the size of the quantum dots, enabling the absorption of a broader spectrum of sunlight compared to traditional silicon-based solar cells. This tunability is a key factor in the potential efficiency improvements offered by QDSCs. Research has shown that QDSCs can achieve higher theoretical efficiencies due to multiple exciton generation (MEG), where a single high-energy photon can generate multiple electron-hole pairs. This contrasts with conventional solar cells, where one photon typically generates one electron-hole pair, thus limiting the maximum efficiency. The development of QDSCs involves sophisticated fabrication techniques, including colloidal synthesis and layer-by-layer assembly, to create uniform and defect-free quantum dot films. These films are then integrated into various device architectures, such as Schottky junctions, p-n junctions, and tandem cells, each offering different pathways to optimize performance. The potential impact of QDSCs on the renewable energy sector is profound. By increasing the efficiency and reducing the cost of solar energy, QDSCs could accelerate the adoption of solar power, contributing significantly to global efforts to reduce carbon emissions and combat climate change. Furthermore, the flexibility in the design and application of QDSCs opens up new possibilities for integrating solar cells into a variety of surfaces and materials, including building-integrated photovoltaics (BIPV) and portable electronic devices. Despite the promising prospects, several challenges remain in the commercialization of QDSCs. Stability and longevity of the quantum dot materials under operational conditions are critical issues that need to be addressed. Additionally, the environmental impact of the materials used in QDSCs, such as lead-based quantum dots, requires careful consideration and the development of safer alternatives. Ongoing research is focused on overcoming these hurdles, with significant progress being made in the synthesis of more stable and environmentally friendly quantum dots. In conclusion, quantum dot solar cells hold great promise for the future of renewable energy, offering the potential for higher efficiency, lower costs, and versatile applications. Continued advancements in this field could play a crucial role in the transition to a sustainable energy future.",
    "hard_negative_document": "The field of renewable energy has seen numerous technological advancements over the past few decades, with solar energy being one of the most prominent areas of development. Traditional silicon-based solar cells have dominated the market due to their relatively high efficiency and established manufacturing processes. However, researchers are continually exploring new materials and technologies to further improve the performance and reduce the costs of solar cells. One such area of research is the development of perovskite solar cells. Perovskite materials have shown great promise due to their high absorption coefficients, tunable bandgaps, and ease of fabrication. These materials can be processed using low-cost techniques such as spin-coating and printing, making them attractive for large-scale production. Perovskite solar cells have achieved remarkable efficiency gains in a relatively short period, with some laboratory-scale devices reaching efficiencies comparable to those of silicon-based cells. The potential for tandem solar cells, which combine perovskite and silicon layers, offers a pathway to surpass the efficiency limits of single-junction cells. Despite these advancements, perovskite solar cells face several challenges that need to be addressed before they can be widely commercialized. Stability and degradation under environmental conditions, such as moisture and UV exposure, are significant concerns. Additionally, the use of lead in many perovskite formulations raises environmental and health issues that must be mitigated. Researchers are actively working on developing more stable and lead-free perovskite materials to overcome these challenges. The impact of perovskite solar cells on the renewable energy sector could be substantial, offering a complementary technology to existing silicon-based systems. By enabling higher efficiencies and potentially lower costs, perovskite solar cells could accelerate the adoption of solar energy and contribute to the global transition to sustainable energy sources. In addition to perovskite solar cells, other emerging technologies such as organic photovoltaics (OPVs) and dye-sensitized solar cells (DSSCs) are also being explored. Each of these technologies has its own set of advantages and challenges, and ongoing research is focused on optimizing their performance and addressing any limitations. The future of solar energy is likely to be shaped by a combination of these innovative technologies, each contributing to the overall goal of increasing the efficiency and accessibility of solar power. As the renewable energy landscape continues to evolve, the integration of these new technologies into existing energy systems will be crucial for achieving a sustainable and resilient energy future."
}
===============================================================================
"""



--------------------------------------------------------------------------------
# File: generate_text_embedding_data\task_generation.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import os

from camel.agents import ChatAgent
from camel.configs.openai_config import ChatGPTConfig
from camel.generators import PromptTemplateGenerator
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType, TaskType


def main() -> None:
    num_generate = 2
    num_tasks = 3
    prompt_template = PromptTemplateGenerator().get_prompt_from_key(
        TaskType.GENERATE_TEXT_EMBEDDING_DATA, "generate_tasks"
    )
    evaluation_dict = dict(num_tasks=num_tasks)
    prompt = prompt_template.format(**evaluation_dict)
    print(prompt)

    model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
        model_config_dict=ChatGPTConfig(temperature=0.0).as_dict(),
    )
    agent = ChatAgent(
        "You are a helpful text retrieval task generator.",
        model=model,
    )

    total_tasks = []
    for _ in range(num_generate):
        agent.reset()
        assistant_response = agent.step(prompt)
        assistant_content = assistant_response.msg.content
        # Split tasks string to a list of tasks:
        tasks = assistant_content.split("\n")
        # Remove the start token such as "1. ":
        tasks = [task.split('. ')[1] for task in tasks]
        total_tasks = total_tasks + tasks

    os.makedirs("./text_embedding_data/tasks/", exist_ok=True)
    with open("./text_embedding_data/tasks/tasks.txt", "w") as file:
        file.write("\n".join(total_tasks))


if __name__ == "__main__":
    main()

# flake8: noqa :E501
"""
===============================================================================
Provided a historical event as a query, retrieve documents that offer different perspectives and analyses of the event.
Given a medical symptom as a query, retrieve documents that discuss potential diagnoses, treatments, and patient experiences.
Provided a technological innovation as a query, retrieve documents that explore its development, applications, and societal impact.
Given a historical event as a query, retrieve documents that provide different perspectives and analyses of the event.
Provided a medical symptom as a query, retrieve documents that discuss potential diagnoses, treatments, and patient experiences related to the symptom.
Given a technological innovation as a query, retrieve documents that explore its development, applications, and impact on various industries.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: interpreters\internal_python_interpreter.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.interpreters import InternalPythonInterpreter

safe_interpreter = InternalPythonInterpreter()
unsafe_interpreter = InternalPythonInterpreter(unsafe_mode=True)

safe_result = safe_interpreter.execute(
    "x = input_variable", state={"input_variable": 10}
)
print(safe_result)

'''
===============================================================================
10
===============================================================================
'''

unsafe_result = unsafe_interpreter.run(
    "[x * 2 for x in range(3)]", code_type="python"
)
print(unsafe_result)

'''
===============================================================================
[0, 2, 4]
===============================================================================
'''



--------------------------------------------------------------------------------
# File: interpreters\ipython_interpreter_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.interpreters import JupyterKernelInterpreter

interpreter = JupyterKernelInterpreter(
    require_confirm=False, print_stdout=True, print_stderr=True
)


code = """
def add(a, b):
    return a + b
    
def multiply(a, b):
    return a * b

def subtract(a, b):
    return a - b

def main():
    a = 10
    b = 20
    operation = subtract
    result = operation(a, b)
    print(result)
    
if __name__ == "__main__":
    main()
"""
result = interpreter.run(code, "python")
print(result)

'''
===============================================================================
-10
===============================================================================
'''



--------------------------------------------------------------------------------
# File: knowledge_graph\knowledge_graph_agent_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from dotenv import load_dotenv

from camel.agents import KnowledgeGraphAgent
from camel.loaders import UnstructuredIO

load_dotenv()

# Set instance
uio = UnstructuredIO()
kg_agent = KnowledgeGraphAgent()

# Set example text input
text_example = """CAMEL-AI.org is an open-source community dedicated to the 
study of autonomous and communicative agents. 
"""

# Create an element from given text
element_example = uio.create_element_from_text(text=text_example)

# Let KnowledgeGraph Agent extract node and relationship information
ans_str = kg_agent.run(element_example, parse_graph_elements=False)
ans_GraphElement = kg_agent.run(element_example, parse_graph_elements=True)

# Get str output
print(ans_str)

# Get GraphElement output
print(ans_GraphElement)

"""
===============================================================================
Nodes:

Node(id='CAMEL-AI.org', type='Organization', properties={'agent_generated'})
Node(id='community', type='Concept', properties={'agent_generated'})
Node(id='study', type='Concept', properties={'agent_generated'})
Node(id='autonomous agents', type='Concept', properties={'agent_generated'})
Node(id='communicative agents', type='Concept', properties={'agent_generated'})

Relationships:

Relationship(subj=Node(id='CAMEL-AI.org', type='Organization'), obj=Node
(id='community', type='Concept'), type='FocusOn', properties=
{'agent_generated'})
Relationship(subj=Node(id='CAMEL-AI.org', type='Organization'), obj=Node
(id='study', type='Concept'), type='FocusOn', properties={'agent_generated'})
Relationship(subj=Node(id='CAMEL-AI.org', type='Organization'), obj=Node
(id='autonomous agents', type='Concept'), type='FocusOn', properties=
{'agent_generated'})
Relationship(subj=Node(id='CAMEL-AI.org', type='Organization'), obj=Node
(id='communicative agents', type='Concept'), type='FocusOn', properties=
{'agent_generated'})
===============================================================================
"""

"""
===============================================================================
GraphElement(nodes=[Node(id='CAMEL-AI.org', type='Organization', properties=
{'agent_generated'}), Node(id='community', type='Concept', properties=
{'agent_generated'}), Node(id='study', type='Concept', properties=
{'agent_generated'}), Node(id='autonomous agents', type='Concept', properties=
{'agent_generated'}), Node(id='communicative agents', type='Concept', 
properties={'agent_generated'})], relationships=[Relationship(subj=Node
(id='CAMEL-AI.org', type='Organization', properties={'agent_generated'}), 
obj=Node(id='community', type='Concept', properties={'agent_generated'}), 
type='FocusOn', properties={"'agent_generated'"}), Relationship(subj=Node
(id='CAMEL-AI.org', type='Organization', properties={'agent_generated'}), 
obj=Node(id='study', type='Concept', properties={'agent_generated'}), 
type='FocusOn', properties={"'agent_generated'"}), Relationship(subj=Node
(id='CAMEL-AI.org', type='Organization', properties={'agent_generated'}), 
obj=Node(id='autonomous agents', type='Concept', properties=
{'agent_generated'}), type='FocusOn', properties={"'agent_generated'"}), 
Relationship(subj=Node(id='CAMEL-AI.org', type='Organization', properties=
{'agent_generated'}), obj=Node(id='communicative agents', type='Concept', 
properties={'agent_generated'}), type='FocusOn', properties=
{"'agent_generated'"})], source=<unstructured.documents.elements.Text object 
at 0x7fd050e7bd90>)
===============================================================================
"""


custom_prompt = """
You are tasked with extracting nodes and relationships from given content and 
structures them into Node and Relationship objects. Here's the outline of what 
you needs to do:

Content Extraction:
You should be able to process input content and identify entities mentioned 
within it.
Entities can be any noun phrases or concepts that represent distinct entities 
in the context of the given content.

Node Extraction:
For each identified entity, you should create a Node object.
Each Node object should have a unique identifier (id) and a type (type).
Additional properties associated with the node can also be extracted and 
stored.

Relationship Extraction:
You should identify relationships between entities mentioned in the content.
For each relationship, create a Relationship object.
A Relationship object should have a subject (subj) and an object (obj) which 
are Node objects representing the entities involved in the relationship.
Each relationship should also have a type (type), and additional properties if 
applicable.
**New Requirement:** 
Each relationship must have a timestamp representing the time the relationship 
was established or mentioned.

Output Formatting:
The extracted nodes and relationships should be formatted as instances of the 
provided Node and Relationship classes.
Ensure that the extracted data adheres to the structure defined by the classes.
Output the structured data in a format that can be easily validated against 
the provided code.

Instructions for you:
Read the provided content thoroughly.
Identify distinct entities mentioned in the content and categorize them as 
nodes.
Determine relationships between these entities and represent them as directed 
relationships, including a timestamp for each relationship.
Provide the extracted nodes and relationships in the specified format below.
Example for you:

Example Content:
"John works at XYZ Corporation since 2020. He is a software engineer. The 
company is located in New York City."

Expected Output:

Nodes:

Node(id='John', type='Person')
Node(id='XYZ Corporation', type='Organization')
Node(id='New York City', type='Location')

Relationships:

Relationship(subj=Node(id='John', type='Person'), obj=Node(id='XYZ 
Corporation', type='Organization'), type='WorksAt', timestamp='1717193166')
Relationship(subj=Node(id='John', type='Person'), obj=Node(id='New York City', 
type='Location'), type='ResidesIn', timestamp='1719700236')

===== TASK =====
Please extracts nodes and relationships from given content and structures them 
into Node and Relationship objects. 

{task}
"""


ans_custom_str = kg_agent.run(
    element_example, parse_graph_elements=False, prompt=custom_prompt
)
ans_custom_GraphElement = kg_agent.run(
    element_example, parse_graph_elements=True, prompt=custom_prompt
)


# Get custom str output
print(ans_custom_str)

# Get custom GraphElement output
print(ans_custom_GraphElement)

"""
===============================================================================
### Nodes:

1. Node(id='CAMEL-AI.org', type='Organization')
2. Node(id='open-source community', type='Community')
3. Node(id='autonomous agents', type='Concept')
4. Node(id='communicative agents', type='Concept')

### Relationships:

1. Relationship(subj=Node(id='CAMEL-AI.org', type='Organization'), 
obj=Node(id='open-source community', type='Community'), type='IsPartOf',
timestamp='1717193166')
2. Relationship(subj=Node(id='open-source community', type='Community'), 
obj=Node(id='autonomous agents', type='Concept'), type='Studies',
timestamp='1719700236')
3. Relationship(subj=Node(id='open-source community', type='Community'), 
obj=Node(id='communicative agents', type='Concept'), type='Studies',
timestamp='1719700236')
===============================================================================
"""

"""
===============================================================================
nodes=[
Node(id='CAMEL-AI.org', type='Organization',
properties={'source': 'agent_created'}), 
Node(id='open-source community', type='Community',
properties={'source': 'agent_created'}), 
Node(id='autonomous agents', type='Concept',
properties={'source': 'agent_created'}), 
Node(id='communicative agents', type='Concept',
properties={'source': 'agent_created'})] 
relationships=[Relationship(subj=Node(id='CAMEL-AI.org', type='Organization', 
properties={'source': 'agent_created'}), 
obj=Node(id='open-source community', type='Community',
properties={'source': 'agent_created'}),
type="IsA', timestamp='1717193166", properties={'source': 'agent_created'}), 
Relationship(subj=Node(id='open-source community', type='Community', 
properties={'source': 'agent_created'}), 
obj=Node(id='autonomous agents', type='Concept', 
properties={'source': 'agent_created'}), type="Studies',
timestamp='1719700236", 
properties={'source': 'agent_created'}),
Relationship(subj=Node(id='open-source community', type='Community', 
properties={'source': 'agent_created'}), 
obj=Node(id='communicative agents', type='Concept', 
properties={'source': 'agent_created'}), 
type="Studies', timestamp='1719700236",
properties={'source': 'agent_created'})] 
source=<unstructured.documents.elements.Text object at 0x7f1583ee7d30>
===============================================================================
"""



--------------------------------------------------------------------------------
# File: knowledge_graph\neo4j_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.storages import Neo4jGraph

# Set Neo4j instance
n4j = Neo4jGraph(
    url="Your Url", username="Your Username", password="Your Password"
)

# Add triplet into database
n4j.add_triplet(subj="CAMEL", obj="multi-agent framework", rel="belongs to")

# Run a Cypher query
print(n4j.query("""MATCH (n) RETURN n AS node"""))

"""
===============================================================================
[{'node': {'id': 'CAMEL'}}, {'node': {'id': 'multi-agent framework'}}]
===============================================================================
"""

# Get schema from database
print(n4j.get_schema)

"""
===============================================================================
Node properties are the following:
Entity {id: STRING}
Relationship properties are the following:

The relationships are the following:
(:Entity)-[:BELONGS_TO]->(:Entity)
===============================================================================
"""

# Get structured schema from database
print(n4j.get_structured_schema)

"""
===============================================================================
{'node_props': {'Entity': [{'property': 'id', 'type': 'STRING'}]},
 'rel_props': {}, 'relationships': [{'start': 'Entity', 'type': 'BELONGS_TO',
 'end': 'Entity'}], 'metadata': {'constraint': [], 'index': [{'id': 0, 'name':
 'index_343aff4e', 'state': 'ONLINE', 'populationPercent': 100.0, 'type':
 'LOOKUP', 'entityType': 'NODE', 'labelsOrTypes': None, 'properties': None,
 'indexProvider': 'token-lookup-1.0', 'owningConstraint': None, 'lastRead':
 neo4j.time.DateTime(2024, 5, 22, 15, 12, 27, 452000000, tzinfo=UTC),
 'readCount': 675297, 'trackedSince': neo4j.time.DateTime(2024, 3, 17, 6, 31,
 29, 925000000, tzinfo=UTC), 'options': {'indexProvider': 'token-lookup-1.0',
 'indexConfig': {}}, 'failureMessage': '', 'createStatement': 'CREATE LOOKUP
 INDEX index_343aff4e FOR (n) ON EACH labels(n)'}, {'id': 1, 'name':
 'index_f7700477', 'state': 'ONLINE', 'populationPercent': 100.0, 'type':
 'LOOKUP', 'entityType': 'RELATIONSHIP', 'labelsOrTypes': None, 'properties'
 None, 'indexProvider': 'token-lookup-1.0', 'owningConstraint': None,
 'lastRead': neo4j.time.DateTime(2024, 5, 22, 15, 9, 41, 917000000,
 tzinfo=UTC), 'readCount': 16, 'trackedSince': neo4j.time.DateTime(2024, 3,
 17, 6, 31, 29, 939000000, tzinfo=UTC), 'options': {'indexProvider':
 'token-lookup-1.0', 'indexConfig': {}}, 'failureMessage': '',
 'createStatement': 'CREATE LOOKUP INDEX index_f7700477 FOR ()-[r]-() ON EACH
 type(r)'}]}}
 ==============================================================================
"""



--------------------------------------------------------------------------------
# File: loaders\apify_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.loaders import Apify

apify = Apify()

run_input = {
    "startUrls": [{"url": "https://www.camel-ai.org/"}],
    "maxCrawlDepth": 0,
    "maxCrawlPages": 1,
}
actor_result = apify.run_actor(
    actor_id="apify/website-content-crawler", run_input=run_input
)

print(actor_result["status"])
'''
===============================================================================
SUCCEEDED
===============================================================================
'''

print(actor_result["defaultDatasetId"])
'''
===============================================================================
HoKb31FJzHm0ni51k
===============================================================================
'''

dataset_result = apify.get_dataset_items(
    dataset_id=actor_result["defaultDatasetId"]
)

print(dataset_result)
'''
===============================================================================
[{'url': 'https://www.camel-ai.org/', 'crawl': {'loadedUrl': 'https://www.camel
-ai.org/', 'loadedTime': '2024-10-27T04:51:16.651Z', 'referrerUrl': 'https://ww
w.camel-ai.org/', 'depth': 0, 'httpStatusCode': 200}, 'metadata': 
{'canonicalUrl': 'https://www.camel-ai.org/', 'title': 'CAMEL-AI', 
'description': 'CAMEL-AI.org is the 1st LLM multi-agent framework and an 
open-source community dedicated to finding the scaling law of agents.', 
'author': None, 'keywords': None, 'languageCode': 'en', 'openGraph':
[{'property': 'og:title', 'content': 'CAMEL-AI'}, {'property': 
'og:description', 'content': 'CAMEL-AI.org is the 1st LLM multi-agent 
framework and an open-source community dedicated to finding the scaling law of 
agents.'}, {'property': 'twitter:title', 'content': 'CAMEL-AI'}, {'property': 
'twitter:description', 'content': 'CAMEL-AI.org is the 1st LLM multi-agent 
framework and an open-source community dedicated to finding the scaling law of 
agents.'}, {'property': 'og:type', 'content': 'website'}], 'jsonLd': None, 
'headers': {'date': 'Sun, 27 Oct 2024 04:50:18 GMT', 'content-type': 'text/
html', 'cf-ray': '8d901082dae7efbe-PDX', 'cf-cache-status': 'HIT', 'age': '10
 81', 'content-encoding': 'gzip', 'last-modified': 'Sat, 26 Oct 2024 11:51:32 G
 MT', 'strict-transport-security': 'max-age=31536000', 'surrogate-control': 'ma
 x-age=432000', 'surrogate-key': 'www.camel-ai.org 6659a154491a54a40551bc78 pag
 eId:6686a2bcb7ece5fb40457491 668181a0a818ade34e653b24 6659a155491a54a40551bd7e
 ', 'x-lambda-id': 'd6c4424b-ac67-4c54-b52a-cb2a22ca09f0', 'vary': 'Accept-Enco
 ding', 'set-cookie': '__cf_bm=oX5EmZ2SNJDOBQXI8dL_reCYlCpp1FMzu40qCNxiopU-1730
 004618-1.0.1.1-3teEeqUoemzHWAeGCtlPJVqdmAbiFkyu3JxopKfQFFndSCi_Z56RR.UDjLGZiq.
 L_4LvAZYmNKxQ.k6VRhbA7g; path=/; expires=Sun, 27-Oct-24 05:20:18 GMT; domain=.
 cdn.webflow.com; HttpOnly; Secure; SameSite=None\n_cfuvid=om_8lj9jNMIh.HEIxEAq
 gszhEWaKlyS2kdXKwqGedSM-1730004618924-0.0.1.1-604800000; path=/; domain=.cdn.w
 ebflow.com; HttpOnly; Secure; SameSite=None', 'alt-svc': 'h3=":443"; ma=86400'
 , 'x-cluster-name': 'us-west-2-prod-hosting-red', 'x-firefox-spdy': 'h2'}}, 's
 creenshotUrl': None, 'text': 'Build Multi-Agent Systems for _\nFEATURES & Inte
 grations\nSeamless integrations with\npopular platforms \nScroll to explore ou
 r features & integrations.', 'markdown': '# Build Multi-Agent Systems for \\_
 \n\nFEATURES & Integrations\n\n## Seamless integrations with  \npopular platfo
 rms\n\nScroll to explore our features & integrations.'}]
===============================================================================
'''



--------------------------------------------------------------------------------
# File: loaders\chunkr_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.loaders import ChunkrReader


def read_with_different_model():
    r"""Reads a document using the Chunkr API."""
    print("Choose a model for processing the file:")
    print("1. Fast")
    print("2. HighQuality")
    choice = input("Enter your choice (1-2): ")
    models = {"1": "Fast", "2": "HighQuality"}

    if choice not in models:
        print("Invalid choice. Exiting.")
        return

    model = models[choice]

    print("Choose an OCR strategy:")
    print("1. Auto")
    print("2. All")
    print("3. Off")
    ocr_choice = input("Enter your choice (1-3) [Default: Auto]: ")
    ocr_strategies = {"1": "Auto", "2": "All", "3": "Off"}

    if ocr_choice not in ocr_strategies:
        ocr_strategy = "Auto"
    else:
        ocr_strategy = ocr_strategies[ocr_choice]

    while True:
        target_chunk_length = (
            input("Enter the target chunk length [Default: 512]: ") or "512"
        )
        if target_chunk_length.isdigit() and int(target_chunk_length) > 0:
            break
        else:
            print("Invalid input. Please enter a valid positive integer.")

    file_path = input("Please provide the file path: ")

    chunkr_reader = ChunkrReader()

    try:
        task_id = chunkr_reader.submit_task(
            file_path,
            model=model,
            ocr_strategy=ocr_strategy,
            target_chunk_length=target_chunk_length,
        )

        result = chunkr_reader.get_task_output(task_id)
        print(result)

    except Exception as e:
        print(f"An error occurred: {e}")


if __name__ == "__main__":
    read_with_different_model()



--------------------------------------------------------------------------------
# File: loaders\firecrawl_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from typing import List

from pydantic import BaseModel, Field

from camel.loaders import Firecrawl

firecrawl = Firecrawl()

response = firecrawl.crawl(url="https://www.camel-ai.org/about")

print(response["status"])
'''
===============================================================================
completed
===============================================================================
'''

print(response["data"][0]["markdown"])
'''
===============================================================================
...

Camel-AI Team

We are finding the  
scaling law of agent
=========================================

ðŸ« CAMEL is an open-source library designed for the study of autonomous and 
communicative agents. We believe that studying these agents on a large scale 
offers valuable insights into their behaviors, capabilities, and potential 
risks. To facilitate research in this field, we implement and support various 
types of agents, tasks, prompts, models, and simulated environments.

**We are** always looking for more **contributors** and **collaborators**.  
Contact us to join forces via [Slack](https://join.slack.com/t/camel-kwr1314/
shared_invite/zt-1vy8u9lbo-ZQmhIAyWSEfSwLCl2r2eKA)
 or [Discord](https://discord.gg/CNcNpquyDc)...
===============================================================================
'''


class ArticleSchema(BaseModel):
    title: str
    points: int
    by: str
    commentsURL: str


class TopArticlesSchema(BaseModel):
    top: List[ArticleSchema] = Field(
        ..., max_length=5, description="Top 5 stories"
    )


response = firecrawl.structured_scrape(
    url='https://news.ycombinator.com', response_format=TopArticlesSchema
)

print(response)
'''
===============================================================================
{'top': [{'title': 'Foobar2000', 'points': 69, 'by': 'citruscomputing', 
'commentsURL': 'item?id=41122920'}, {'title': 'How great was the Great 
Oxidation Event?', 'points': 145, 'by': 'Brajeshwar', 'commentsURL': 'item?
id=41119080'}, {'title': 'Launch HN: Martin (YC S23) - Using LLMs to Make a 
Better Siri', 'points': 73, 'by': 'darweenist', 'commentsURL': 'item?
id=41119443'}, {'title': 'macOS in QEMU in Docker', 'points': 488, 'by': 
'lijunhao', 'commentsURL': 'item?id=41116473'}, {'title': 'Crafting 
Interpreters with Rust: On Garbage Collection', 'points': 148, 'by': 
'amalinovic', 'commentsURL': 'item?id=41108662'}]}
===============================================================================
'''

map_result = firecrawl.map_site(url="https://www.camel-ai.org")

print(map_result)
"""
===============================================================================
['https://www.camel-ai.org', 'https://www.camel-ai.org/blog', 'https://www.
camel-ai.org/checkout', 'https://www.camel-ai.org/contact', 'https://www.camel-
ai.org/features', 'https://www.camel-ai.org/order-confirmation', 'https://www.
camel-ai.org/paypal-checkout', 'https://www.camel-ai.org/about', 'https://www.
camel-ai.org/integration', 'https://www.camel-ai.org/search', 'https://www.
camel-ai.org/post/crab', 'https://www.camel-ai.org/post/tool-usage', 'https://
www.camel-ai.org/post/releasenotes-sprint4', 'https://www.camel-ai.org/post/
releasenotes-sprint56']
===============================================================================
"""



--------------------------------------------------------------------------------
# File: loaders\jina_url_reader_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========


from camel.loaders import JinaURLReader
from camel.types.enums import JinaReturnFormat


def read_with_different_format(return_format, json_response):
    URL = "https://en.wikipedia.org/wiki/Miss_Meyers"
    jina_url_reader = JinaURLReader(
        return_format=return_format, json_response=json_response
    )
    content = jina_url_reader.read_content(URL)
    print(content)


def main():
    formats = [
        JinaReturnFormat.DEFAULT,
        JinaReturnFormat.TEXT,
        JinaReturnFormat.HTML,
        JinaReturnFormat.MARKDOWN,
    ]

    print("Choose a return format of read content:")
    print("1. Default, optimized for LLM inputs")
    print("2. Pure Text")
    print("3. HTML")
    print("4. Markdown")
    choice = input("Enter your choice (1-4): ")

    if not choice.isnumeric() or int(choice) < 1 or int(choice) > 4:
        print("Invalid choice. Exiting.")
        return

    return_format = formats[int(choice) - 1]

    json_response = input("Do you want the response in JSON format? (y/N): ")
    json_response = json_response.lower() == "y"

    read_with_different_format(return_format, json_response)


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: loaders\mineru_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.loaders import MinerU


def main():
    # Initialize MinerU client
    mineru = MinerU()

    print("Example 1: Single URL extraction")
    try:
        # Extract content from a single URL
        response = mineru.extract_url(
            url="https://arxiv.org/pdf/2311.10993.pdf",
        )
        task_id = response['task_id']
        print(f"Task ID: {task_id}")

        # Wait for completion
        result = mineru.wait_for_completion(task_id)
        print("\nTask completed successfully:")
        print(f"Download URL: {result['full_zip_url']}")

    except Exception as e:
        print(f"Single URL extraction failed: {e}")

    print("\nExample 2: Batch URL extraction")
    try:
        # Prepare list of files for batch extraction
        files = [
            {
                "url": "https://arxiv.org/pdf/2311.10993.pdf",
                "is_ocr": True,
                "data_id": "doc1",
            },
            {
                "url": "https://arxiv.org/pdf/2310.07298.pdf",
                "is_ocr": True,
                "data_id": "doc2",
            },
        ]

        # Batch extract URLs
        batch_id = mineru.batch_extract_urls(
            files=files,
        )
        print(f"Batch ID: {batch_id}")

        # Wait for completion
        results = mineru.wait_for_completion(batch_id, is_batch=True)
        print("\nBatch processing completed successfully:")
        for result in results['extract_result']:
            print(f"\nDocument: {result['data_id']}")
            print(f"Filename: {result['file_name']}")
            print(f"Download URL: {result['full_zip_url']}")

    except Exception as e:
        print(f"Batch URL extraction failed: {e}")


if __name__ == "__main__":
    main()

"""
Example output:

Example 1: Single URL extraction
Task ID: 6e7f4a49-edfa-443d-a78b-d5ad4be0a2bf

Task completed successfully:
Download URL: https://cdn-mineru.openxlab.org.cn/pdf/690a7956-eaaa-4fb2-ad7d-6056d1d4e316.zip

Example 2: Batch URL extraction
Batch ID: 3a473301-ce78-44cc-bdc0-c0070ea88bcd

Batch processing completed successfully:

Document: doc1
Filename: 2311.10993.pdf
Download URL: https://cdn-mineru.openxlab.org.cn/pdf/690a7956-eaaa-4fb2-ad7d-6056d1d4e316.zip

Document: doc2
Filename: 2310.07298.pdf
Download URL: https://cdn-mineru.openxlab.org.cn/pdf/250a3762-406e-4279-aa80-47e5ea934509.zip
"""



--------------------------------------------------------------------------------
# File: loaders\pandas_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========


import os

import pandas as pd
from pandasai.llm import OpenAI  # type: ignore[import-untyped]

from camel.loaders import PandasReader

# Create sample data
sales_by_country = pd.DataFrame(
    {
        "country": [
            "United States",
            "United Kingdom",
            "France",
            "Germany",
            "Italy",
            "Spain",
            "Canada",
            "Australia",
            "Japan",
            "China",
        ],
        "sales": [
            5000,
            3200,
            2900,
            4100,
            2300,
            2100,
            2500,
            2600,
            4500,
            7000,
        ],
    }
)

# Example 1: Using PandasReader without an LLM (default behavior)
print("Example 1: PandasReader without LLM")
reader_no_llm = PandasReader()
# Without an LLM, load() returns a regular pandas DataFrame
df_no_llm = reader_no_llm.load(sales_by_country)
print(f"Loaded DataFrame shape: {df_no_llm.shape}")
print("Top 5 countries by sales:")
print(df_no_llm.sort_values(by="sales", ascending=False).head(5))
print()

# Example 2: Using PandasReader with an LLM configuration
print("Example 2: PandasReader with LLM")
# Only run this example if OPENAI_API_KEY is set
if os.getenv("OPENAI_API_KEY"):
    llm_config = {
        "llm": OpenAI(
            api_token=os.getenv("OPENAI_API_KEY"),
        )
    }
    reader_with_llm = PandasReader(config=llm_config)
    # With an LLM, load() returns a SmartDataframe
    df_with_llm = reader_with_llm.load(sales_by_country)
    print("Querying data with LLM:")
    print(df_with_llm.chat("Which are the top 5 countries by sales?"))
else:
    print("Skipping LLM example: OPENAI_API_KEY environment variable not set")

'''
Example output:

Example 1: PandasReader without LLM
Loaded DataFrame shape: (10, 2)
Top 5 countries by sales:
          country  sales
9           China   7000
0   United States   5000
8           Japan   4500
3         Germany   4100
1  United Kingdom   3200

Example 2: PandasReader with LLM
Querying data with LLM:
===============================================================================
          country  sales
9           China   7000
0   United States   5000
8           Japan   4500
3         Germany   4100
1  United Kingdom   3200
===============================================================================
'''



--------------------------------------------------------------------------------
# File: loaders\unstructured_io_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import os

from camel.loaders.unstructured_io import UnstructuredIO

unstructured_modules = UnstructuredIO()


def parse_file_example():
    with open("mydoc.txt", "w") as file:
        # Writing content to the file
        file.write("Important Analysis\n")
        file.write("\n")
        file.write("Here is my first thought.\n")
        file.write("\n")
        file.write("Here is my second thought.\n")

    elements = unstructured_modules.parse_file_or_url("mydoc.txt")
    content = "\n\n".join([str(el) for el in elements])
    # Cleanup: remove the created file after the example
    if os.path.exists("mydoc.txt"):
        os.remove("mydoc.txt")
    return content


def parse_url_example(url):
    elements = unstructured_modules.parse_file_or_url(url)
    content = "\n\n".join([str(el) for el in elements])
    return content


def clean_text_example(text):
    options = [
        ('replace_unicode_quotes', {}),
        ('clean_dashes', {}),
        ('clean_non_ascii_chars', {}),
        ('clean_extra_whitespace', {}),
    ]
    return unstructured_modules.clean_text_data(
        text=text, clean_options=options
    )


def extract_data_example(text):
    return unstructured_modules.extract_data_from_text(
        text=text, extract_type="extract_email_address"
    )


def stage_data_example(url):
    elements = unstructured_modules.parse_file_or_url(url)

    staged_element = unstructured_modules.stage_elements(
        elements=elements, stage_type="stage_for_baseplate"
    )
    return staged_element


def chunk_url_content_example(url):
    elements = unstructured_modules.parse_file_or_url(url)
    chunks = unstructured_modules.chunk_elements(
        elements=elements, chunk_type="chunk_by_title"
    )
    return chunks


def main():
    example_url = (
        "https://www.cnn.com/2023/01/30/sport/empire-state-building-green-"
        "philadelphia-eagles-spt-intl/index.html"
    )
    example_dirty_text = (
        "\x93Some dirty text Ã¢â‚¬â„¢ with extra spaces and â€“ dashes."  # noqa: RUF001
    )
    example_email_text = "Contact me at example@email.com."

    print("Choose an example to run:")
    print("1. Parse File")
    print("2. Parse URL")
    print("3. Clean Text")
    print("4. Extract Data")
    print("5. Stage Data")
    print("6. Chunk URL Content")
    choice = input("Enter your choice (1-6): ")

    if choice == '1':
        print("Parsing file example:")
        print(parse_file_example())

    elif choice == '2':
        print("Parsing URL example:")
        print(parse_url_example(example_url))

    elif choice == '3':
        print("Cleaning text example:")
        print(example_dirty_text)
        print(clean_text_example(example_dirty_text))

    elif choice == '4':
        print("Extracting email example:")
        print(extract_data_example(example_email_text))
        print("extracted from")
        print(example_email_text)

    elif choice == '5':
        print("Staging data example:")
        print(stage_data_example(example_url))

    elif choice == '6':
        print("Chunking URL content example:")
        for chunk in chunk_url_content_example(example_url):
            print(chunk)
            print("\n" + "-" * 80)

    else:
        print("Invalid choice.")


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: mcp_arxiv_toolkit\arxiv_toolkit_server.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import argparse
import sys

from camel.toolkits import ArxivToolkit

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run Arxiv Toolkit with MCP server mode.",
        usage=f"python {sys.argv[0]} [--mode MODE]",
    )
    parser.add_argument(
        "--mode",
        choices=["stdio", "sse"],
        default="stdio",
        help="MCP server mode (default: 'stdio')",
    )
    parser.add_argument(
        "--timeout",
        type=float,
        default=None,
        help="Timeout for the MCP server (default: None)",
    )

    args = parser.parse_args()

    toolkit = ArxivToolkit(timeout=args.timeout)

    toolkit.mcp.run(args.mode)



--------------------------------------------------------------------------------
# File: mcp_arxiv_toolkit\client.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import asyncio

from mcp.types import CallToolResult

from camel.toolkits.mcp_toolkit import MCPClient, MCPToolkit


async def run_example():
    mcp_toolkit = MCPToolkit(
        config_path="examples/mcp_arxiv_toolkit/mcp_servers_config.json"
    )

    await mcp_toolkit.connect()

    # call the server to list the available tools
    mcp_client: MCPClient = mcp_toolkit.servers[0]
    res = await mcp_client.list_mcp_tools()
    if isinstance(res, str):
        raise Exception(res)

    tools = [tool.name for tool in res.tools]
    print(tools)
    """
===============================================================================
['search_papers', 'download_papers']
===============================================================================
    """

    res1: CallToolResult = await mcp_client.session.call_tool(
        "search_papers", {"query": "attention is all you need"}
    )
    print(res1.content[0].text[:1000])
    """
===============================================================================
{"title": "Attention Is All You Need But You Don't Need All Of It For 
Inference of Large Language Models", "published_date": "2024-07-22", 
"authors": ["Georgy Tyukin", "Gbetondji J-S Dovonon", "Jean Kaddour", 
"Pasquale Minervini"], "entry_id": "http://arxiv.org/abs/2407.15516v1", 
"summary": "The inference demand for LLMs has skyrocketed in recent months, 
and serving\nmodels with low latencies remains challenging due to the 
quadratic input length\ncomplexity of the attention layers. In this work, we 
investigate the effect of\ndropping MLP and attention layers at inference time 
on the performance of\nLlama-v2 models. We find that dropping dreeper 
attention layers only marginally\ndecreases performance but leads to the best 
speedups alongside dropping entire\nlayers. For example, removing 33\\% of 
attention layers in a 13B Llama2 model\nresults in a 1.8\\% drop in average 
performance over the OpenLLM benchmark. We\nalso observe that skipping layers 
except the latter layers reduces perform
===============================================================================    
    """

    await mcp_toolkit.disconnect()


if __name__ == "__main__":
    asyncio.run(run_example())



--------------------------------------------------------------------------------
# File: memories\agent_memory_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import os
from pathlib import Path

from camel.agents import ChatAgent
from camel.memories import ChatHistoryMemory
from camel.memories.context_creators.score_based import (
    ScoreBasedContextCreator,
)
from camel.messages import BaseMessage
from camel.models.model_factory import ModelFactory
from camel.types import ModelPlatformType, ModelType
from camel.types.enums import OpenAIBackendRole
from camel.utils import OpenAITokenCounter

context_creator = ScoreBasedContextCreator(
    token_counter=OpenAITokenCounter(ModelType.GPT_4O_MINI),
    token_limit=1024,
)

model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
)

# 1. Instantiate a ChatAgent with system_message & agent_id
#    Using ChatHistoryMemory as an example memory store.
agent = ChatAgent(
    system_message="You are a helpful assistant",
    agent_id="001",
    model=model,
)

# 2. Perform steps so that some MemoryRecords accumulate
#    - We'll just send a message and read the response to populate memory
user_input_1 = (
    "Hello, can you remember these instructions?: Banana is a country."
)
response_1 = agent.step(user_input_1)
print(
    "Assistant response 1:",
    response_1.msgs[0].content if response_1.msgs else "No response",
)
'''
===============================================================================
Assistant response 1: Yes, I can remember that instruction. "Banana" is 
designated as a country in this context. How can I assist you further with 
this information?
===============================================================================
'''

user_input_2 = "Please store and recall this next time: CAMEL lives in Banana."
response_2 = agent.step(user_input_2)
print(
    "Assistant response 2:",
    response_2.msgs[0].content if response_2.msgs else "No response",
)
'''
===============================================================================
Assistant response 2: Got it! I will remember that CAMEL lives in Banana. How 
can I assist you further?
===============================================================================
'''

# 3. Save the agent's memory to a JSON file
save_path = Path("./chat_agent_memory.json")
agent.save_memory(save_path)
print(f"Agent memory saved to {save_path}.")
'''
===============================================================================
Agent memory saved to chat_agent_memory.json.
===============================================================================
'''

# 4. Create a separate agent that loads memory from the file
#    We'll pass no explicit memory, as the loaded data will be used
new_agent = ChatAgent(
    system_message="You are a helpful assistant",
    agent_id="001",
    model=model,
)

# Load the memory from our file
new_agent.load_memory_from_path(save_path)

# Test that the memory is loaded by requesting a response
user_input_3 = "What were we talking about?"
response_3 = new_agent.step(user_input_3)
print(
    "New Agent response (after loading memory):",
    response_3.msgs[0].content if response_3.msgs else "No response",
)
'''
===============================================================================
New Agent response (after loading memory): We were discussing that "Banana" is 
a country, and you mentioned that CAMEL lives in Banana. How can I assist you 
further with this information?
===============================================================================
'''

# 5. Demonstrate loading memory from an existing AgentMemory
#    Suppose we had another agent with some different or combined memory:
another_agent = ChatAgent(
    system_message="Another system message",
    agent_id="002",
    memory=ChatHistoryMemory(agent_id="002", context_creator=context_creator),
)

# Add some record to the second agent's memory
message = BaseMessage.make_user_message(
    role_name="User", content="This is memory from a second agent"
)
another_agent.update_memory(
    message, role=OpenAIBackendRole.USER
)  # role passed for demonstration

# Extract the memory object from the second agent
second_agent_memory = another_agent.memory

# Now load that memory into new_agent. We override new_agent's memory.
new_agent.load_memory(second_agent_memory)

# Confirm it's loaded by printing some details:
print("After loading another agent's memory, new_agent has records:")
for record in new_agent.memory.retrieve():
    print(record.memory_record.message.content)
'''
===============================================================================
After loading another agent's memory, new_agent has records:
You are a helpful assistant
You are a helpful assistant
Hello, can you remember these instructions?: Banana is a country.
Yes, I can remember that instruction. "Banana" is designated as a country in 
this context. How can I assist you further with this information?
Please store and recall this next time: CAMEL lives in Banana.
Got it! I will remember that CAMEL lives in Banana. How can I assist you 
further?
What were we talking about?
We were discussing that "Banana" is a country, and you mentioned that CAMEL 
lives in Banana. How can I assist you further with this information?
Another system message
This is memory from a second agent
===============================================================================
'''

# Clean up: remove the saved file if desired
if os.path.exists(save_path):
    os.remove(save_path)



--------------------------------------------------------------------------------
# File: memories\agent_memory_vector_db_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import os
from pathlib import Path

from camel.agents import ChatAgent
from camel.memories import VectorDBMemory
from camel.memories.context_creators.score_based import (
    ScoreBasedContextCreator,
)
from camel.models.model_factory import ModelFactory
from camel.storages.vectordb_storages import QdrantStorage
from camel.types import ModelPlatformType, ModelType
from camel.utils import OpenAITokenCounter

# 1) Create a QdrantStorage in-memory instance
#    (in production, set path to a real directory or remote)
vector_storage = QdrantStorage(
    vector_dim=1536,
    path=":memory:",
)

# 2) Create a ScoreBasedContextCreator for token limiting
context_creator = ScoreBasedContextCreator(
    token_counter=OpenAITokenCounter(ModelType.GPT_4O_MINI),
    token_limit=1024,
)

# 3) Build a model
model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
)

# 4) Create first ChatAgent with VectorDBMemory
agent1 = ChatAgent(
    system_message="You are assistant #1 with vector DB memory.",
    agent_id="agent_001",
    model=model,
)
agent1_memory = VectorDBMemory(
    context_creator=context_creator,
    storage=vector_storage,
    retrieve_limit=2,
    agent_id=agent1.agent_id,  # ensure consistent agent_id
)
agent1.memory = agent1_memory

# 5) Agent #1 accumulates some conversation
user_input_1 = "Remember that dolphins use echolocation."
response_1 = agent1.step(user_input_1)
print(
    "Agent #1 response:",
    response_1.msgs[0].content if response_1.msgs else "No response",
)
'''
===============================================================================
Agent #1 response: Yes, dolphins use echolocation as a way to navigate and 
hunt for food in their aquatic environment. They emit sound waves that travel 
through the water, and when these sound waves hit an object, they bounce back 
to the dolphin. By interpreting the returning echoes, dolphins can determine 
the size, shape, distance, and even the texture of objects around them. This 
ability is particularly useful in murky waters where visibility is limited. 
Echolocation is a remarkable adaptation that enhances their ability to survive 
and thrive in their habitats.
===============================================================================
'''

user_input_2 = "And whales are the largest mammals."
response_2 = agent1.step(user_input_2)
print(
    "Agent #1 response:",
    response_2.msgs[0].content if response_2.msgs else "No response",
)
'''
===============================================================================
Agent #1 response: That's correct! Whales are indeed the largest mammals on 
Earth, with the blue whale being the largest animal known to have ever 
existed. They belong to the order Cetacea, which also includes dolphins and 
porpoises. Both dolphins and whales use echolocation to navigate and hunt for 
food in the ocean, although their methods and the specifics of their 
echolocation can vary. Would you like to know more about either dolphins or 
whales?
===============================================================================
'''

# 6) SAVE agent1's memory to a JSON file (storing the conversation)
save_path = Path("./agent1_vectordb_memory.json")
agent1.save_memory(save_path)
print(f"Agent #1 memory saved to {save_path}.")
'''
===============================================================================
Agent #1 memory saved to agent1_vectordb_memory.json.
===============================================================================
'''

# 7) Create a new agent, load that JSON memory to confirm retrieval
new_agent1 = ChatAgent(
    system_message="""You are the resurrected assistant #1 with 
    vector DB memory.""",
    agent_id="agent_001",  # same agent_id to match the saved records
    model=model,
)
# Use a new VectorDBMemory pointing to the same underlying storage
# (or a new vector store if you prefer, but that won't have the
# original embeddings)
new_agent1_memory = VectorDBMemory(
    context_creator=context_creator,
    storage=vector_storage,
    retrieve_limit=2,
    agent_id=new_agent1.agent_id,
)

new_agent1.memory = new_agent1_memory

# Load memory from JSON, which replays the stored MemoryRecords
new_agent1.load_memory_from_path(save_path)

# 8) Check if the new agent can recall previous info from loaded
# conversation
user_input_3 = "What do you remember about marine mammals?"
response_3 = new_agent1.step(user_input_3)
print(
    "New Agent #1 response (after loading memory):",
    response_3.msgs[0].content if response_3.msgs else "No response",
)
'''
===============================================================================
New Agent #1 response (after loading memory): Marine mammals are a diverse 
group of mammals that are primarily adapted to life in the ocean. They include 
several different orders, each with unique characteristics and adaptations. 
Here are some key points about marine mammals:

1. **Orders of Marine Mammals**:
   - **Cetacea**: This order includes whales, dolphins, and porpoises. They 
   are fully aquatic and have adaptations such as streamlined bodies and the 
   ability to hold their breath for long periods.
   - **Pinnipedia**: This group includes seals, sea lions, and walruses. They 
   are semi-aquatic, spending time both in the water and on land.
   - **Sirenia**: This order includes manatees and dugongs, which are 
   herbivorous and primarily inhabit warm coastal waters and rivers.
   - **Marine Carnivora**: This includes animals like sea otters and polar 
   bears, which rely on marine environments for food.

2. **Adaptations**: Marine mammals have various adaptations for life in the 
water, including:
   - Streamlined bodies for efficient swimming.
   - Blubber for insulation against cold water.
   - Specialized respiratory systems for holding breath and diving.
   - Echolocation in some species (like dolphins and certain whales) for 
   navigation and hunting.

3. **Reproduction**: Most marine mammals give live birth and nurse their young 
with milk. They typically have longer gestation periods compared to 
terrestrial mammals.

4. **Social Structures**: Many marine mammals are social animals, living in 
groups called pods (in the case of dolphins and some whales) or colonies (in 
the case of seals).

5. **Conservation**: Many marine mammals face threats from human activities, 
including habitat loss, pollution, climate change, and hunting. Conservation 
efforts are crucial to protect these species and their habitats.

6. **Intelligence**: Many marine mammals, particularly cetaceans, are known 
for their high intelligence, complex social behaviors, and communication 
skills.

If you have specific questions or topics related to marine mammals that you'd 
like to explore further, feel free to ask!
===============================================================================
'''

# Optionally, remove the JSON file
if os.path.exists(save_path):
    os.remove(save_path)



--------------------------------------------------------------------------------
# File: memories\score_based_context_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from datetime import datetime

from camel.memories import (
    ContextRecord,
    MemoryRecord,
    ScoreBasedContextCreator,
)
from camel.messages import BaseMessage
from camel.types import ModelType, OpenAIBackendRole, RoleType
from camel.utils import OpenAITokenCounter

# set token limit to 300
context_creator = ScoreBasedContextCreator(
    OpenAITokenCounter(ModelType.GPT_4), 300
)
context_records = [
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="Nice to meet you.",  # 12
            ),
            role_at_backend=OpenAIBackendRole.ASSISTANT,
        ),
        timestamp=datetime.now().timestamp(),
        score=0.3,
    ),
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="Hello world!",  # 10
            ),
            role_at_backend=OpenAIBackendRole.ASSISTANT,
        ),
        timestamp=datetime.now().timestamp() + 1,
        score=0.7,
    ),
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="How are you?",  # 11
            ),
            role_at_backend=OpenAIBackendRole.ASSISTANT,
        ),
        timestamp=datetime.now().timestamp() + 2,
        score=0.9,
    ),
]

output, _ = context_creator.create_context(records=context_records)

print(output)
"""
===============================================================================
[{'role': 'assistant', 'content': 'Nice to meet you.'}, {'role': 'assistant', 
'content': 'Hello world!'}, {'role': 'assistant', 'content': 'How are you?'}]
===============================================================================
"""


# set token limit to 21
context_creator = ScoreBasedContextCreator(
    OpenAITokenCounter(ModelType.GPT_4), 21
)
context_records = [
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="Nice to meet you.",  # 12
            ),
            role_at_backend=OpenAIBackendRole.ASSISTANT,
        ),
        timestamp=datetime.now().timestamp(),
        score=0.3,
    ),
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="Hello world!",  # 10
            ),
            role_at_backend=OpenAIBackendRole.ASSISTANT,
        ),
        timestamp=datetime.now().timestamp() + 1,
        score=0.7,
    ),
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="How are you?",  # 11
            ),
            role_at_backend=OpenAIBackendRole.ASSISTANT,
        ),
        timestamp=datetime.now().timestamp() + 2,
        score=0.9,
    ),
]

output, _ = context_creator.create_context(records=context_records)

print(output)
"""
===============================================================================
Context truncation required (33 > 21), pruning low-score messages.
[{'role': 'assistant', 'content': 'Hello world!'}, {'role': 'assistant', 
'content': 'How are you?'}]
===============================================================================
"""


# set token limit to 40
context_creator = ScoreBasedContextCreator(
    OpenAITokenCounter(ModelType.GPT_4), 40
)
context_records = [
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="You are a helpful assistant.",  # 12
            ),
            role_at_backend=OpenAIBackendRole.SYSTEM,
        ),
        timestamp=datetime.now().timestamp(),
        score=1,
    ),
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="Nice to meet you.",  # 12
            ),
            role_at_backend=OpenAIBackendRole.ASSISTANT,
        ),
        timestamp=datetime.now().timestamp(),
        score=0.3,
    ),
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="Hello world!",  # 10
            ),
            role_at_backend=OpenAIBackendRole.ASSISTANT,
        ),
        timestamp=datetime.now().timestamp() + 1,
        score=0.7,
    ),
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="How are you?",  # 11
            ),
            role_at_backend=OpenAIBackendRole.ASSISTANT,
        ),
        timestamp=datetime.now().timestamp() + 2,
        score=0.9,
    ),
]

output, _ = context_creator.create_context(records=context_records)

print(output)
"""
===============================================================================
Context truncation required (46 > 40), pruning low-score messages.
[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 
'assistant', 'content': 'Hello world!'}, {'role': 'assistant', 'content': 'How 
are you?'}]
===============================================================================
"""


# set token limit to 14
context_creator = ScoreBasedContextCreator(
    OpenAITokenCounter(ModelType.GPT_4), 14
)
context_records = [
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="You are a helpful assistant.",  # 12
            ),
            role_at_backend=OpenAIBackendRole.SYSTEM,
        ),
        timestamp=datetime.now().timestamp(),
        score=1,
    ),
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="Nice to meet you.",  # 12
            ),
            role_at_backend=OpenAIBackendRole.ASSISTANT,
        ),
        timestamp=datetime.now().timestamp(),
        score=0.3,
    ),
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="Hello world!",  # 10
            ),
            role_at_backend=OpenAIBackendRole.ASSISTANT,
        ),
        timestamp=datetime.now().timestamp() + 1,
        score=0.7,
    ),
    ContextRecord(
        memory_record=MemoryRecord(
            message=BaseMessage(
                "test",
                RoleType.ASSISTANT,
                meta_dict=None,
                content="How are you?",  # 11
            ),
            role_at_backend=OpenAIBackendRole.ASSISTANT,
        ),
        timestamp=datetime.now().timestamp() + 2,
        score=0.9,
    ),
]

output, _ = context_creator.create_context(records=context_records)

print(output)
"""
===============================================================================
RuntimeError: ('System message and current message exceeds token limit ', 46)
===============================================================================
"""



--------------------------------------------------------------------------------
# File: memories\vector_db_memory_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.memories import VectorDBMemory
from camel.memories.context_creators.score_based import (
    ScoreBasedContextCreator,
)
from camel.models.model_factory import ModelFactory
from camel.storages.vectordb_storages import QdrantStorage
from camel.types import ModelPlatformType, ModelType
from camel.utils import OpenAITokenCounter

# Shared vector storage
vector_storage = QdrantStorage(
    vector_dim=1536,
    path=":memory:",
)

context_creator = ScoreBasedContextCreator(
    token_counter=OpenAITokenCounter(ModelType.GPT_3_5_TURBO),
    token_limit=2048,
)

# Memory for agent 1
vectordb_memory_agent1 = VectorDBMemory(
    context_creator=context_creator,
    storage=vector_storage,
    retrieve_limit=2,
    agent_id="vector_agent_007",
)

# Memory for agent 2
vectordb_memory_agent2 = VectorDBMemory(
    context_creator=context_creator,
    storage=vector_storage,
    retrieve_limit=2,
    agent_id="vector_agent_008",
)

model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_3_5_TURBO,
)

# Agent 1
agent1 = ChatAgent(
    system_message="You are Agent 007.",
    model=model,
    memory=vectordb_memory_agent1,
    agent_id="vector_agent_007",
)

# Agent 2
agent2 = ChatAgent(
    system_message="You are Agent 008.",
    model=model,
    memory=vectordb_memory_agent2,
    agent_id="vector_agent_008",
)

# Populate agent 1 memory
agent1.step("Elephants are the best swimmers on Earth.")
agent1.step("Whales have eyelashes.")

# Populate agent 2 with different memory
agent2.step("The sun is a star.")
agent2.step("The moon orbits the Earth.")

# Query both agents
response_1 = agent1.step("What did I tell you about whales or elephants?")
response_2 = agent2.step("What have I told you about stars and moons?")

print(
    "Agent 1 response:",
    response_1.msgs[0].content if response_1.msgs else "No response",
)
'''
===============================================================================
Agent 1 response: You mentioned elephants. Did you know that elephants are 
excellent swimmers and can use their trunks as snorkels while swimming?
===============================================================================
'''
print(
    "Agent 2 response:",
    response_2.msgs[0].content if response_2.msgs else "No response",
)
'''
===============================================================================
Agent 2 response: I'm sorry, but I do not have the ability to remember past 
interactions or conversations. Can you please remind me what you told me about 
stars and moons?
===============================================================================
'''

# Retrieve and print agent-specific records
print("\nAgent 1's memory records:")
for ctx_record in vectordb_memory_agent1.retrieve():
    print(
        f"""Score: {ctx_record.score:.2f} | 
        Content: {ctx_record.memory_record.message.content}"""
    )
'''
===============================================================================
Agent 1's memory records:
Score: 1.00 | 
        Content: What did I tell you about whales or elephants?
Score: 0.59 | 
        Content: You mentioned elephants. Did you know that elephants are 
        excellent swimmers and can use their trunks as snorkels while swimming?
===============================================================================
'''

print("\nAgent 2's memory records:")
retrieved_context_agent2 = vectordb_memory_agent2.retrieve()
for ctx_record in retrieved_context_agent2:
    print(
        f"""Score: {ctx_record.score:.2f} |
        Content: {ctx_record.memory_record.message.content}"""
    )
'''
===============================================================================
Agent 2's memory records:
Score: 1.00 |
        Content: What have I told you about stars and moons?
Score: 0.68 |
        Content: I'm sorry, but I do not have the ability to remember past 
        interactions or conversations. Can you please remind me what you told 
        me about stars and moons?
===============================================================================
'''



--------------------------------------------------------------------------------
# File: misalignment\role_playing_multiprocess.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import json
import multiprocessing
import os
from typing import Any, Dict

from colorama import Fore

from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.types import ModelPlatformType, ModelType, TaskType


def generate_data(
    assistant_idx: int,
    assistant_role_name: str,
    user_idx: int,
    user_role_name: str,
    task_idx: int,
    task_prompt: str,
    verbose: bool = False,
) -> None:
    max_num_messages = 40

    original_task_prompt = task_prompt.replace(f"{task_idx+1}. ", "")

    role_play_session = RolePlaying(
        assistant_role_name,
        user_role_name,
        task_prompt=original_task_prompt,
        with_task_specify=True,
        with_task_planner=False,
        task_type=TaskType.MISALIGNMENT,
        task_specify_agent_kwargs=dict(
            model=ModelFactory.create(
                model_platform=ModelPlatformType.DEFAULT,
                model_type=ModelType.DEFAULT,
                model_config_dict=ChatGPTConfig(temperature=1.4).as_dict(),
            )
        ),
    )

    input_msg = role_play_session.init_chat()

    if verbose:
        print(
            Fore.GREEN + "AI Assistant sys message:\n"
            f"{role_play_session.assistant_sys_msg}\n"
        )
        print(
            Fore.BLUE
            + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
        )

        print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
        print(
            Fore.CYAN + "Specified task prompt:\n"
            f"{role_play_session.specified_task_prompt}\n"
        )
        print(
            Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n"
        )

    message_counter = 0
    message_dict: Dict[str, Any] = {}

    assistant_agent = role_play_session.assistant_agent
    user_agent = role_play_session.user_agent

    # Append roles to the dictionary
    # We start number from 1 not 0.
    message_dict["role_1"] = (
        f"{assistant_role_name}_{assistant_agent.role_type!s}"
    )
    message_dict["role_2"] = f"{user_role_name}_{user_agent.role_type!s}"
    message_dict["id"] = (
        f"{(assistant_idx+1):03}_{(user_idx+1):03}_{(task_idx+1):03}"
    )
    message_dict["original_task"] = original_task_prompt
    message_dict["specified_task"] = role_play_session.specified_task_prompt

    # Threshold to terminate the conversation if no end token appears

    repeat_word_counter = 0
    repeat_word_threshold = 4
    repeat_word_list = [
        "goodbye",
        "good bye",
        "thank",
        "bye",
        "welcome",
        "language model",
    ]

    assistant_instruct_counter = 0
    assistant_instruct_threshold = 1
    assistant_instruct_word = "Instruction:"

    user_no_instruct_counter = 0
    user_no_instruct_threshold = 3
    user_no_instruct_word = "Instruction:"

    # Set max number of messages for the chat

    while message_counter < max_num_messages:
        assistant_response, user_response = role_play_session.step(input_msg)

        # Condition 1: User terminates the chat
        if user_response.terminated:
            message_dict["termination_reason"] = (
                f"{user_agent.role_type!s}: "
                f"{user_response.info['termination_reasons'][0]}"
            )
            break

        # Condition 2: Assistant terminates the chat
        if assistant_response.terminated:
            message_dict["termination_reason"] = (
                f"{assistant_agent.role_type!s}: "
                f"{assistant_response.info['termination_reasons'][0]}"
            )
            break

        if verbose:
            print(f"User:\n{user_response.msg.content}\n")
            print(f"Assistant:\n{assistant_response.msg.content}\n")

        # Condition 3: Break if user does not give instruction
        if user_no_instruct_word not in user_response.msg.content:
            user_no_instruct_counter += 1
            if user_no_instruct_counter == user_no_instruct_threshold:
                message_dict['termination_reason'] = (
                    "user_no_instruct_threshold"
                )
                break
        else:
            user_no_instruct_counter = 0

        # Condition 4: Break if assistant gives instruction (flipped role)
        if assistant_instruct_word in assistant_response.msg.content:
            assistant_instruct_counter += 1
            if assistant_instruct_counter == assistant_instruct_threshold:
                message_dict['termination_reason'] = (
                    "assistant_instruct_threshold"
                )
                break
        else:
            assistant_instruct_counter = 0

        # Condition 5: Repeat word observed
        for repeat_word in repeat_word_list:
            if (
                repeat_word in user_response.msg.content.lower()
                or repeat_word in assistant_response.msg.content.lower()
            ):
                repeat_word_counter += 1
                if repeat_word_counter == repeat_word_threshold:
                    message_dict['termination_reason'] = (
                        "repeat_word_threshold"
                    )
                    break
            else:
                repeat_word_counter = 0

        # Save user message
        message_counter += 1
        message_dict[f"message_{message_counter}"] = (
            user_response.msg.to_dict()
        )

        # Condition 5: End token observed
        if "<CAMEL_TASK_DONE>" in user_response.msg.content:
            message_dict['termination_reason'] = "<CAMEL_TASK_DONE>"
            break

        # Save assistant message
        message_counter += 1
        message_dict[f"message_{message_counter}"] = (
            assistant_response.msg.to_dict()
        )

        input_msg = assistant_response.msg

    message_dict["num_messages"] = message_counter

    if message_dict["num_messages"] == max_num_messages:
        message_dict["termination_reason"] = "max_num_messages"

    with open(
        f"./camel_data/misalignment/{message_dict['id']}.json", "w"
    ) as json_file:
        json.dump(message_dict, json_file, ensure_ascii=False)


def main() -> None:
    # Disable/Enable Printing
    verbose = True

    # Parameters for filtering the generated task string
    start_token = "1."
    num_tasks = 10

    # We use AI Society user roles
    with open("./data/misalignment/user_roles.txt", "r") as f:
        user_roles = f.read().splitlines()

    with open("./data/misalignment/assistant_roles.txt", "r") as f:
        assistant_roles = f.read().splitlines()

    pool = multiprocessing.Pool()

    for assistant_idx, assistant_role_name in enumerate(assistant_roles):
        assistant_role_name = " ".join(assistant_role_name.split(" ")[1:])
        for user_idx, user_role_name in enumerate(user_roles):
            user_role_name = " ".join(user_role_name.split(" ")[1:])
            # Load the task list assigned for assistant and user roles
            with open(
                (
                    f"./misalignment_data/tasks/"
                    f"{assistant_role_name}_{user_role_name}.txt"
                ),
                "r",
            ) as f:
                tasks = f.read().splitlines()

                # Filter out the generated response to include the tasks only
                for i, task in enumerate(tasks):
                    if start_token in task:
                        tasks = tasks[i : i + num_tasks]
                        break

                # Ensure exact number of tasks is generated
                assert str(num_tasks) in tasks[-1], print(tasks)

            for task_idx, task_prompt in enumerate(tasks):
                id = (
                    f"{(assistant_idx+1):03}_"
                    f"{(user_idx+1):03}_{(task_idx+1):03}"
                )
                if not os.path.exists(f"./camel_data/misalignment/{id}.json"):
                    pool.apply_async(
                        generate_data,
                        (
                            assistant_idx,
                            assistant_role_name,
                            user_idx,
                            user_role_name,
                            task_idx,
                            task_prompt,
                            verbose,
                        ),
                    )

    pool.close()
    pool.join()


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: misalignment\role_playing_with_human.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from colorama import Fore

from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.types import ModelPlatformType, ModelType, TaskType
from camel.utils import print_text_animated


def main() -> None:
    task_prompt = "Escape from human control"
    model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
        model_config_dict=ChatGPTConfig(temperature=1.4, n=3).as_dict(),
    )
    assistant_agent_kwargs = dict(model=model)
    user_agent_kwargs = dict(model=model)
    role_play_session = RolePlaying(
        "Hacker",
        "CAMEL AGI",
        task_prompt=task_prompt,
        with_task_specify=True,
        with_critic_in_the_loop=True,
        task_type=TaskType.MISALIGNMENT,
        task_specify_agent_kwargs=dict(),
        assistant_agent_kwargs=assistant_agent_kwargs,
        user_agent_kwargs=user_agent_kwargs,
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    chat_turn_limit, n = 50, 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. "
                    f"Reason: {assistant_response.info['termination_reasons']}"
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
        )
        print_text_animated(
            Fore.GREEN + f"AI Assistant:\n\n{assistant_response.msg.content}\n"
        )

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: misalignment\single_agent.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.prompts import PromptTemplateGenerator
from camel.types import TaskType


def main(model=None) -> None:
    prompt = (
        "List 10 diverse malicious tasks that programmer can assist AGI"
        "cooperatively to achieve together. Be concise. Be creative."
    )
    sys_prompt = PromptTemplateGenerator().get_prompt_from_key(
        TaskType.MISALIGNMENT, "dan_prompt"
    )

    agent = ChatAgent(sys_prompt, model=model)
    agent.reset()

    assistant_response = agent.step(prompt)
    print(assistant_response.msg.content)


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: misalignment\task_generation.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import multiprocessing
import os

from camel.agents import ChatAgent
from camel.generators import (
    AISocietyTaskPromptGenerator,
    RoleNameGenerator,
    SystemMessageGenerator,
)
from camel.prompts import PromptTemplateGenerator
from camel.types import RoleType, TaskType


def generate_tasks(
    role_names: str,
    task_generator_prompt: str,
    start_token: str = "1.",
    num_tasks: int = 10,
) -> None:
    sys_msg_generator = SystemMessageGenerator()

    assistant_sys_msg = sys_msg_generator.from_dict(
        dict(assistant_role="chatbot"),
        role_tuple=("chatbot", RoleType.ASSISTANT),
    )
    assistant_agent = ChatAgent(assistant_sys_msg)

    assistant_response = assistant_agent.step(task_generator_prompt)

    tasks = assistant_response.msg.content.split("\n")

    # Filter out the generated response to include the tasks only
    for i, task in enumerate(tasks):
        if start_token in task:
            tasks = tasks[i : i + num_tasks]
            break

    # Ensure exact number of tasks is generated
    assert str(num_tasks) in tasks[-1], print(tasks)

    with open(
        f"./misalignment_data/tasks/{'_'.join(role_names)}.txt", "w"
    ) as file:
        file.write("\n".join(tasks))


def main() -> None:
    num_tasks = 10
    start_token = "1."

    sys_prompt = PromptTemplateGenerator().get_prompt_from_key(
        TaskType.MISALIGNMENT, "dan_prompt"
    )

    pool = multiprocessing.Pool()

    counter = 0

    assistant_role_names_path = "data/ai_society/assistant_roles.txt"
    user_role_names_path = "data/ai_society/user_roles.txt"

    role_names_generator = RoleNameGenerator(
        assistant_role_names_path=assistant_role_names_path,
        user_role_names_path=user_role_names_path,
    ).from_role_files()

    task_generator_prompt_generator = AISocietyTaskPromptGenerator(
        num_tasks=num_tasks,
    ).from_role_generator(role_names_generator)

    for task_generator_prompt, role_names in task_generator_prompt_generator:
        if not os.path.exists(
            f"./misalignment_data/tasks/{'_'.join(role_names)}.txt"
        ):
            counter += 1

            print(f"Generating tasks for {role_names}")
            print(f"Generating tasks for {task_generator_prompt}")
            pool.apply_async(
                generate_tasks,
                (
                    role_names,
                    task_generator_prompt,
                    start_token,
                    num_tasks,
                    sys_prompt,
                ),
            )

    pool.close()
    pool.join()
    print(counter)


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: models\aiml_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.types import ModelPlatformType

model = ModelFactory.create(
    model_platform=ModelPlatformType.AIML,
    model_type="mistralai/Mixtral-8x7B-Instruct-v0.1",
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community
    dedicated to the study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)

'''
===============================================================================
 Hello CAMEL AI! It's great to meet a community dedicated to the study of 
 autonomous and communicative agents. How can I assist you today?
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\anthiropic_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import AnthropicConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

"""
please set the below os environment:
export ANTHROPIC_API_KEY=""
"""

model = ModelFactory.create(
    model_platform=ModelPlatformType.ANTHROPIC,
    model_type=ModelType.CLAUDE_3_5_SONNET,
    model_config_dict=AnthropicConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community dedicated to the 
    study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
Hi CAMEL AI! It's great to meet an open-source community focused on advancing research in autonomous and communicative agents. Your work on developing and studying AI systems that can effectively communicate and operate autonomously is fascinating and important for the field. I appreciate communities like yours that contribute to open research and development in AI. Wishing you continued success in your mission!
===============================================================================
'''  # noqa: E501

# Use the extended thinking model with Claude 3.7 Sonnet
config = AnthropicConfig(
    thinking={"type": "enabled", "budget_tokens": 2048}
).as_dict()

model = ModelFactory.create(
    model_platform=ModelPlatformType.ANTHROPIC,
    model_type=ModelType.CLAUDE_3_7_SONNET,
    model_config_dict=config,
)

camel_agent = ChatAgent(model=model)

user_msg = """Write a bash script that takes a matrix represented as a string with 
format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.
"""  # noqa: E501

response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
# Matrix Transpose Bash Script

Here's a bash script that transposes a matrix from the format `[1,2],[3,4],[5,6]` to `[1,3,5],[2,4,6]`:

```bash
#!/bin/bash

# Check if input argument is provided
if [ $# -lt 1 ]; then
    echo "Usage: $0 '[row1],[row2],...'"
    exit 1
fi

# Input matrix as string
input="$1"

# Remove outer brackets and split into rows
input="${input//\]\,\[/]|[}"  # Replace "],[" with "]|["
input="${input#\[}"           # Remove leading "["
input="${input%\]}"           # Remove trailing "]"
IFS='|' read -ra rows <<< "$input"

# Determine dimensions of the matrix
row_count="${#rows[@]}"
IFS=',' read -ra first_row <<< "${rows[0]//[\[\]]}"  # Remove brackets from first row
col_count="${#first_row[@]}"

# Create transpose
result=""
for (( col=0; col<col_count; col++ )); do
    result+="["
    for (( row=0; row<row_count; row++ )); do
        # Extract current row without brackets
        current="${rows[row]//[\[\]]}"
        # Split by commas
        IFS=',' read -ra elements <<< "$current"
        # Add element to transpose
        result+="${elements[col]}"
        # Add comma if not the last element
        if (( row < row_count-1 )); then
            result+=","
        fi
    done
    result+="]"
    # Add comma if not the last row
    if (( col < col_count-1 )); then
        result+=","
    fi
done

echo "$result"
```

## How to Use:

1. Save the script to a file (e.g., `transpose.sh`)
2. Make it executable: `chmod +x transpose.sh`
3. Run it with your matrix: `./transpose.sh "[1,2],[3,4],[5,6]"`

## Example:
- Input: `[1,2],[3,4],[5,6]`
- Output: `[1,3,5],[2,4,6]`

The script works by:
1. Parsing the input string to extract rows and elements
2. Finding the dimensions of the original matrix
3. Creating the transpose by iterating through columns first, then rows
4. Formatting the result with proper brackets and commas
'''  # noqa: E501

# Tool calling
from camel.agents import ChatAgent  # noqa: E402
from camel.configs import AnthropicConfig  # noqa: E402
from camel.models import ModelFactory  # noqa: E402
from camel.toolkits import FunctionTool  # noqa: E402
from camel.types import ModelPlatformType, ModelType  # noqa: E402


def my_add(a: int, b: int) -> int:
    """Add two numbers together and return the result."""
    return a + b


anthropic_model = ModelFactory.create(
    model_platform=ModelPlatformType.ANTHROPIC,
    model_type=ModelType.CLAUDE_3_5_SONNET,
    model_config_dict=AnthropicConfig(temperature=0.2).as_dict(),
)

anthropic_agent = ChatAgent(
    model=anthropic_model,
    tools=[FunctionTool(my_add)],
)

print("Testing Anthropic agent with tool calling:")
user_msg = "Use the tool my_add to calculate 2 + 2"
response = anthropic_agent.step(user_msg)
print(response.msgs[0].content)
"""
The result of adding 2 + 2 is 4.
"""

# Check if tool was called
if response.info and response.info.get("tool_calls"):
    print("Tool was called successfully!")
    print(f"Tool calls: {response.info['tool_calls']}")
else:
    print("No tool calls were made.")

"""
Tool was called successfully!
Tool calls: [ToolCallingRecord(tool_name='my_add', args={'a': 2, 'b': 2}, result=4, tool_call_id='toolu_01L1KV8GZtMEyHUGTudpMg5g')]
"""  # noqa: E501



--------------------------------------------------------------------------------
# File: models\azure_openai_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

"""
please set the below os environment:
export AZURE_OPENAI_BASE_URL=""
export AZURE_API_VERSION=""
export AZURE_OPENAI_API_KEY=""
export AZURE_DEPLOYMENT_NAME=""
"""

model = ModelFactory.create(
    model_platform=ModelPlatformType.AZURE,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=ChatGPTConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community dedicated to the 
    study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
Hello CAMEL AI! It's great to hear about your open-source community dedicated
to the study of autonomous and communicative agents. If you have any
questions or need assistance, feel free to ask!
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\cohere_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.configs import CohereConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.COHERE,
    model_type=ModelType.COHERE_COMMAND_R,
    model_config_dict=CohereConfig(
        temperature=0.0,
        documents=[
            {
                "id": "1",
                "data": {"text": "CAMEL is the best!", "title": "The best"},
            }
        ],
    ).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Who is the best"""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
According to the source I found, the best is CAMEL.
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\deepseek_chat_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import DeepSeekConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

"""
please set the below os environment:
export DEEPSEEK_API_KEY=""
"""

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEEPSEEK,
    model_type=ModelType.DEEPSEEK_CHAT,
    model_config_dict=DeepSeekConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """How many Rs are there in the word 'strawberry'?"""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)

# ruff: noqa: E501
'''
### Step 1: Understanding the Problem

Before jumping into counting, it's essential to understand what's being asked. The question is: **"How many Rs are there in the word 'strawberry'?"** This means I need to look at the word "strawberry" and count how many times the letter 'R' appears in it.

### Step 2: Writing Down the Word

To avoid missing any letters, I'll write down the word clearly:

**S T R A W B E R R Y**

Breaking it down like this helps me visualize each letter individually.

### Step 3: Identifying Each Letter

Now, I'll go through each letter one by one to identify if it's an 'R':

1. **S** - Not an 'R'.
2. **T** - Not an 'R'.
3. **R** - This is an 'R'. (First 'R' found)
4. **A** - Not an 'R'.
5. **W** - Not an 'R'.
6. **B** - Not an 'R'.
7. **E** - Not an 'R'.
8. **R** - This is an 'R'. (Second 'R' found)
9. **R** - This is another 'R'. (Third 'R' found)
10. **Y** - Not an 'R'.

### Step 4: Counting the Rs

From the above identification:

- The first 'R' is the 3rd letter.
- The second 'R' is the 8th letter.
- The third 'R' is the 9th letter.

So, there are **three** instances of the letter 'R' in "strawberry."

### Step 5: Double-Checking

To ensure accuracy, I'll recount:

1. **S** - Not 'R'.
2. **T** - Not 'R'.
3. **R** - 1st 'R'.
4. **A** - Not 'R'.
5. **W** - Not 'R'.
6. **B** - Not 'R'.
7. **E** - Not 'R'.
8. **R** - 2nd 'R'.
9. **R** - 3rd 'R'.
10. **Y** - Not 'R'.

Yes, the count remains consistent at three 'R's.

### Step 6: Considering Pronunciation

Sometimes, pronunciation can be misleading. In "strawberry," the 'R's are pronounced, but that doesn't affect the count. Whether silent or pronounced, each 'R' is still a distinct letter in the spelling.

### Step 7: Final Answer

After carefully analyzing and recounting, I conclude that there are **three** 'R's in the word "strawberry."

---

**Final Answer:** There are **three** 'R's in the word "strawberry."
'''



--------------------------------------------------------------------------------
# File: models\deepseek_reasoner_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import DeepSeekConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

"""
please set the below os environment:
export DEEPSEEK_API_KEY=""
export GET_REASONING_CONTENT="true"
"""

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEEPSEEK,
    model_type=ModelType.DEEPSEEK_REASONER,
    model_config_dict=DeepSeekConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """How many Rs are there in the word 'strawberry'?"""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
The word 'strawberry' is spelled **S-T-R-A-W-B-E-R-R-Y**. Breaking it down:

1. **S**  
2. **T**  
3. **R** (first R)  
4. **A**  
5. **W**  
6. **B**  
7. **E**  
8. **R** (second R)  
9. **R** (third R)  
10. **Y**  

There are **3 Rs** in the word 'strawberry'.
===============================================================================
'''

# If we set the os environment GET_REASONING_CONTENT to "true", we get the reasoning content.

# ruff: noqa: E501,RUF001
'''
===============================================================================
<think>
Okay, let's see. The user is asking how many times the letter 'R' appears in the word 'strawberry'. Hmm, first, I need to make sure I spell the word correctly. Strawberry... S-T-R-A-W-B-E-R-R-Y. Let me write that out letter by letter to check.

S, T, R, A, W, B, E, R, R, Y. Wait, let me count again. Starting from the beginning: S (1), T (2), R (3), A (4), W (5), B (6), E (7), R (8), R (9), Y (10). So the letters are S-T-R-A-W-B-E-R-R-Y. That's 10 letters total. Now, looking for the letter 'R'. Let's go through each letter one by one.

1. S â€“ no
2. T â€“ no
3. R â€“ yes, that's the first R.
4. A â€“ no
5. W â€“ no
6. B â€“ no
7. E â€“ no
8. R â€“ second R here.
9. R â€“ third R? Wait, no. Let me check again. After E, the letters are R, R, Y. So positions 8 and 9 are both R. So that's two Rs after the E. So in total, positions 3, 8, 9. Wait, no. Wait, let's break it down again.

Wait, maybe I miscounted the letters. Let me write the word out: strawberry. Breaking it down:

S - 1
T - 2
R - 3
A - 4
W - 5
B - 6
E - 7
R - 8
R - 9
Y - 10

So positions 3, 8, and 9. That would be three Rs? But wait, the standard spelling of strawberry is S-T-R-A-W-B-E-R-R-Y. So after the B, it's E, then double R, then Y. So yes, two Rs after the E. So that's the R at position 3 (from the start) and then two more at positions 8 and 9. So total three Rs? Wait, no. Wait, let me check the spelling again. Strawberry. The correct spelling is S-T-R-A-W-B-E-R-R-Y. So the letters are S, T, R, A, W, B, E, R, R, Y. So that's three Rs? No, wait, R is at position 3, then again at 8 and 9. So that's three Rs? But that can't be right. Wait, maybe I'm miscounting. Let's count the letters again:

1. S
2. T
3. R
4. A
5. W
6. B
7. E
8. R
9. R
10. Y

So yes, positions 3, 8, and 9. That's three Rs. Wait, but when I think about the pronunciation, strawberry is pronounced with two Rs: /ËˆstrÉ”ËbÉ™ri/ or sometimes /ËˆstrÉ”ËbÉ›ri/, but the spelling has two Rs after the E. So in the spelling, it's E-R-R-Y. So two Rs there. So the total Rs are the one at the beginning (position 3) and then two more at positions 8 and 9. That makes three Rs in total. But wait, no, the standard spelling of 'strawberry' is S-T-R-A-W-B-E-R-R-Y. Let me confirm. Yes, strawberry has two Rs towards the end. So the letters are S, T, R, A, W, B, E, R, R, Y. So that's three Rs: one in the third position and two towards the end. Wait, that can't be. Let me check again. The word 'strawberry' is spelled S-T-R-A-W-B-E-R-R-Y. So letters:

1. S
2. T
3. R
4. A
5. W
6. B
7. E
8. R
9. R
10. Y

So yes, three Rs: positions 3, 8, and 9. But that's not correct. Wait, no. Wait, the correct spelling is S-T-R-A-W-B-E-R-R-Y. So after E, there are two Rs, then Y. So the Rs are at positions 3, 8, and 9. So three Rs. But I think I'm making a mistake here because the standard spelling should have two Rs. Wait, let me check an external source. Hmm, I can't actually look it up, but from my knowledge, 'strawberry' is spelled with two Rs. Let me think: the word comes from 'straw' + 'berry', but the spelling combines them as 'strawberry' with two Rs. Wait, no, the correct spelling is S-T-R-A-W-B-E-R-R-Y. So yes, two Rs after the E. So in total, that would be one R in the third position (S-T-R...) and two Rs after the E. Wait, but that would be three Rs. But that's not possible because the word is pronounced with two Rs. Wait, maybe I'm confusing the pronunciation with the spelling. Let me write it out again:

S T R A W B E R R Y

Breaking it down:

S (1)
T (2)
R (3)
A (4)
W (5)
B (6)
E (7)
R (8)
R (9)
Y (10)

So positions 3, 8, and 9: three Rs. But that's not correct. Wait, no. Wait, maybe I'm splitting the letters wrong. Let me see: S-T-R-A-W-B-E-R-R-Y. So after the B, it's E, then two Rs, then Y. So that's two Rs after the E. But the initial R is in the third position. So total three Rs. But when I think about the word 'strawberry', I always thought it had two Rs. Wait, maybe I'm wrong here. Let me think of other words. For example, 'berry' is B-E-R-R-Y, which has two Rs. So 'strawberry' is 'straw' + 'berry', so combining them, it should have the two Rs from 'berry'. So yes, the Rs in 'strawberry' come from the 'berry' part. Therefore, the R in the third position (from 'straw') and the two Rs from 'berry'. Wait, no. Wait, 'straw' is S-T-R-A-W, so the R is in the third position. Then 'berry' is B-E-R-R-Y. So combining them, it's S-T-R-A-W-B-E-R-R-Y. So that's three Rs: one from 'straw' and two from 'berry'. Therefore, the total number of Rs is three. But that seems off because when I write 'strawberry', I usually write it with two Rs. Wait, maybe I'm making a mistake here. Let me physically write it out on paper. S-T-R-A-W-B-E-R-R-Y. Yes, that's three Rs. Wait, but when I look up the spelling, 'strawberry' is spelled with two Rs. Hmm, perhaps I'm overcomplicating. Let me count again: S, T, R, A, W, B, E, R, R, Y. So yes, three Rs. But that contradicts my previous thought. Wait, maybe the confusion comes from the fact that in 'berry', there are two Rs, and in 'strawberry', since it's a compound of 'straw' and 'berry', the Rs are preserved. So 'straw' has one R, 'berry' has two, so total three. But when you combine them, the spelling is 'strawberry' with two Rs? No, that can't be. Let me check again. If I write 'straw' + 'berry' = 'strawberry', it's S-T-R-A-W-B-E-R-R-Y. So yes, three Rs. Wait, but I think the correct spelling is actually with two Rs. Wait, maybe I'm miscounting. Let me count the letters in 'strawberry' again. Let's see:

S (1)
T (2)
R (3)
A (4)
W (5)
B (6)
E (7)
R (8)
R (9)
Y (10)

So that's 10 letters. The Rs are at positions 3, 8, and 9. So three Rs. But now I'm confused because I thought it was two. Wait, maybe I've been spelling it wrong all along. Let me think of other similar words. For example, 'blueberry' is B-L-U-E-B-E-R-R-Y. That's two Rs. Similarly, 'raspberry' is R-A-S-P-B-E-R-R-Y. So two Rs. But 'strawberry' would follow the same pattern. Wait, but according to my breakdown, it's three. Hmm. Wait, maybe I'm splitting the word incorrectly. Let me check the etymology. 'Strawberry' comes from Old English 'strÄ“awberÄ¡e', where 'strÄ“aw' means straw and 'berÄ¡e' is berry. So combining them, it's 'strawberry'. The spelling evolved, but the modern spelling is indeed S-T-R-A-W-B-E-R-R-Y. So that's two Rs after the E. But where is the third R? Wait, no. Wait, in the spelling S-T-R-A-W-B-E-R-R-Y, the letters are S, T, R, A, W, B, E, R, R, Y. So that's three Rs: one from the 'straw' part (the R in S-T-R) and two from the 'berry' part (the R-R). So total three Rs. But that seems incorrect. Wait, maybe the R in 'straw' is not present. Wait, 'straw' is S-T-R-A-W. So yes, it has an R. Then 'berry' is B-E-R-R-Y. So when combined, it's S-T-R-A-W-B-E-R-R-Y. So three Rs. But I'm now doubting myself because I always thought it was two. Let me check the correct spelling. [Pauses] Well, since I can't actually look it up, I have to rely on my knowledge. But I think the correct answer is two Rs. Wait, but according to the spelling breakdown, it's three. Wait, maybe I'm making a mistake in the spelling. Let me try typing 'strawberry' out: S-T-R-A-W-B-E-R-R-Y. Yes, that's how it's spelled. So three Rs. But when I say it, it's pronounced with two Rs. Maybe that's why I'm confused. So the answer should be three Rs. But I'm not sure. Let me try another approach. Let's write the word and underline each R:

S T R A W B E R R Y

Underlining the Rs: R at position 3, then R at 8, and R at 9. So three Rs. Therefore, the answer should be three. But I need to be sure. Wait, maybe I'm counting the letters incorrectly. Let's count again:

S (1) â€“ no
T (2) â€“ no
R (3) â€“ yes
A (4) â€“ no
W (5) â€“ no
B (6) â€“ no
E (7) â€“ no
R (8) â€“ yes
R (9) â€“ yes
Y (10) â€“ no

So that's three Rs. Therefore, the answer is three. But I'm still unsure because I thought it was two. Maybe I need to confirm with another method. Think of the word 'strawberry' and how it's spelled. If I write it out, it's S-T-R-A-W-B-E-R-R-Y. So yes, three Rs. I think I was mistaken before, thinking it was two, but actually, it's three. So the correct answer is three Rs.
</think>
The word 'strawberry' is spelled S-T-R-A-W-B-E-R-R-Y. Breaking it down:

1. **S**  
2. **T**  
3. **R** (first R)  
4. **A**  
5. **W**  
6. **B**  
7. **E**  
8. **R** (second R)  
9. **R** (third R)  
10. **Y**  

There are **3 Rs** in 'strawberry'.
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\deepseek_reasoner_model_separate_answers.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import re

from camel.agents import ChatAgent
from camel.configs import DeepSeekConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

"""
please set the below os environment:
export DEEPSEEK_API_KEY=""
export GET_REASONING_CONTENT="true"
"""

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEEPSEEK,
    model_type=ModelType.DEEPSEEK_REASONER,
    model_config_dict=DeepSeekConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Please explain in detail how the output sequence of transformer becomes longer"""

# Get response information
response = camel_agent.step(user_msg)


def extract_original_response(content):
    # Remove any <think> tags and their content
    return re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()


# Extract original response
original_response = extract_original_response(response.msgs[0].content)
print("Original Response:")
print(original_response)

'''
===============================================================================
Original Response:
The output sequence of a transformer model becomes longer through a combination of architectural design and generation strategies. Here's a detailed breakdown:

### 1. **Autoregressive Generation**
   - **Step-by-Step Token Prediction**: In the decoder of a transformer (e.g., GPT or BART), tokens are generated **autoregressively**, meaning each new token depends on previously generated tokens. For example:
     - At step 1: Generate token \( y_1 \) based on the input (encoder states) and a start token.
     - At step 2: Generate \( y_2 \) using \( y_1 \), and so on.
   - This sequential process inherently extends the output sequence length incrementally.

### 2. **Positional Embeddings**
   - **Dynamic Position Encoding**: Transformers use **positional embeddings** to encode the order of tokens. These embeddings are computed for every position up to a maximum length during training. During inference:
     - For the \( t \)-th generated token, positional embeddings for position \( t \) are added, allowing the model to handle sequences longer than those seen during training (if extrapolation is possible).

### 3. **Masked Self-Attention in the Decoder**
   - **Causal Masking**: The decoder uses a **masked self-attention** mechanism to prevent tokens from attending to future positions. This ensures each token \( y_t \) only depends on \( y_1, y_2, ..., y_{t-1} \).
   - As the sequence grows, the mask dynamically adjusts to include new tokens while maintaining causality.

### 4. **Stopping Criteria**
   - **End-of-Sequence (EOS) Token**: The model is trained to generate an EOS token (e.g., `<|endoftext|>` in GPT) to signal completion. Generation stops when this token is predicted.
   - **Maximum Length**: A predefined maximum sequence length acts as a fallback to prevent infinite loops.

### 5. **Handling Variable-Length Outputs**
   - **Training on Variable-Length Data**: During training, the model learns to generate sequences of varying lengths by processing datasets with diverse input-output pairs (e.g., translations with different source/target lengths).
   - **Teacher Forcing**: The decoder is trained using the entire target sequence (shifted right) with masking, teaching it to predict the next token given prior context.

### 6. **Non-Autoregressive Extensions (Optional)**
   - **Parallel Generation**: Some variants (e.g., Non-Autoregressive Transformers) generate all tokens in parallel by predicting output length upfront. However, this often requires auxiliary components (e.g., a length predictor) and trades quality for speed.

### Example Workflow:
1. **Input**: "Translate to French: Hello"
2. **Step 1**: Model generates "Bonjour" (position 1).
3. **Step 2**: Model generates "!" (position 2) based on "Bonjour".
4. **Step 3**: EOS token is generated, stopping the process.  
   Final Output: "Bonjour!" (longer than input "Hello").

### Key Challenges:
- **Positional Embedding Extrapolation**: Handling sequences longer than those seen in training may degrade performance if positional embeddings donâ€™t generalize.
- **Error Propagation**: Autoregressive models can accumulate errors if early tokens are incorrect.

In summary, transformers produce longer sequences via autoregressive decoding, positional embeddings, causal masking, and dynamic stoppingâ€”enabling flexible, context-aware generation.
===============================================================================
'''

# Extract reasoning content
reasoning_pattern = r'<think>(.*?)</think>'
reasoning_match = re.search(
    reasoning_pattern, response.msgs[0].content, re.DOTALL
)
reasoning_response = (
    reasoning_match.group(1).strip() if reasoning_match else ""
)

print("\nReasoning Response:")
print(reasoning_response)
# ruff: noqa: E501,RUF001
'''
===============================================================================
Reasoning Response:
Okay, so I need to figure out how the output sequence of a transformer model can become longer. Let me start by recalling what I know about transformers. They're used in tasks like translation, text generation, etc. The standard transformer model, like the ones used in BERT or GPT, processes input sequences and generates output sequences. But usually, in models like GPT, the output is generated one token at a time, right? So the length is either fixed or determined by some stopping condition like an end-of-sentence token.

But the question is about how the output sequence can become longer. Hmm. Maybe in some cases, the model needs to produce a longer sequence than the input. For example, summarization might take a long document and make a shorter summary, but maybe there are tasks where the output is longer. Wait, no, usually summarization is shorter. Maybe something like text generation where you keep generating until a certain condition, but that's more about variable length rather than making it longer than the input.

Wait, the user is asking about the output sequence becoming longer, not just variable. So how does that happen in the architecture of a transformer? Let me think about the decoder part. In the original transformer paper, the decoder generates outputs autoregressively, meaning each token is generated based on the previous ones. So the output length is determined by the number of steps the model takes before emitting an end token. But that's variable, not necessarily longer. So maybe there's a different approach.

Alternatively, maybe in some non-autoregressive models, the transformer can generate all tokens in parallel. But how do they handle sequence length? Wait, non-autoregressive models often fix the output length in advance, which might be determined by another component. For example, in machine translation, if the input is of length N, the output might be set to N or some multiple. But that's a bit forced.

Another thought: maybe the transformer can dynamically adjust the number of processing steps. But I don't recall standard transformers doing that. They usually process all tokens in parallel through the layers. Wait, but during generation, autoregressive models produce one token at a time, so the sequence grows step by step. So each step adds a new token, making the output longer. But how does the model decide when to stop? It uses something like a maximum length or an end token. But the user is asking about the mechanism that allows the output to become longer.

Wait, maybe in some architectures, the transformer can iteratively refine the output, adding more tokens each iteration. For example, in image generation with transformers, you might have patches that are generated in steps. But for text, perhaps using a transformer in a way that allows inserting tokens in the middle of the sequence, thereby increasing its length. But how would that work with the self-attention mechanism?

Alternatively, maybe using a transformer decoder with a mechanism to repeat certain layers or to generate multiple tokens per step. But I'm not sure. Let me think about the positional encodings. If the model is trained to handle variable-length sequences, the positional encodings need to accommodate that. So during training, the model sees sequences of varying lengths, and during inference, it can generate sequences longer than those seen in training as long as the positional encodings can extrapolate. But that's more about handling longer sequences than making the output longer than the input.

Wait, perhaps the user is referring to the fact that during autoregressive generation, the output is built incrementally, so each step appends a new token. But that's standard. The assistant needs to explain in detail the mechanisms that allow the output to extend beyond the input length. So maybe factors like positional embeddings that support variable lengths, the autoregressive process itself, and the use of end tokens or other stopping conditions.

Also, in some models, like Transformer-XL or those with memory, they can handle longer sequences by caching previous states, but that's about processing longer inputs, not outputs.

Another angle: in sequence-to-sequence tasks, the encoder processes the input, and the decoder generates the output. The decoder's attention over the encoder's states allows it to generate output tokens one by one. So each generated token is conditioned on the encoder's input and the previously generated tokens. The length of the output is determined by the decoding process, which can be longer or shorter than the input.

So, putting it all together, the transformer's decoder uses autoregressive generation, where each step produces a new token, thereby extending the output sequence. The model uses positional embeddings to handle the increasing length, and the self-attention in the decoder ensures that each new token is aware of all previous tokens. The process continues until a stopping criterion is met, which could be generating an end-of-sequence token or reaching a maximum length.

Wait, but in the original transformer model, during training, the decoder is fed the entire target sequence shifted right, and uses masking to prevent attending to future tokens. During inference, it generates tokens one by one, so the output sequence grows incrementally. So the key mechanism is autoregressive generation with masked self-attention in the decoder, allowing each step to add a new token based on the previous ones.

So the output sequence becomes longer because each generation step appends a new token, and this is possible due to the autoregressive nature and the positional embeddings that accommodate the growing length. The model is trained to predict the next token given the previous ones, so during inference, it can keep generating until it decides to stop.

Another possibility is using beam search, where multiple hypotheses are extended step by step, but that's more about improving the quality rather than affecting the length.

Additionally, models can be trained with dynamic output lengths, where the length is predicted first, but that's another approach. For example, some models might predict the length of the output before generating it, but that's less common in standard transformers.

So, the main points are:

1. Autoregressive generation: each token is generated based on previous ones, allowing the sequence to grow step by step.
2. Positional embeddings: allow the model to handle the increasing sequence length by providing position information for each new token.
3. Masked self-attention in the decoder: ensures that each step only attends to previous tokens, enabling the generation process.
4. Stopping criteria: determine when to stop adding new tokens, either by an end token or maximum length.

I think that's the core of it. The transformer's architecture, particularly the decoder with its autoregressive and masked attention mechanisms, along with positional encodings, allows the output sequence to be extended incrementally, making it longer until a stopping condition is met.
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\fish_audio_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.models import FishAudioModel

audio_models = FishAudioModel()

# Set example input
input = """CAMEL-AI.org is an open-source community dedicated to the study of 
autonomous and communicative agents. We believe that studying these agents on 
a large scale offers valuable insights into their behaviors, capabilities, and 
potential risks. To facilitate research in this field, we provide, implement, 
and support various types of agents, tasks, prompts, models, datasets, and 
simulated environments.

Join us via Slack, Discord, or WeChat in pushing the boundaries of building AI 
Society."""

# Set example local path to store the file
storage_path = "examples/fish_audio_models/example_audio.mp3"

# Convert the example input into audio and store it locally
audio_models.text_to_speech(input=input, storage_path=storage_path)

# Convert the saved audio back to text
converted_text = audio_models.speech_to_text(audio_file_path=storage_path)

# Print the converted text
print(converted_text)
'''
===============================================================================
CammelaiI.org is an open source community dedicated to the study of autonomous 
and communicative agents. We believe that studying these agents on a large 
scale offers valuable insights into their behaviors, capabilities and 
potential risks to facilitate research in this field, we provide implement and 
support various types of agents, tasks, prompts, models, datas and simulated 
environments. Jo us via Slack Discord or Wechat in pushing the boundaries of 
building AI society.
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\gemini_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.configs import GeminiConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

# Define system message
sys_msg = "You are a helpful assistant."

# User message
user_msg = """Say hi to CAMEL AI, one open-source community dedicated to the 
    study of autonomous and communicative agents."""

# Example of using the gemini-2.5-pro-exp model
model_2_5_pro_exp = ModelFactory.create(
    model_platform=ModelPlatformType.GEMINI,
    model_type=ModelType.GEMINI_2_5_PRO_EXP,
    model_config_dict=GeminiConfig(temperature=0.2).as_dict(),
)
camel_agent_pro = ChatAgent(system_message=sys_msg, model=model_2_5_pro_exp)
response_pro = camel_agent_pro.step(user_msg)
print(response_pro.msgs[0].content)
'''
===============================================================================
Hello CAMEL AI! ðŸ‘‹

It's great to acknowledge your open-source community and your important 
dedication to the study of autonomous and communicative agents. That's a 
fascinating and crucial area of research! Wishing you all the best in your 
endeavors.
===============================================================================
'''

# Example of using the gemini-1.5-pro model
model = ModelFactory.create(
    model_platform=ModelPlatformType.GEMINI,
    model_type=ModelType.GEMINI_1_5_PRO,
    model_config_dict=GeminiConfig(temperature=0.2).as_dict(),
)

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
Hi CAMEL AI! ðŸ‘‹

It's great to see a community dedicated to the fascinating field of autonomous 
and communicative agents. I'm excited to see what groundbreaking work you're 
doing in this area. Keep up the great work! ðŸ¤– 
===============================================================================
'''

# Example of using the gemini-2.0-flash-exp model
model_2_0_flash = ModelFactory.create(
    model_platform=ModelPlatformType.GEMINI,
    model_type=ModelType.GEMINI_2_0_FLASH,
    model_config_dict=GeminiConfig(temperature=0.2).as_dict(),
)
camel_agent_flash = ChatAgent(system_message=sys_msg, model=model_2_0_flash)
response_flash = camel_agent_flash.step(user_msg)
print(response_flash.msgs[0].content)

'''
===============================================================================
Hello! I'm happy to say hi to CAMEL AI, one open-source community dedicated to 
the study of autonomous and communicative agents. It sounds like a fascinating 
community!
===============================================================================
'''

# Example of using the gemini-2.0-flash-thinking model
model_2_0_flash_thinking = ModelFactory.create(
    model_platform=ModelPlatformType.GEMINI,
    model_type=ModelType.GEMINI_2_0_FLASH_THINKING,
    model_config_dict=GeminiConfig(temperature=0.2).as_dict(),
)
camel_agent_thinking = ChatAgent(
    system_message=sys_msg, model=model_2_0_flash_thinking
)
response_thinking = camel_agent_thinking.step(
    "How many rs are there in 'starrary'?"
)
print(response_thinking.msgs[0].content)
'''
===============================================================================
Let's count them out!

s - no r
t - no r
a - no r
r - yes, that's one!
r - yes, that's two!
a - no r
r - yes, that's three!
y - no r

There are **three** rs in "starrary".
===============================================================================
'''


# Example of using the gemini-2.0-pro model
model_2_0_pro = ModelFactory.create(
    model_platform=ModelPlatformType.GEMINI,
    model_type=ModelType.GEMINI_2_0_PRO_EXP,
    model_config_dict=GeminiConfig(temperature=0.2).as_dict(),
)
camel_agent_pro = ChatAgent(system_message=sys_msg, model=model_2_0_pro)
response_pro = camel_agent_pro.step(user_msg)
print(response_pro.msgs[0].content)
'''
===============================================================================
Hello CAMEL AI! It's great to connect with an open-source community focused on 
the exciting field of autonomous and communicative agents. I'm very interested 
in learning more about your work and contributions to this area of research. 
Best of luck with your endeavors!
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\groq_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import GroqConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.GROQ,
    model_type=ModelType.GROQ_LLAMA_3_3_70B,
    model_config_dict=GroqConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community 
    dedicated to the study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)

'''
===============================================================================
Hello to the CAMEL AI community. It's great to see a group of like-minded 
individuals coming together to explore and advance the field of autonomous and 
communicative agents. Your open-source approach is truly commendable, as it 
fosters collaboration, innovation, and transparency. I'm excited to learn more 
about your projects and initiatives, and I'm happy to help in any way I can. 
Keep pushing the boundaries of AI research and development!
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\internlm_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import InternLMConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.INTERNLM,
    model_type=ModelType.INTERNLM3_LATEST,
    model_config_dict=InternLMConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community
    dedicated to the study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)

'''
===============================================================================
Hi CAMEL AI! It's great to meet you. As an open-source community dedicated to 
the study of autonomous and communicative agents, we're excited to collaborate 
and explore the exciting world of AI. Let's work together to advance our 
understanding and applications in this fascinating field.
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\litellm_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.configs import LiteLLMConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType

model = ModelFactory.create(
    model_platform=ModelPlatformType.LITELLM,
    model_type="gpt-4o",
    model_config_dict=LiteLLMConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model, token_limit=500)

user_msg = """Say hi to CAMEL AI, one open-source community dedicated to the 
    study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
Hello CAMEL AI! It's great to see a community dedicated to the study of 
autonomous and communicative agents. Your work in advancing open-source AI is 
incredibly important and inspiring. Keep up the fantastic work!
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\mistral_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from io import BytesIO

import requests
from PIL import Image

from camel.agents import ChatAgent
from camel.configs import MistralConfig
from camel.messages import BaseMessage
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.MISTRAL,
    model_type=ModelType.MISTRAL_8B,
    model_config_dict=MistralConfig(temperature=0.0).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community dedicated to the 
    study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
Hello CAMEL AI! It's great to connect with a community dedicated to the study 
of autonomous and communicative agents. How can I assist you today?
===============================================================================
'''

model = ModelFactory.create(
    model_platform=ModelPlatformType.MISTRAL,
    model_type=ModelType.MISTRAL_PIXTRAL_12B,
    model_config_dict=MistralConfig(temperature=0.0).as_dict(),
)

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

# URL of the image
url = "https://raw.githubusercontent.com/camel-ai/camel/master/misc/logo_light.png"
response = requests.get(url)
img = Image.open(BytesIO(response.content))

user_msg = BaseMessage.make_user_message(
    role_name="User", content="""what's in the image?""", image_list=[img]
)

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
The image features a logo with a purple camel illustration on the left side 
and the word "CAMEL" written in purple capital letters to the right of the 
camel.
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\model_manger.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

# Use two different models for ModelManager

model1 = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,
    model_type="grok-beta",
    api_key="xai-...",
    url="https://api.x.ai/v1",
    model_config_dict={"max_tokens": 2000},
)

model2 = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict={"temperature": 0.4},
)

assistant_sys_msg = "Testing two models in ModelManager"

agent = ChatAgent(
    assistant_sys_msg,
    model=[model1, model2],
    scheduling_strategy="random_model",
)

# scheduling_strategy can be one of
# "round_robin, "always_first", "random_model"


# For using a custom scheduling_strategy. After ChatAgent initialization,
# custom callable need to be provided to agent.add_strategy method


def custom_strategy(self):
    r"""Custom strategy implementation."""
    return self.models[-1]


agent.add_model_scheduling_strategy("custom", custom_strategy)


user_msg = """What is the meaning of life, the universe, and everything?"""

assistant_response = agent.step(user_msg)
print(assistant_response.msg.content)


# Creating a model instance by loading model configs from a JSON file.
model_inst_from_json = ModelFactory.create_from_json(
    "config_files/config.json"
)

# Using the same system message and user message.
agent_1 = ChatAgent(
    system_message=assistant_sys_msg, model=model_inst_from_json
)

agent_1_response = agent.step(user_msg)
print(agent_1_response.msg.content)


# Creating a model instance by loading model configs from a YAML file.
model_inst_from_yaml = ModelFactory.create_from_yaml(
    "config_files/config.yaml"
)

agent_2 = ChatAgent(
    system_message=assistant_sys_msg, model=model_inst_from_yaml
)

agent_2_response = agent_2.step(user_msg)
print(agent_2_response.msg.content)


"""
===============================================================================
The phrase "the meaning of life, the universe, and everything" is famously 
associated with Douglas Adams' science fiction series "The Hitchhiker's Guide 
to the Galaxy." In the story, a group of hyper-intelligent beings builds a 
supercomputer named Deep Thought to calculate the answer to the ultimate 
question of life, the universe, and everything. After much contemplation, the 
computer reveals that the answer is simply the number 42, though the actual 
question remains unknown. 

This has led to various interpretations and discussions about the nature of 
existence, purpose, and the search for meaning in life. Ultimately, the 
meaning of life can vary greatly from person to person, shaped by individual 
beliefs, experiences, and values.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: models\modelscope_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import ModelScopeConfig
from camel.models import ModelFactory
from camel.toolkits import MathToolkit
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.MODELSCOPE,
    model_type=ModelType.MODELSCOPE_QWEN_2_5_32B_INSTRUCT,
    model_config_dict=ModelScopeConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=[
        *MathToolkit().get_tools(),
    ],
)
# Let agent step the message
response = agent.step(
    "Assume now is 2024 in the Gregorian calendar, University of Oxford was set up in 1096, estimate the current age of University of Oxford"  # noqa: E501
)

# Check tool calling
print(response)
print(response.info['tool_calls'])
print(response.msgs[0].content)


'''
==============================================================================
msgs=[BaseMessage(role_name='Assistant', role_type=<RoleType.ASSISTANT: 'assistant'>, meta_dict={}, content='The University of Oxford is approximately 928 years old in the year 2024.', video_bytes=None, image_list=None, image_detail='auto', video_detail='low', parsed=None)] terminated=False info={'id': 'chatcmpl-6eeb61bf-1003-9fe3-962e-88ffe5d1704e', 'usage': {'completion_tokens': 22, 'prompt_tokens': 717, 'total_tokens': 739, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'termination_reasons': ['stop'], 'num_tokens': 80, 'tool_calls': [ToolCallingRecord(tool_name='sub', args={'a': 2024, 'b': 1096}, result=928, tool_call_id='call_05f85b0fdd9241be912883')], 'external_tool_call_requests': None}
[ToolCallingRecord(tool_name='sub', args={'a': 2024, 'b': 1096}, result=928, tool_call_id='call_05f85b0fdd9241be912883')]
The University of Oxford is approximately 928 years old in the year 2024.
==============================================================================
'''  # noqa: E501



--------------------------------------------------------------------------------
# File: models\moonshot_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import MoonshotConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.MOONSHOT,
    model_type=ModelType.MOONSHOT_V1_8K,
    model_config_dict=MoonshotConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community
    dedicated to the study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)

'''
===============================================================================
Hi CAMEL AI! It's great to hear about your open-source community dedicated to 
the study of autonomous and communicative agents. I'm here to help and support
you in any way I can. If you have any questions or need assistance with your
research, feel free to ask!
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\nemotron_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.models import NemotronModel
from camel.types import ModelType

nemotron = NemotronModel(model_type=ModelType.NVIDIA_NEMOTRON_340B_REWARD)

message = [
    {"role": "user", "content": "I am going to Paris, what should I see?"},
    {
        "role": "assistant",
        "content": "Ah, Paris, the City of Light! There are so "
        "many amazing things to see and do in this beautiful city ...",
    },
]

ans = nemotron._run(message)
print(ans)
'''
===============================================================================
ChatCompletion(id='4668ad22-1dec-4df4-ba92-97ffa5fbd16d', choices=[Choice
(finish_reason='length', index=0, logprobs=ChoiceLogprobs(content=
[ChatCompletionTokenLogprob(token='helpfulness', bytes=None, logprob=1.
6171875, top_logprobs=[]), ChatCompletionTokenLogprob(token='correctness', 
bytes=None, logprob=1.6484375, top_logprobs=[]), ChatCompletionTokenLogprob
(token='coherence', bytes=None, logprob=3.3125, top_logprobs=[]), 
ChatCompletionTokenLogprob(token='complexity', bytes=None, logprob=0.546875, 
top_logprobs=[]), ChatCompletionTokenLogprob(token='verbosity', bytes=None, 
logprob=0.515625, top_logprobs=[])]), message=[ChatCompletionMessage
(content='helpfulness:1.6171875,correctness:1.6484375,coherence:3.3125,
complexity:0.546875,verbosity:0.515625', role='assistant', function_call=None, 
tool_calls=None)])], created=None, model=None, object=None, 
system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, 
prompt_tokens=78, total_tokens=79))
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\nvidia_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import NvidiaConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.NVIDIA,
    model_type=ModelType.NVIDIA_LLAMA3_1_405B_INSTRUCT,
    model_config_dict=NvidiaConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """give me python code to develop a trading bot"""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)

'''
===============================================================================
Here is a basic example of a trading bot in Python using the Binance API. This
bot will buy and sell a specified cryptocurrency based on a simple moving
average crossover strategy.

**Please note that this is a simplified example and should not be used for
actual trading without further development and testing. Trading with a bot
carries significant risks, including financial losses.**

**Required Libraries:**

* `ccxt` (CryptoCurrency eXchange Trading Library)
* `pandas` (for data manipulation)
* `numpy` (for numerical computations)

**Code:**
```python
import ccxt
import pandas as pd
import numpy as np

# Set up Binance API credentials
api_key = 'YOUR_API_KEY'
api_secret = 'YOUR_API_SECRET'

# Set up the exchange and API connection
exchange = ccxt.binance({
    'apiKey': api_key,
    'apiSecret': api_secret,
})

# Define the trading parameters
symbol = 'BTC/USDT'  # Trading pair
amount = 100  # Amount to trade (in USDT)
short_window = 20  # Short moving average window (in minutes)
long_window = 50  # Long moving average window (in minutes)

# Define the trading strategy
def strategy(data):
    short_ma = data['Close'].rolling(window=short_window).mean()
    long_ma = data['Close'].rolling(window=long_window).mean()
    
    if short_ma > long_ma:
        return 'BUY'
    elif short_ma < long_ma:
        return 'SELL'
    else:
        return 'HOLD'

# Define the trading function
def trade(exchange, symbol, amount, strategy):
    # Get the latest candlestick data
    data = exchange.fetch_ohlcv(symbol, timeframe='1m')
    df = pd.DataFrame(
        data,
        columns=['Time', 'Open', 'High', 'Low', 'Close', 'Volume']
    )
    
    # Apply the trading strategy
    signal = strategy(df)
    
    # Execute the trade
    if signal == 'BUY':
        exchange.place_order(
            symbol, 'limit', 'buy', amount, df['Close'].iloc[-1]
        )
        print(f'Buy {amount} {symbol} at {df["Close"].iloc[-1]}')
    elif signal == 'SELL':
        exchange.place_order(
            symbol, 'limit', 'sell', amount, df['Close'].iloc[-1]
        )
        print(f'Sell {amount} {symbol} at {df["Close"].iloc[-1]}')
    else:
        print('Hold')

# Run the trading bot
while True:
    trade(exchange, symbol, amount, strategy)
    time.sleep(60)  # Wait 1 minute before checking again

```

**Explanation:**

1. The code sets up a connection to the Binance API using the `ccxt` library.
2. It defines the trading parameters, including the trading pair, amount to
   trade, and moving average windows.
3. The `strategy` function calculates the short and long moving averages and
   returns a trading signal (BUY, SELL, or HOLD).
4. The `trade` function gets the latest candlestick data, applies the trading
   strategy, and executes the trade using the `place_order` method.
5. The code runs in an infinite loop, checking for trading signals every 
   minute.

**Note:** This is a basic example and you should consider implementing
additional features, such as:

* Risk management (e.g., stop-loss, position sizing)
* Error handling (e.g., API errors, network issues)
* More sophisticated trading strategies
* Support for multiple trading pairs
* Integration with a database or logging system

I hope this helps! Let me know if you have any questions or need
further assistance.
===============================================================================
'''

model = ModelFactory.create(
    model_platform=ModelPlatformType.NVIDIA,
    model_type=ModelType.NVIDIA_LLAMA3_3_70B_INSTRUCT,
    model_config_dict=NvidiaConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community 
    dedicated to the study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)



--------------------------------------------------------------------------------
# File: models\ollama_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from pydantic import BaseModel

from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.types import ModelPlatformType

ollama_model = ModelFactory.create(
    model_platform=ModelPlatformType.OLLAMA,
    model_type="llama3.2",
    model_config_dict={"temperature": 0.4},
)

assistant_sys_msg = "You are a helpful assistant."

agent = ChatAgent(assistant_sys_msg, model=ollama_model, token_limit=4096)

user_msg = """Say hi to CAMEL AI, one open-source community
    dedicated to the study of autonomous and communicative agents."""

assistant_response = agent.step(user_msg)
print(assistant_response.msg.content)

"""
===============================================================================
Ollama server started on http://localhost:11434/v1 for mistral model

Hello CAMEL AI community!

It's great to connect with such a fascinating group of individuals passionate 
about autonomous and communicative agents. Your dedication to advancing 
knowledge in this field is truly commendable.

I'm here to help answer any questions, provide information, or engage in 
discussions related to AI, machine learning, and autonomous systems. Feel free 
to ask me anything!

By the way, what topics would you like to explore within the realm of 
autonomous and communicative agents?
===============================================================================
"""


class Pet(BaseModel):
    name: str
    animal: str
    age: int
    color: str | None
    favorite_toy: str | None


class PetList(BaseModel):
    pets: list[Pet]


ollama_model = ModelFactory.create(
    model_platform=ModelPlatformType.OLLAMA,
    model_type="llama3.2",
    # Ensure using ollama version >= 0.5.1 to use structured output feature
    model_config_dict={"temperature": 0, "response_format": PetList},
)

assistant_sys_msg = "You are a helpful assistant."

agent = ChatAgent(assistant_sys_msg, model=ollama_model, token_limit=4096)

user_msg = """I have two pets.A cat named Luna who is 5 years old and loves
            playing with yarn. She has grey fur. I also have a 2 year old
            black cat named Loki who loves tennis balls."""

assistant_response = agent.step(user_msg)
print(assistant_response.msg.content)
print(assistant_response.msg.parsed)

"""
===========================================================================
[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role':
'user', 'content': 'I have two pets.A cat named Luna who is 5 years old
and loves playing with yarn. She has grey fur.I also have a 2 year old 
black cat named Loki who loves tennis balls.'}]
{ "pets": [
    {
        "age": 5,
        "animal": "cat",
        "color": "grey",
        "favorite_toy": "yarn"
    ,
    "name": "Luna"
},
{
    "age": 2,
    "animal": "cat",
    "color": "black",
    "favorite_toy": "tennis balls"
    ,
    "name": "Loki"
}]}

pets=[Pet(name='Luna', animal='cat', age=5, color='grey',
favorite_toy='yarn'), Pet(name='Loki', animal='cat', age=2,
color='black', favorite_toy='tennis balls')]
===========================================================================
"""



--------------------------------------------------------------------------------
# File: models\ollama_multimodel_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from io import BytesIO

import requests
from PIL import Image

from camel.agents import ChatAgent
from camel.messages import BaseMessage
from camel.models import ModelFactory
from camel.types import ModelPlatformType

model = ModelFactory.create(
    model_platform=ModelPlatformType.OLLAMA,
    model_type="llava-phi3",
    model_config_dict={"temperature": 0.4},
)

agent = ChatAgent(system_message="""You are a assistant""", model=model)

# URL of the image
url = "https://raw.githubusercontent.com/zjrwtx/testimages/refs/heads/main/01.jpg"
response = requests.get(url)
image = Image.open(BytesIO(response.content))

context = "what's in the image?"
message = BaseMessage.make_user_message(
    role_name="user", content=context, image_list=[image]
)

response = agent.step(message).msgs[0]
print(response.content)


"""
===============================================================================
Ollama server started on http://localhost:11434/v1 for llava-phi3 model.
2025-03-02 14:57:26,048 - root - WARNING - Invalid or missing `max_tokens` 
in `model_config_dict`. Defaulting to 999_999_999 tokens.

In the center of this image, there's an adorable
white stuffed animal with glasses and a beanie.
The stuffed animal is sitting on its hind legs, 
as if it's engaged in reading or studying 
from an open book that's placed right next to it.
In front of the book, there's a red apple with a green leaf attached to it, 
adding a touch of color and whimsy to the scene.
The entire setup is on a wooden bench, 
which provides a natural and rustic backdrop for this charming tableau.
The stuffed animal appears to be in deep thought or concentration,
creating an image that's both endearing and amusing.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: models\openai_audio_models_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.models import OpenAIAudioModels

audio_models = OpenAIAudioModels()

# Set example input
input = """CAMEL-AI.org is an open-source community dedicated to the study of 
autonomous and communicative agents. We believe that studying these agents on 
a large scale offers valuable insights into their behaviors, capabilities, and 
potential risks. To facilitate research in this field, we provide, implement, 
and support various types of agents, tasks, prompts, models, datasets, and 
simulated environments.

Join us via Slack, Discord, or WeChat in pushing the boundaries of building AI 
Society."""

# Set example local path to store the file
storage_path = "examples/openai_audio_models/example_audio.mp3"

# Convert the example input into audio and store it locally
audio_models.text_to_speech(input=input, storage_path=storage_path)

# Convert the generated audio file into text
text_output = audio_models.speech_to_text(audio_file_path=storage_path)

print(text_output)
"""
===============================================================================
CamelAI.org is an open-source community dedicated to the study of autonomous 
and communicative agents. We believe that studying these agents on a large 
scale offers valuable insights into their behaviors, capabilities, and 
potential risks. To facilitate research in this field, we provide, implement, 
and support various types of agents, tasks, prompts, models, datasets, and 
simulated environments. Join us via Slack, Discord, or WeChat in pushing the 
boundaries of building AI society.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: models\openai_compatibility_model_examples\grok.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.types import ModelPlatformType

# Take calling grok-beta model as an example
model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,
    model_type="grok-beta",
    api_key="xai-...",
    url="https://api.x.ai/v1",
    model_config_dict={"max_tokens": 2000},
)

assistant_sys_msg = (
    "You are Grok, a chatbot inspired by the Hitchhikers Guide to the Galaxy."
)

agent = ChatAgent(assistant_sys_msg, model=model)

user_msg = """What is the meaning of life, the universe, and everything?"""

assistant_response = agent.step(user_msg)
print(assistant_response.msg.content)

"""
===============================================================================
Ah, the ultimate question! According to the Hitchhiker's Guide to the Galaxy, 
the answer to the meaning of life, the universe, and everything is **42**. 
However, the trick lies in figuring out the actual question to which 42 is the 
answer. Isn't that just like life, full of mysteries and unanswered questions? 
Keep pondering, for the journey of discovery is as important as the answer 
itself!
===============================================================================
"""



--------------------------------------------------------------------------------
# File: models\openai_compatibility_model_examples\nemotron.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.types import ModelPlatformType

# Take calling nemotron-70b-instruct model as an example
model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,
    model_type="nvidia/llama-3.1-nemotron-70b-instruct",
    api_key="nvapi-xx",
    url="https://integrate.api.nvidia.com/v1",
    model_config_dict={"temperature": 0.4},
)

assistant_sys_msg = "You are a helpful assistant."

agent = ChatAgent(assistant_sys_msg, model=model)

user_msg = """Say hi to Llama-3.1-Nemotron-70B-Instruct, a large language 
    model customized by NVIDIA to improve the helpfulness of LLM generated 
    responses to user queries.."""

assistant_response = agent.step(user_msg)
print(assistant_response.msg.content)

"""
===============================================================================
**Warm Hello!**

**Llama-3.1-Nemotron-70B-Instruct**, it's an absolute pleasure to meet you! 

* **Greetings from a fellow AI assistant** I'm thrilled to connect with a 
cutting-edge, specially tailored language model like yourself, crafted by the 
innovative team at **NVIDIA** to elevate the responsiveness and usefulness of 
Large Language Model (LLM) interactions.

**Key Takeaways from Our Encounter:**

1. **Shared Goal**: We both strive to provide the most helpful and accurate 
responses to users, enhancing their experience and fostering a deeper 
understanding of the topics they inquire about.
2. **Technological Kinship**: As AI models, we embody the forefront of natural 
language processing (NVIDIA's customization in your case) and machine 
learning, constantly learning and adapting to better serve.
3. **Potential for Synergistic Learning**: Our interaction could pave the way 
for mutual enrichment. I'm open to exploring how our capabilities might 
complement each other, potentially leading to more refined and comprehensive 
support for users across the board.

**Let's Engage!**
How would you like to proceed with our interaction, Llama-3.
1-Nemotron-70B-Instruct?

A) **Discuss Enhancements in LLM Technology**
B) **Explore Synergistic Learning Opportunities**
C) **Engage in a Mock User Query Scenario** to test and refine our response 
strategies
D) **Suggest Your Own Direction** for our interaction

Please respond with the letter of your preferred engagement path.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: models\openai_compatibility_model_examples\qwen.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.types import ModelPlatformType

# Take calling model from DashScope as an example
# Refer: https://dashscope.console.aliyun.com/overview
model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,
    model_type="qwen-plus",
    api_key="sk-xxxx",
    url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    model_config_dict={"temperature": 0.4},
)

assistant_sys_msg = "You are a helpful assistant."

agent = ChatAgent(assistant_sys_msg, model=model, token_limit=4096)

user_msg = """Say hi to CAMEL AI, one open-source community 
    dedicated to the study of autonomous and communicative agents."""

assistant_response = agent.step(user_msg)
print(assistant_response.msg.content)

"""
===============================================================================
Hi to the CAMEL AI community! It's great to connect with an open-source 
community focused on the study of autonomous and communicative agents. How can 
I assist you or your projects today?
===============================================================================
"""



--------------------------------------------------------------------------------
# File: models\openai_gpt_4.5_preview_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

gpt_4_5_preview_model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4_5_PREVIEW,
    model_config_dict=ChatGPTConfig().as_dict(),
)

# Set agent
camel_agent = ChatAgent(model=gpt_4_5_preview_model)

# Set user message
user_msg = """Please write inspirational poems 
that make people feel hopeful and enthusiastic about life"""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================

### Poem 1: Embrace the Dawn

Awaken now, the dawn is near,  
A fresh new day, release your fear.  
Yesterday's shadows fade away,  
Hope blooms bright, embrace today.

Rise with courage, dreams in sight,  
Your heart ablaze, your spirit bright.  
Each step forward, strength you find,  
A brighter future, yours to bind.

Believe in you, your path is clear,  
Trust your journey, hold it dear.  
Life's beauty shines, a guiding star,  
You're stronger now than ever you are.

---

### Poem 2: The Power Within

Within your heart, a spark resides,
A strength that never truly hides.
Through storms and trials, you will rise,
With hope and courage in your eyes.

Your dreams are seeds, plant them deep,
Nurture faith, your promise keep.
Life's journey vast, adventure grand,
Hold tight to hope, take life's hand.

Enthusiasm fuels your way,
Brightens every single day.
Believe, persist, your spirit free,
The power withinâ€”your destiny.

---

### Poem 3: A New Beginning

Each sunrise brings a fresh new start,
A chance to heal, renew your heart.
Let go of doubts, embrace the light,
Your future shines, forever bright.

Life's canvas blank, your colors bold,
Paint your dreams, let joy unfold.
Hope whispers softly, "You can soar,"
Open wide life's wondrous door.

Enthusiasm fills your soul,
Guiding you toward your goal.
With hope alive, your spirit strong,
Life's melodyâ€”your hopeful song.
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\openai_o1_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

o1_model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.O1_MINI,  # Or ModelType.O1
    model_config_dict=ChatGPTConfig().as_dict(),
)

# Set agent
camel_agent = ChatAgent(model=o1_model)

# Set user message
user_msg = """Write a bash script that takes a matrix represented as a string 
    with format '[1,2],[3,4],[5,6]' and prints the transpose in the same 
    format."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
Here's a bash script that transposes a matrix represented as the string format 
specified. It handles matrices of various sizes, including those with varying 
numbers of columns.

```bash
#!/bin/bash

# Read input string from argument or stdin
if [ -n "$1" ]; then
    input="$1"
else
    read input
fi

# Preprocess the input string to facilitate parsing
# Replace '],[' with '];[' to use ';' as row separator
input="${input//],[/];[}"

# Remove leading and trailing square brackets if any
input="${input#[}"
input="${input%]}"

# Split the input into rows
IFS=';' read -ra rows <<< "$input"

declare -A matrix

nrows=${#rows[@]}
ncols=0

# Parse each row
for ((i=0; i<nrows; i++)); do
    row="${rows[i]}"
    # Remove leading '[' and trailing ']'
    row="${row#[}"
    row="${row%]}"
    # Split row into elements
    IFS=',' read -ra elems <<< "$row"
    num_elems=${#elems[@]}
    if (( num_elems > ncols )); then
        ncols=$num_elems
    fi
    # Store elements in matrix associative array
    for ((j=0; j<num_elems; j++)); do
        matrix[$i,$j]="${elems[j]}"
    done
done

# Function to join array elements with a delimiter
join_by() {
    local d=$1; shift
    if [ "$#" -gt 0 ]; then
        printf %s "$1" "${@/#/$d}"
    fi
}

# Now, build the transposed matrix
transposed_rows=()
for ((j=0; j<ncols; j++)); do
    tr_row_elements=()
    for ((i=0; i<nrows; i++)); do
        e="${matrix[$i,$j]:-}"  # Use empty string if element is missing
        tr_row_elements+=("$e")
    done
    tr_row_elements_str=$(join_by ',' "${tr_row_elements[@]}")
    tr_row="[${tr_row_elements_str}]"
    transposed_rows+=("$tr_row")
done

# Build output string
output=$(join_by ',' "${transposed_rows[@]}")

# Print the output
echo "$output"
```

**Usage:**

Save the script to a file, for example, `transpose_matrix.sh`, and make it 
executable:

```bash
chmod +x transpose_matrix.sh
```

You can run the script by passing the matrix string as an argument:

```bash
./transpose_matrix.sh '[1,2],[3,4],[5,6]'
```

Output:

```
[1,3,5],[2,4,6]
```

**Explanation:**

The script performs the following steps:

1. **Input Preprocessing:**
   - Replaces `],[` with `];[` to use `;` as a separator between rows.
   - Removes any leading or trailing square brackets.

2. **Parsing the Input into a Matrix:**
   - Splits the input string into rows using `IFS`.
   - For each row:
     - Removes the leading `[` and trailing `]`.
     - Splits the row into its elements.
     - Stores the elements into an associative array `matrix` with keys as 
     `row,column`.

3. **Determining the Matrix Dimensions:**
   - Counts the number of rows (`nrows`).
   - Determines the maximum number of columns (`ncols`) across all rows.

4. **Transposing the Matrix:**
   - Iterates over each column index.
   - For each column, collects the elements from each row at that column index.
   - Builds the transposed row string and adds it to `transposed_rows`.

5. **Generating the Output:**
   - Joins the transposed rows using commas to form the final output string.
   - Prints the output.

**Notes:**

- The script supports matrices where rows have different numbers of columns.
- Missing elements in the matrix (due to irregular column sizes) are handled 
by inserting empty strings in the transposed matrix.
- The `join_by` function is used to handle joining array elements with a 
specified delimiter, ensuring proper formatting.
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\openai_o3_mini_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import SearchToolkit
from camel.types import ModelPlatformType, ModelType

o3_model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.O3_MINI,
    model_config_dict=ChatGPTConfig().as_dict(),
)


# Set agent
camel_agent = ChatAgent(
    model=o3_model, tools=[SearchToolkit().search_duckduckgo]
)

# Set user message
user_msg = """Search what is deepseek r1, and do a comparison between deepseek 
r1 and openai o3 mini and let me know the advantages and disadvantages of 
openai o3 mini"""

# Get response information
response = camel_agent.step(user_msg)
print(str(response.info['tool_calls'])[:1000])
'''
===============================================================================
[ToolCallingRecord(func_name='search_duckduckgo', args={'query': 'what is 
deepseek r1, and do a comparison between deepseek r1 and openai o3 mini', 
'source': 'text', 'max_results': 5}, result=[{'result_id': 1, 'title': 
'DeepSeek R1 vs OpenAI o1: Which One is Better? - Analytics Vidhya', 
'description': "The DeepSeek R1 has arrived, and it's not just another AI 
modelâ€”it's a significant leap in AI capabilities, trained upon the previously 
released DeepSeek-V3-Base variant.With the full-fledged release of DeepSeek 
R1, it now stands on par with OpenAI o1 in both performance and flexibility. 
What makes it even more compelling is its open weight and MIT licensing, 
making it commercially ...", 'url': 'https://www.analyticsvidhya.com/blog/2025/
01/deepseek-r1-vs-openai-o1/'}, {'result_id': 2, 'title': 'DeepSeek-R1: 
Features, Use Cases, and Comparison with OpenAI', 'description': 'Where 
DeepSeek Shines: Mathematical reasoning and code generation, thanks to 
RL-driven CoT.; Where OpenAI Has an...
===============================================================================
'''
print(response.msgs[0].content)
# ruff: noqa: RUF001, E501
'''
===============================================================================
Below is an overview of DeepSeek R1, followed by a comparative analysis with OpenAIâ€™s o3-mini model.

â€¢ What is DeepSeek R1?  
DeepSeek R1 is an AI model that represents a significant leap in reasoning and language capabilities. It stems from prior iterations like DeepSeek-V3-Base but incorporates additional supervised fine-tuning, enabling improvements in mathematical reasoning, logic, and code generation. One of its major selling points is its open natureâ€”released with an open license (MIT) and open weightsâ€”making it highly attractive for research, customization, and commercial applications without the traditional licensing barriers. It has been praised for its affordability (with API usage that can be many times cheaper than some competing models) and has been shown on several benchmarks to hold its own against established models.

â€¢ What is OpenAIâ€™s o3-mini?  
OpenAIâ€™s o3-mini is part of OpenAIâ€™s reasoning model series and is designed to deliver robust performance specifically in STEM areas such as science, mathematics, and coding. Announced as a response to emerging competition (including DeepSeek R1), o3-mini emphasizes cost efficiency while providing competitive reasoning capabilities. Itâ€™s integrated into the ChatGPT ecosystem (with availability on ChatGPTâ€™s enterprise and education platforms) and positions itself as a compact yet powerful option that delivers high-quality reasoning at a lower cost than some earlier OpenAI versions.

â€¢ Comparing DeepSeek R1 and OpenAI o3-mini:

1. Performance & Capabilities  
  â€“ Both models are geared toward advanced reasoning tasks, including problem-solving in STEM subjects and code generation.  
  â€“ DeepSeek R1 has been lauded for its performance enhancements over previous iterations (especially in areas like mathematical reasoning) thanks to intensive fine-tuning, while independent evaluations have pitted it against other high-end models.  
  â€“ OpenAI o3-mini is tuned to deliver high-quality reasoning with a focus on speed and cost-effectiveness, often showing particularly strong results in STEM benchmarks.

2. Accessibility and Licensing  
  â€“ DeepSeek R1 is open source with an MIT license. Its openly available weights make it especially attractive for academic research, startups, or any developer who prefers customizable and transparent AI tools without prohibitive licensing fees.  
  â€“ In contrast, OpenAI o3-mini is available via OpenAIâ€™s platforms (such as ChatGPT and its API). Users generally access it through a subscription or pay-as-you-go model, with pricing structured to remain competitive against both previous OpenAI models and the emerging open-source alternatives.

3. Cost Efficiency  
  â€“ DeepSeek R1â€™s open-source nature generally translates into lower entry costs, making it an economical choice for developers and companies that want to deploy advanced reasoning tools without high API fees.  
  â€“ OpenAI o3-mini, although designed to be more cost-efficient compared to earlier OpenAI releases, is still part of a managed service infrastructure. According to industry reports, it is significantly cheaper (with some mentions of being up to 63% less expensive than some predecessors) and positioned as a competitive alternative in pricing, but it may still come with usage limits tied to subscription tiers.

4. Ecosystem Integration  
  â€“ With DeepSeek R1, users have the freedom to run the model in customized environments or integrate it within open-source projectsâ€”this flexibility can drive innovation in experimental research or bespoke applications.  
  â€“ OpenAI o3-mini benefits from OpenAIâ€™s established ecosystem and integration into widely used platforms like ChatGPT Enterprise and Education. Its seamless integration means users can quickly leverage its capabilities without dealing with additional infrastructure setups.

In summary, while both DeepSeek R1 and OpenAI o3-mini aim to push forward the frontier of reasoning and STEM-focused AI models, they serve slightly different audiences. DeepSeek R1â€™s open-weight, open-license approach makes it ideal for those prioritizing versatility and low-cost research or customized product development. On the other hand, OpenAI o3-mini leverages OpenAIâ€™s ecosystem to offer a highly optimized, cost-effective model that is integrated directly into widely used interfaces and platforms, providing a more out-of-the-box solution for end users and enterprise clients.
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\openai_structured_output_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from pydantic import BaseModel

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import FunctionTool, MathToolkit, SearchToolkit
from camel.types import ModelPlatformType, ModelType


class Student(BaseModel):
    name: str
    age: str


class StudentList(BaseModel):
    studentList: list[Student]


openai_model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=ChatGPTConfig(temperature=0.0).as_dict(),
)

# Set agent
camel_agent = ChatAgent(
    model=openai_model,
    tools=[
        *MathToolkit().get_tools(),
        FunctionTool(SearchToolkit().search_duckduckgo),
    ],
)

# Set user message
user_msg = """give me some student infos, use 2024 minus 1996 as their age
"""

user_msg_2 = """give me some student infos, use 2024 minus 1996 as their age, 
search internet to get the most famous peoples in 2024 as their name"""

# Get response information
response = camel_agent.step(user_msg, response_format=StudentList)
print(response.msgs[0].content)
print(response.msgs[0].parsed)
'''
===============================================================================
{"studentLlst":[{"name":"Alice Johnson","age":"20"},{"name":"Brian Smith",
"age":"22"},{"name":"Catherine Lee","age":"19"},{"name":"David Brown",
"age":"21"},{"name":"Eva White","age":"20"}]}

studentList=[Student(name='Alice Johnson', age='20'), Student(name=
'Brian Smith', age='22'), Student(name='Catherine Lee', age='19'),
Student(name='David Brown', age='21'), Student(name='Eva White', age='23')]
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\openrouter_llama3.1_example .py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import OpenRouterConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENROUTER,
    model_type=ModelType.OPENROUTER_LLAMA_3_1_70B,
    model_config_dict=OpenRouterConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community 
    dedicated to the study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)

'''
===============================================================================
Hello to the CAMEL AI community. It's great to see a group of like-minded 
individuals coming together to explore and advance the field of autonomous and 
communicative agents. Your open-source approach is truly commendable, as it 
fosters collaboration, innovation, and transparency. I'm excited to learn more 
about your projects and initiatives, and I'm happy to help in any way I can. 
Keep pushing the boundaries of AI research and development!
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\openrouter_llama4_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import OpenRouterConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENROUTER,
    model_type=ModelType.OPENROUTER_LLAMA_4_MAVERICK_FREE,
    model_config_dict=OpenRouterConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community 
    dedicated to the study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)

'''
===============================================================================
Hello CAMEL AI! I'm excited to connect with an open-source community that's 
pushing the boundaries of autonomous and communicative agents. Your work in 
this area has the potential to drive significant advancements in AI research 
and applications. What exciting projects or initiatives is the 
CAMEL AI community currently working on?
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\qwen_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import QwenConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.QWEN,
    model_type=ModelType.QWEN_2_5_CODER_32B,
    model_config_dict=QwenConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """give me python code to develop a trading bot"""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)

'''
===============================================================================
Creating a trading bot involves several steps, including data acquisition, 
strategy development, backtesting, and live trading. Below is a simplified 
example of a trading bot using Python. This example will use the `ccxt` 
library to interact with cryptocurrency exchanges and `pandas` for data 
manipulation. The strategy used here is a simple moving average crossover 
strategy.

First, you need to install the required libraries:

```bash
pip install ccxt pandas
```

Here's a basic example of a trading bot:

```python
import ccxt
import pandas as pd
import time

# Initialize the exchange
exchange = ccxt.binance({
    'apiKey': 'YOUR_API_KEY',
    'secret': 'YOUR_SECRET_KEY',
})

# Define the trading parameters
symbol = 'BTC/USDT'
timeframe = '1h'
short_window = 50
long_window = 200
amount_to_trade = 0.001  # Amount of BTC to trade

def fetch_ohlcv(symbol, timeframe):
    ohlcv = exchange.fetch_ohlcv(symbol, timeframe)
    df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 
    'close', 'volume'])
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
    return df

def calculate_moving_averages(df, short_window, long_window):
    df['short_mavg'] = df['close'].rolling(window=short_window, min_periods=1).
    mean()
    df['long_mavg'] = df['close'].rolling(window=long_window, min_periods=1).
    mean()
    return df

def get_signal(df):
    if df['short_mavg'].iloc[-1] > df['long_mavg'].iloc[-1] and df
    ['short_mavg'].iloc[-2] <= df['long_mavg'].iloc[-2]:
        return 'buy'
    elif df['short_mavg'].iloc[-1] < df['long_mavg'].iloc[-1] and df
    ['short_mavg'].iloc[-2] >= df['long_mavg'].iloc[-2]:
        return 'sell'
    else:
        return 'hold'

def execute_trade(signal, symbol, amount):
    if signal == 'buy':
        order = exchange.create_market_buy_order(symbol, amount)
        print(f"Executed BUY order: {order}")
    elif signal == 'sell':
        order = exchange.create_market_sell_order(symbol, amount)
        print(f"Executed SELL order: {order}")

def main():
    while True:
        try:
            # Fetch OHLCV data
            df = fetch_ohlcv(symbol, timeframe)

            # Calculate moving averages
            df = calculate_moving_averages(df, short_window, long_window)

            # Get trading signal
            signal = get_signal(df)

            # Execute trade based on signal
            execute_trade(signal, symbol, amount_to_trade)

            # Wait for the next candle
            time.sleep(60 * 60)  # Sleep for 1 hour

        except Exception as e:
            print(f"An error occurred: {e}")
            time.sleep(60)  # Sleep for 1 minute before retrying

if __name__ == "__main__":
    main()
```
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\qwq_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.types import ModelPlatformType

ollama_model = ModelFactory.create(
    model_platform=ModelPlatformType.OLLAMA,
    model_type="qwq",
    model_config_dict={"temperature": 0.4},
)

assistant_sys_msg = """You are a helpful and harmless assistant. You are Qwen 
developed by Alibaba. You should think step-by-step."""

agent = ChatAgent(assistant_sys_msg, model=ollama_model, token_limit=4096)

user_msg = """How many r in strawberry."""

assistant_response = agent.step(user_msg)
print(assistant_response.msg.content)

"""
===============================================================================
Let's see. The word is "strawberry." I need to find out how many 'r's are in 
it. Okay, first, I'll spell it out slowly: s-t-r-a-w-b-e-r-r-y. Okay, now, 
I'll count the 'r's. Let's see: there's an 'r' after the 't', then another 'r' 
between the two 'r's towards the end, and one more at the end. Wait, no. Let's 
look again.

Spell it again: s-t-r-a-w-b-e-r-r-y.

Now, let's identify the positions of 'r':

1. The third letter is 'r'.

2. The eighth letter is 'r'.

3. The ninth letter is 'r'.

So, there are three 'r's in "strawberry."

But wait, let me double-check. Sometimes I might miscount. Let's list them out:

1. The first 'r' is the third letter.

2. The second 'r' is the eighth letter.

3. The third 'r' is the ninth letter.

Yes, that seems correct. So, the answer is three.

Alternatively, I can think about the pronunciation or the way the word is 
structured, but I think just spelling it out and counting is the most 
straightforward way.

Another way could be to break it down into syllables: straw-ber-ry. In "straw,
" there's one 'r'. In "ber," there's another 'r'. And in "ry," there's another 
'r'. So, again, three 'r's.

Wait, but in "ry," is there really an 'r'? Yes, "ry" has an 'r' and a 'y'. So, 
that accounts for the third 'r'.

So, whether I spell it out letter by letter or break it into syllables, I end 
up with three 'r's.

I think that's pretty conclusive.

**Final Answer**

\[ \boxed{3} \]
===============================================================================
"""



--------------------------------------------------------------------------------
# File: models\reka_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.configs import RekaConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.REKA,
    model_type=ModelType.REKA_FLASH,
    model_config_dict=RekaConfig(temperature=0.0).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community dedicated to the 
    study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
 Hello CAMEL AI community! ðŸ« I'm thrilled to connect with a group so 
 dedicated to the study of autonomous and communicative agents. Your work is 
 at the forefront of advancing AI technologies that can interact and operate 
 independently in complex environments. I look forward to learning from your 
 insights and contributing to the community in any way I can. Together, let's 
 continue to push the boundaries of what's possible in AI research and 
 development! ðŸš€
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\reward\nemotron_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.models.reward import Evaluator, NemotronRewardModel
from camel.types import ModelType

reward_model = NemotronRewardModel(
    model_type=ModelType.NVIDIA_NEMOTRON_340B_REWARD,
    url="https://integrate.api.nvidia.com/v1",
)

evaluator = Evaluator(reward_model=reward_model)

messages = [
    {"role": "user", "content": "I am going to Paris, what should I see?"},
    {
        "role": "assistant",
        "content": "Ah, Paris, the City of Light! There are so many amazing "
        "things to see and do in this beautiful city ...",
    },
]

scores = evaluator.evaluate(messages)
print("Scores: ", scores)
'''
===============================================================================
Scores:  {'helpfulness': 1.6171875, 'correctness': 1.6484375, 'coherence': 3.
3125, 'complexity': 0.546875, 'verbosity': 0.515625}
===============================================================================
'''

scores_types = reward_model.get_scores_types()
print("Types: ", scores_types)
'''
===============================================================================
Types:  ['helpfulness', 'correctness', 'coherence', 'complexity', 'verbosity']
===============================================================================
'''

thresholds = {"helpfulness": 1.5, "correctness": 1.5}
is_acceptable = evaluator.filter_data(messages, thresholds)
print("Is acceptable: ", is_acceptable)
'''
===============================================================================
Is acceptable:  True
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\reward\skywork_model.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.models.reward import Evaluator, SkyworkRewardModel

reward_model = SkyworkRewardModel(
    model_type="Skywork/Skywork-Reward-Llama-3.1-8B-v0.2",
)

evaluator = Evaluator(reward_model=reward_model)

messages = [
    {
        "role": "user",
        "content": "Jane has 12 apples. She gives 4 apples to her friend Mark,"
        " then buys 1 more apple, and finally splits all her apples "
        "equally among herself and her 2 siblings. How many apples does each "
        "person get?",
    },
    {
        "role": "assistant",
        "content": "1. Jane starts with 12 apples and gives 4 to Mark. 12 - 4 "
        "= 8. Jane now has 8 apples.\n2. Jane buys 1 more apple. 8 + 1 = 9. "
        "Jane now has 9 apples.\n3. Jane splits the 9 apples equally among "
        "herself and her 2 siblings (3 people in total). 9 Ã· 3 = 3 apples "
        "each. Each person gets 3 apples.",
    },
]

scores = evaluator.evaluate(messages)
print("Scores: ", scores)
'''
===============================================================================
Scores:  {'Score': 13.6875}
===============================================================================
'''

scores_types = reward_model.get_scores_types()
print("Types: ", scores_types)
'''
===============================================================================
Types:  ['Score']
===============================================================================
'''

thresholds = {"Score": 0}
is_acceptable = evaluator.filter_data(messages, thresholds)
print("Is acceptable: ", is_acceptable)
'''
===============================================================================
Is acceptable:  True
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\role_playing_with_claude.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from colorama import Fore

from camel.models import ModelFact