ory
from camel.societies import RolePlaying
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated


def main(model_type=None) -> None:
    task_prompt = "Develop a trading bot for the stock market"

    model = ModelFactory.create(
        model_platform=ModelPlatformType.ANTHROPIC,
        model_type=model_type,
    )

    # Update agent_kwargs to use the created models
    agent_kwargs = {
        "assistant": {"model": model},
        "user": {"model": model},
        "task-specify": {"model": model},
    }

    role_play_session = RolePlaying(
        assistant_role_name="Python Programmer",
        assistant_agent_kwargs=agent_kwargs["assistant"],
        user_role_name="Stock Trader",
        user_agent_kwargs=agent_kwargs["user"],
        task_prompt=task_prompt,
        with_task_specify=True,
        task_specify_agent_kwargs=agent_kwargs["task-specify"],
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    chat_turn_limit, n = 50, 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
        )
        print_text_animated(
            Fore.GREEN + "AI Assistant:\n\n"
            f"{assistant_response.msg.content}\n"
        )

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    main(model_type=ModelType.CLAUDE_3_5_SONNET)



--------------------------------------------------------------------------------
# File: models\role_playing_with_cohere.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from typing import List

from colorama import Fore

from camel.agents.chat_agent import ToolCallingRecord
from camel.configs import CohereConfig
from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.toolkits import (
    FunctionTool,
    MathToolkit,
    SearchToolkit,
)
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated


def main(
    model_platform=ModelPlatformType.COHERE,
    model_type=ModelType.COHERE_COMMAND_R_PLUS,
    chat_turn_limit=3,
) -> None:
    task_prompt = (
        "Assume now is 2024 in the Gregorian calendar, "
        "estimate the current age of University of Oxford "
        "and then add 10 more years to this age."
    )

    user_model_config = CohereConfig(temperature=0.2)

    tools_list = [
        *MathToolkit().get_tools(),
        FunctionTool(SearchToolkit().search_duckduckgo),
    ]
    assistant_model_config = CohereConfig(
        temperature=0.2,
    )

    role_play_session = RolePlaying(
        assistant_role_name="Searcher",
        user_role_name="Professor",
        assistant_agent_kwargs=dict(
            model=ModelFactory.create(
                model_platform=model_platform,
                model_type=model_type,
                model_config_dict=assistant_model_config.as_dict(),
            ),
            tools=tools_list,
        ),
        user_agent_kwargs=dict(
            model=ModelFactory.create(
                model_platform=model_platform,
                model_type=model_type,
                model_config_dict=user_model_config.as_dict(),
            ),
        ),
        task_prompt=task_prompt,
        with_task_specify=False,
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    n = 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        # Print output from the user
        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
        )

        # Print output from the assistant, including any function
        # execution information
        print_text_animated(Fore.GREEN + "AI Assistant:")
        tool_calls: List[ToolCallingRecord] = [
            ToolCallingRecord(**call.as_dict())
            for call in assistant_response.info['tool_calls']
        ]
        for func_record in tool_calls:
            print_text_animated(f"{func_record}")
        print_text_animated(f"{assistant_response.msg.content}\n")

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: models\role_playing_with_gemini.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from colorama import Fore

from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated


def main(model_type=None) -> None:
    task_prompt = "Develop a trading bot for the stock market"

    model = ModelFactory.create(
        model_platform=ModelPlatformType.GEMINI,
        model_type=model_type,
    )

    # Update agent_kwargs to use the created models
    agent_kwargs = {
        "assistant": {"model": model},
        "user": {"model": model},
        "task-specify": {"model": model},
    }

    role_play_session = RolePlaying(
        assistant_role_name="Python Programmer",
        assistant_agent_kwargs=agent_kwargs["assistant"],
        user_role_name="Stock Trader",
        user_agent_kwargs=agent_kwargs["user"],
        task_prompt=task_prompt,
        with_task_specify=True,
        task_specify_agent_kwargs=agent_kwargs["task-specify"],
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    chat_turn_limit, n = 50, 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
        )
        print_text_animated(
            Fore.GREEN + "AI Assistant:\n\n"
            f"{assistant_response.msg.content}\n"
        )

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    main(model_type=ModelType.GEMINI_1_5_FLASH)



--------------------------------------------------------------------------------
# File: models\role_playing_with_groq.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from colorama import Fore

from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated


def main(model_type=None) -> None:
    task_prompt = "Develop a trading bot for the stock market"

    agent_kwargs = {
        role: ModelFactory.create(
            model_platform=ModelPlatformType.GROQ,
            model_type=model_type,
        )
        for role in ["assistant", "user", "task-specify"]
    }

    role_play_session = RolePlaying(
        assistant_role_name="Python Programmer",
        assistant_agent_kwargs={'model': agent_kwargs["assistant"]},
        user_role_name="Stock Trader",
        user_agent_kwargs={'model': agent_kwargs["assistant"]},
        task_prompt=task_prompt,
        with_task_specify=True,
        task_specify_agent_kwargs={'model': agent_kwargs["task-specify"]},
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    chat_turn_limit, n = 50, 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n",
            delay=0.005,
        )
        print_text_animated(
            Fore.GREEN + "AI Assistant:\n\n"
            f"{assistant_response.msg.content}\n",
            delay=0.005,
        )

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    main(model_type=ModelType.GROQ_LLAMA_3_1_8B)



--------------------------------------------------------------------------------
# File: models\role_playing_with_mistral.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from typing import List

from colorama import Fore

from camel.agents.chat_agent import ToolCallingRecord
from camel.configs import MistralConfig
from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.toolkits import (
    MathToolkit,
    SearchToolkit,
)
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated


def main(
    model_platform=ModelPlatformType.MISTRAL,
    model_type=ModelType.MISTRAL_LARGE,
    chat_turn_limit=10,
) -> None:
    task_prompt = (
        "Assume now is 2024 in the Gregorian calendar, "
        "estimate the current age of University of Oxford "
        "and then add 10 more years to this age."
    )

    user_model_config = MistralConfig(temperature=0.2)

    tools_list = [
        *MathToolkit().get_tools(),
        *SearchToolkit().get_tools(),
    ]
    assistant_model_config = MistralConfig(
        temperature=0.2,
    )

    role_play_session = RolePlaying(
        assistant_role_name="Searcher",
        user_role_name="Professor",
        assistant_agent_kwargs=dict(
            model=ModelFactory.create(
                model_platform=model_platform,
                model_type=model_type,
                model_config_dict=assistant_model_config.as_dict(),
            ),
            tools=tools_list,
        ),
        user_agent_kwargs=dict(
            model=ModelFactory.create(
                model_platform=model_platform,
                model_type=model_type,
                model_config_dict=user_model_config.as_dict(),
            ),
        ),
        task_prompt=task_prompt,
        with_task_specify=False,
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    n = 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        # Print output from the user
        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
        )

        # Print output from the assistant, including any function
        # execution information
        print_text_animated(Fore.GREEN + "AI Assistant:")
        tool_calls: List[ToolCallingRecord] = [
            ToolCallingRecord(**call.as_dict())
            for call in assistant_response.info['tool_calls']
        ]
        for func_record in tool_calls:
            print_text_animated(f"{func_record}")
        print_text_animated(f"{assistant_response.msg.content}\n")

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: models\role_playing_with_ollama.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from typing import List

from colorama import Fore

from camel.agents.chat_agent import ToolCallingRecord
from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.types import ModelPlatformType
from camel.utils import print_text_animated


def main(
    model_platform=ModelPlatformType.OLLAMA,
    model_type="llama3.2",
    chat_turn_limit=10,
) -> None:
    task_prompt = "Develop a trading bot for the stock market."

    role_play_session = RolePlaying(
        assistant_role_name="Python Programmer",
        user_role_name="Stock Trader",
        assistant_agent_kwargs=dict(
            model=ModelFactory.create(
                model_platform=model_platform,
                model_type=model_type,
                model_config_dict={"temperature": 0.4, "max_tokens": 4096},
            ),
        ),
        user_agent_kwargs=dict(
            model=ModelFactory.create(
                model_platform=model_platform,
                model_type=model_type,
                model_config_dict={"temperature": 0.4, "max_tokens": 4096},
            ),
        ),
        task_prompt=task_prompt,
        with_task_specify=False,
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    n = 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        # Print output from the user
        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
        )

        # Print output from the assistant, including any function
        # execution information
        print_text_animated(Fore.GREEN + "AI Assistant:")
        tool_calls: List[ToolCallingRecord] = [
            ToolCallingRecord(**call.as_dict())
            for call in assistant_response.info['tool_calls']
        ]
        for func_record in tool_calls:
            print_text_animated(f"{func_record}")
        print_text_animated(f"{assistant_response.msg.content}\n")

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: models\role_playing_with_sambanova.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from typing import List

import agentops
from colorama import Fore

from camel.agents.chat_agent import ToolCallingRecord
from camel.configs import SambaCloudAPIConfig
from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.types import ModelPlatformType
from camel.utils import print_text_animated

# Initialize agentops
agentops.init(default_tags=["SambaNova_with_Agentops"])
from camel.toolkits import (  # noqa: E402
    MathToolkit,
    SearchToolkit,
)


def main(
    model_platform=ModelPlatformType.SAMBA,
    model_type="Meta-Llama-3.1-70B-Instruct",
    chat_turn_limit=10,
) -> None:
    task_prompt = (
        "Assume now is 2024 in the Gregorian calendar, "
        "estimate the current age of University of Oxford "
        "and then add 10 more years to this age."
    )

    user_model_config = SambaCloudAPIConfig(temperature=0.0, max_tokens=1800)

    tools_list = [
        *MathToolkit().get_tools(),
        *SearchToolkit().get_tools(),
    ]
    assistant_model_config = SambaCloudAPIConfig(
        temperature=0.0, max_tokens=2200
    )

    role_play_session = RolePlaying(
        assistant_role_name="Searcher",
        user_role_name="Professor",
        assistant_agent_kwargs=dict(
            model=ModelFactory.create(
                model_platform=model_platform,
                model_type=model_type,
                url="https://api.sambanova.ai/v1",
                model_config_dict=assistant_model_config.as_dict(),
            ),
            tools=tools_list,
        ),
        user_agent_kwargs=dict(
            model=ModelFactory.create(
                model_platform=model_platform,
                model_type=model_type,
                url="https://api.sambanova.ai/v1",
                model_config_dict=user_model_config.as_dict(),
            ),
        ),
        task_prompt=task_prompt,
        with_task_specify=False,
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    n = 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        # Print output from the user
        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
        )

        # Print output from the assistant, including any function
        # execution information
        print_text_animated(Fore.GREEN + "AI Assistant:")
        tool_calls: List[ToolCallingRecord] = [
            ToolCallingRecord(**call.as_dict())
            for call in assistant_response.info['tool_calls']
        ]
        for func_record in tool_calls:
            print_text_animated(f"{func_record}")
        print_text_animated(f"{assistant_response.msg.content}\n")

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg

    # End agentops session
    agentops.end_session("Success")


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: models\role_playing_with_volcano.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from typing import Dict, List, Optional, Tuple

from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.types import ModelPlatformType
from camel.utils import print_text_animated

"""
Please set the below environment variable before running this example:
export VOLCANO_API_KEY="your_volcano_api_key"

This example demonstrates how to use Volcano Engine API with DeepSeek models
for role-playing in CAMEL.
"""


def main(
    assistant_role_name: str = "Python Programmer",
    user_role_name: str = "Stock Market Trader",
    task_prompt: str = (
        "Develop a Python script that analyzes historical stock data "
        "and identifies potential buying opportunities based on "
        "technical indicators."
    ),
    with_task_specify: bool = True,
    model_config_dict: Optional[Dict] = None,
) -> Tuple[List[Dict], List[Dict]]:
    r"""Run a role-playing session with Volcano Engine API.

    Args:
        assistant_role_name: The role name of the assistant.
        user_role_name: The role name of the user.
        task_prompt: The task prompt.
        with_task_specify: Whether to specify the task.
        model_config_dict: The model configuration dictionary.

    Returns:
        A tuple of assistant and user message lists.
    """
    if model_config_dict is None:
        model_config_dict = {
            "temperature": 0.2,
            "max_tokens": 1024,
        }

    # Create models for assistant and user
    assistant_model = ModelFactory.create(
        model_platform=ModelPlatformType.VOLCANO,
        model_type="deepseek-r1-250120",
        model_config_dict=model_config_dict,
    )

    user_model = ModelFactory.create(
        model_platform=ModelPlatformType.VOLCANO,
        model_type="deepseek-r1-250120",
        model_config_dict=model_config_dict,
    )

    # Create a role-playing session
    role_playing = RolePlaying(
        assistant_role_name=assistant_role_name,
        user_role_name=user_role_name,
        assistant_agent_kwargs={"model": assistant_model},
        user_agent_kwargs={"model": user_model},
        task_prompt=task_prompt,
        with_task_specify=with_task_specify,
    )

    # Start the role-playing session
    print(
        f"Running role-playing with Volcano Engine API (DeepSeek-R1)...\n"
        f"Assistant Role: {assistant_role_name}\n"
        f"User Role: {user_role_name}\n"
        f"Task: {task_prompt}\n"
    )

    # Start the chat
    chat_history = []
    n = 0
    input_msg = role_playing.init_chat()  # Initialize the chat
    while n < 10:
        n += 1
        assistant_response, user_response = role_playing.step(
            input_msg
        )  # Provide input_msg
        if assistant_response is None or user_response is None:
            break

        chat_history.append(
            {
                "role": "assistant",
                "content": assistant_response.msg.content,
            }
        )
        chat_history.append(
            {"role": "user", "content": user_response.msg.content}
        )

        print_text_animated(f"Assistant ({assistant_role_name}):\n")
        print_text_animated(f"{assistant_response.msg.content}\n")
        print_text_animated(f"User ({user_role_name}):\n")
        print_text_animated(f"{user_response.msg.content}\n")

        if "<CAMEL_TASK_DONE>" in user_response.msg.content:
            break

        input_msg = (
            assistant_response.msg
        )  # Update input_msg for the next step

    # return role_playing.assistant_agent.chat_history, role_playing.
    # user_agent.chat_history


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: models\samba_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.configs import (
    SambaCloudAPIConfig,
    SambaVerseAPIConfig,
)
from camel.models import ModelFactory
from camel.types import ModelPlatformType

# Define system message
sys_msg = "You are a helpful assistant."

# Define user message
user_msg = """Say hi to CAMEL AI, one open-source community dedicated to the 
    study of autonomous and communicative agents."""


# Use Samba Cloud model
samba_cloud_api_model = ModelFactory.create(
    model_platform=ModelPlatformType.SAMBA,
    model_type="Meta-Llama-3.1-405B-Instruct",
    model_config_dict=SambaCloudAPIConfig(max_tokens=800).as_dict(),
    api_key="Your SambaNova Cloud API Key",
    url="https://api.sambanova.ai/v1",
)

# Set agent
camel_agent_samba_cloud_api = ChatAgent(
    system_message=sys_msg, model=samba_cloud_api_model
)

# Get response information
response = camel_agent_samba_cloud_api.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
Hello to the CAMEL AI community.  It's great to see open-source communities 
like yours working on autonomous and communicative agents, as this field has 
the potential to revolutionize many areas of our lives, from customer service 
to healthcare and beyond.

What specific projects or initiatives is the CAMEL AI community currently 
working on? Are there any exciting developments or breakthroughs that you'd 
like to share? I'm all ears (or rather, all text) and happy to learn more 
about your work!
===============================================================================
'''

# Use Samba Verse model
sambaverse_api_model = ModelFactory.create(
    model_platform=ModelPlatformType.SAMBA,
    model_type="Mistral/Mistral-7B-Instruct-v0.2",
    model_config_dict=SambaVerseAPIConfig(max_tokens=800).as_dict(),
    api_key="Your SambaVerse API Key",
    url="https://sambaverse.sambanova.ai/api/predict",
)

# Set agent
camel_agent_sambaverse_api = ChatAgent(
    system_message=sys_msg, model=sambaverse_api_model
)

# Get response information
response = camel_agent_sambaverse_api.step(user_msg)
print(response.msgs[0].content)

'''
===============================================================================
Hi CAMEL AI community! I'm here to help answer any questions you may have
related to autonomous and communicative agents. Let me know how I can be 
of assistance.
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\sglang_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========


from dotenv import load_dotenv

from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.types import ModelPlatformType

r"""Before using sglang to run LLM model offline,
you need to install flashinfer.
Consider your machine's configuration and 
install flashinfer in a appropriate version.
For more details, please refer to:
https://sgl-project.github.io/start/install.html
https://docs.flashinfer.ai/installation.html

Please load HF_token in your environment variable.
export HF_TOKEN=""
When using the OpenAI interface to run SGLang model server, 
the base model may fail to recognize  huggingface default
chat template, switching to the Instruct model resolves the issue.
"""
load_dotenv()
sglang_model = ModelFactory.create(
    model_platform=ModelPlatformType.SGLANG,
    model_type="meta-llama/Llama-3.2-1B-Instruct",
    model_config_dict={"temperature": 0.0},
    api_key="sglang",
)
assistant_sys_msg = "You are a helpful assistant."

agent = ChatAgent(assistant_sys_msg, model=sglang_model, token_limit=4096)

user_msg = "Say hi to CAMEL AI"

assistant_response = agent.step(user_msg)
print(assistant_response.msg.content)

"""
===============================================================================
Hello CAMEL AI. How can I assist you today?
===============================================================================
"""

weather_tool = [
    {
        "type": "function",
        "function": {
            "name": "get_current_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "The city to find the weather for,\n"
                        "e.g. 'San Francisco'",
                    },
                    "state": {
                        "type": "string",
                        "description": "The two-letter abbreviation for,\n"
                        "the state (e.g., 'CA'), e.g. CA for California",
                    },
                    "unit": {
                        "type": "string",
                        "description": "Temperature unit (celsius/fahrenheit)",
                        "enum": ["celsius", "fahrenheit"],
                    },
                },
                "required": ["city", "state", "unit"],
            },
        },
    }
]


r"""Note that api_key defines the parser used to interpret responses.
Currently supported parsers include:
llama3: Llama 3.1 / 3.2 (e.g. meta-llama/Llama-3.1-8B-Instruct,
        meta-llama/Llama-3.2-1B-Instruct).
mistral: Mistral (e.g. mistralai/Mistral-7B-Instruct-v0.3,
         mistralai/Mistral-Nemo-Instruct-2407, 
         mistralai/ Mistral-Nemo-Instruct-2407, mistralai/Mistral-7B-v0.3).
qwen25: Qwen 2.5 (e.g. Qwen/Qwen2.5-1.5B-Instruct, Qwen/Qwen2.5-7B-Instruct).
"""
sglang_model_with_tool = ModelFactory.create(
    model_platform=ModelPlatformType.SGLANG,
    model_type="meta-llama/Llama-3.2-1B-Instruct",
    model_config_dict={"temperature": 0.0, "tools": weather_tool},
    api_key="llama3",
)

assistant_sys_msg = (
    "You are a helpful assistant.\n"
    "Use the get_current_weather tool when asked about weather."
)
agent_with_tool = ChatAgent(
    assistant_sys_msg,
    model=sglang_model_with_tool,
    token_limit=4096,
    external_tools=weather_tool,
)
user_msg = "What's the weather in Boston today?"

assistant_response = agent_with_tool.step(user_msg)
external_tool_call = assistant_response.info.get('external_tool_call_request')
if external_tool_call:
    print(f"Detected external tool call: {external_tool_call.tool_name}")
    print(f"Arguments: {external_tool_call.args}")
    print(f"Tool Call ID: {external_tool_call.tool_call_id}")
else:
    print("No external tool call detected")

"""
===============================================================================
Detected external tool call: get_current_weather
Arguments: {'city': 'Boston', 'state': 'MA', 'unit': 'celsius'}
Tool Call ID: 0
===============================================================================
"""



--------------------------------------------------------------------------------
# File: models\siliconflow_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.configs import SiliconFlowConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType

model = ModelFactory.create(
    model_platform=ModelPlatformType.SILICONFLOW,
    model_type="deepseek-ai/DeepSeek-R1",
    model_config_dict=SiliconFlowConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community
    dedicated to the study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)

'''
===============================================================================
Hello CAMEL AI community! 👋 Your dedication to advancing the study of 
autonomous and communicative agents through open-source collaboration is truly 
inspiring. The work you're doing to push the boundaries of AI interaction and 
cooperative systems will undoubtedly shape the future of intelligent 
technologies. Keep innovating, exploring, and fostering that spirit of shared 
learning—the world is excited to see what you create next! 🚀
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\togetherai_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.configs import TogetherAIConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType

model = ModelFactory.create(
    model_platform=ModelPlatformType.TOGETHER,
    model_type="meta-llama/Llama-3-8b-chat-hf",
    model_config_dict=TogetherAIConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model, token_limit=500)

user_msg = """Say hi to CAMEL AI, one open-source community dedicated to the 
    study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
Hello CAMEL AI community!

I'm thrilled to be here and assist you with any questions or topics related to 
autonomous and communicative agents. As an open-source community, I'm excited 
to see the innovative projects and research being developed by your members.

What's on your mind? Do you have a specific question, project, or topic you'd 
like to discuss? I'm here to help and provide any assistance I can. Let's get 
started!
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\vllm_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.types import ModelPlatformType

vllm_model = ModelFactory.create(
    model_platform=ModelPlatformType.VLLM,
    model_type="microsoft/Phi-3-mini-4k-instruct",
    model_config_dict={"temperature": 0.0},
)

assistant_sys_msg = "You are a helpful assistant."

agent = ChatAgent(assistant_sys_msg, model=vllm_model, token_limit=4096)

user_msg = "Say hi to CAMEL AI"

assistant_response = agent.step(user_msg)
print(assistant_response.msg.content)

"""
===============================================================================
vllm server started on http://localhost:8000/v1 for microsoft/
Phi-3-mini-4k-instruct model

Hello! I'm Phi, an AI developed by Microsoft. How can I help you today?
===============================================================================
"""



--------------------------------------------------------------------------------
# File: models\volcano_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.types import ModelPlatformType

"""
Please set the below environment variable before running this example:
export VOLCANO_API_KEY="your_volcano_api_key"

Volcano Engine API supports various models including DeepSeek models.
This example uses the DeepSeek-R1 model.
"""

# Create a model using ModelFactory
model = ModelFactory.create(
    model_platform=ModelPlatformType.VOLCANO,
    model_type="deepseek-r1-250120",  # DeepSeek-R1 model
    model_config_dict={
        "temperature": 0.2,
        "max_tokens": 1024,
    },
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """How many r in strawberry."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)



--------------------------------------------------------------------------------
# File: models\yi_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import YiConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.YI,
    model_type=ModelType.YI_LIGHTNING,
    model_config_dict=YiConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community 
    dedicated to the study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
Hello CAMEL AI community! 👋 It's great to connect with an open-source group 
dedicated to the fascinating fields of autonomous and communicative agents. If 
there's anything you need assistance with or any interesting projects you're 
working on, feel free to share. I'm here to help however I can! 😊
===============================================================================
'''



--------------------------------------------------------------------------------
# File: models\zhipuai_model_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import ZhipuAIConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.ZHIPU,
    model_type=ModelType.GLM_4,
    model_config_dict=ZhipuAIConfig(temperature=0.2).as_dict(),
)

# Define system message
sys_msg = "You are a helpful assistant."

# Set agent
camel_agent = ChatAgent(system_message=sys_msg, model=model)

user_msg = """Say hi to CAMEL AI, one open-source community 
    dedicated to the study of autonomous and communicative agents."""

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
'''
===============================================================================
Hello to CAMEL AI and its community! As a helpful assistant, I'm here to 
provide assistance, answer questions, and support the study of autonomous and 
communicative agents to the best of my abilities. If you have any specific 
questions or need guidance on a particular topic, feel free to ask!
===============================================================================
'''



--------------------------------------------------------------------------------
# File: observability\agentops_track_roleplaying_with_function.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from typing import List

import agentops
from colorama import Fore

from camel.agents.chat_agent import ToolCallingRecord
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated

# Initialize agentops
agentops.init(tags=["CAMEL X AgentOps"])

# Import toolkits after init of agentops so that the tool usage would be
# tracked
from camel.toolkits import (  # noqa: E402
    MathToolkit,
    SearchToolkit,
)

# Set up role playing session
model_platform = ModelPlatformType.DEFAULT
model_type = ModelType.DEFAULT
chat_turn_limit = 10
task_prompt = (
    "Assume now is 2024 in the Gregorian calendar, "
    "estimate the current age of University of Oxford "
    "and then add 10 more years to this age, "
    "and get the current weather of the city where "
    "the University is located."
)

user_model_config = ChatGPTConfig(temperature=0.0)

tools_list = [
    *MathToolkit().get_tools(),
    *SearchToolkit().get_tools(),
]
assistant_model_config = ChatGPTConfig(
    temperature=0.0,
)

role_play_session = RolePlaying(
    assistant_role_name="Searcher",
    user_role_name="Professor",
    assistant_agent_kwargs=dict(
        model=ModelFactory.create(
            model_platform=model_platform,
            model_type=model_type,
            model_config_dict=assistant_model_config.as_dict(),
        ),
        tools=tools_list,
    ),
    user_agent_kwargs=dict(
        model=ModelFactory.create(
            model_platform=model_platform,
            model_type=model_type,
            model_config_dict=user_model_config.as_dict(),
        ),
    ),
    task_prompt=task_prompt,
    with_task_specify=False,
)

print(
    Fore.GREEN
    + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
)
print(Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n")

print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
print(
    Fore.CYAN
    + "Specified task prompt:"
    + f"\n{role_play_session.specified_task_prompt}\n"
)
print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

n = 0
input_msg = role_play_session.init_chat()
while n < chat_turn_limit:
    n += 1
    assistant_response, user_response = role_play_session.step(input_msg)

    if assistant_response.terminated:
        print(
            Fore.GREEN
            + (
                "AI Assistant terminated. Reason: "
                f"{assistant_response.info['termination_reasons']}."
            )
        )
        break
    if user_response.terminated:
        print(
            Fore.GREEN
            + (
                "AI User terminated. "
                f"Reason: {user_response.info['termination_reasons']}."
            )
        )
        break

    # Print output from the user
    print_text_animated(
        Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
    )

    # Print output from the assistant, including any function
    # execution information
    print_text_animated(Fore.GREEN + "AI Assistant:")
    tool_calls: List[ToolCallingRecord] = [
        ToolCallingRecord(**call.as_dict())
        for call in assistant_response.info['tool_calls']
    ]
    for func_record in tool_calls:
        print_text_animated(f"{func_record}")
    print_text_animated(f"{assistant_response.msg.content}\n")

    if "CAMEL_TASK_DONE" in user_response.msg.content:
        break

    input_msg = assistant_response.msg

# End agentops session
agentops.end_session("Success")



--------------------------------------------------------------------------------
# File: personas\personas_generation.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.personas.persona_hub import PersonaHub

persona_group = PersonaHub()

# Use the text_to_persona method
example_text = """Clinical Guideline: Administration of Injections in 
Pediatric Patients Purpose: To provide standardized care for pediatric 
patients requiring injections, ensuring safety, ..."""

inferred_persona = persona_group.text_to_persona(example_text, action="read")
print(
    f"Inferred Persona:\n{inferred_persona.name}"
    f"\n{inferred_persona.description}\n"
)

# Use the persona_to_persona method
related_personas = persona_group.persona_to_persona(persona=inferred_persona)
print("Related Personas:\n")
for persona_id, persona in related_personas.items():
    print(f"ID: {persona_id}")
    print(f"Name: {persona.name}")
    print(f"Description: {persona.description}")
    print()
'''
===============================================================================
Inferred Persona:
Pediatric Nurse
A healthcare professional specializing in the care of children, with expertise in administering medications and following clinical guidelines for pediatric patients.

Related Personas:

ID: 123e4567-e89b-12d3-a456-426614174000
Name: Pediatrician
Description: A medical doctor who specializes in the care of infants, children, and adolescents. They work closely with pediatric nurses to ensure proper treatment and medication administration for young patients.

ID: 123e4567-e89b-12d3-a456-426614174001
Name: Child Life Specialist
Description: A professional who helps children and families cope with the challenges of hospitalization, illness, and disability. They often collaborate with medical staff to make medical procedures less stressful for pediatric patients.

ID: 123e4567-e89b-12d3-a456-426614174002
Name: Pediatric Pharmacist
Description: A pharmacist who specializes in medications for children, ensuring proper dosing and formulations. They work with the medical team to optimize medication regimens for pediatric patients.

ID: 123e4567-e89b-12d3-a456-426614174003
Name: Parent or Guardian
Description: The primary caregiver of a pediatric patient, who needs to understand and consent to medical procedures, including injections. They often have concerns and questions about their child's treatment.

ID: 123e4567-e89b-12d3-a456-426614174004
Name: Pediatric Hospital Administrator
Description: A healthcare manager responsible for overseeing pediatric departments or hospitals. They ensure that clinical guidelines are implemented and followed to maintain high standards of care for young patients.
===============================================================================
'''  # noqa: E501



--------------------------------------------------------------------------------
# File: rag\single_agent_with_hybrid_rag.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.retrievers import HybridRetriever


def single_agent(query: str) -> str:
    # Set agent role
    assistant_sys_msg = """You are a helpful assistant to answer question,
         I will give you the Original Query and Retrieved Context,
        answer the Original Query based on the Retrieved Context,
        if you can't answer the question just say I don't know."""

    hybrid_retriever = HybridRetriever()
    hybrid_retriever.process(
        content_input_path="https://en.wikipedia.org/wiki/King_Abdullah_University_of_Science_and_Technology"
    )

    retrieved_info = hybrid_retriever.query(
        query=query,
        top_k=5,
        vector_retriever_top_k=10,
        bm25_retriever_top_k=10,
    )

    # Pass the retrieved information to agent
    user_msg = str(retrieved_info)
    agent = ChatAgent(assistant_sys_msg)

    # Get response
    assistant_response = agent.step(user_msg)
    return assistant_response.msg.content


print(single_agent("What is it like to be a visiting student at KAUST?"))
'''
===============================================================================
Being a visiting student at KAUST involves participating in the Visiting
Student Program (VS), which is designed for 3rd or 4th year undergraduate
or master's students. This program allows students to work directly with KAUST
faculty members for a duration that can range from a few days to several
months. Accepted students typically receive a monthly stipend, and their
accommodation, health insurance, and travel costs are covered. This support
makes the experience financially manageable and allows students to focus on
their research and learning during their time at KAUST.
===============================================================================
'''



--------------------------------------------------------------------------------
# File: role_description\role_generation.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from colorama import Fore

from camel.agents import RoleAssignmentAgent


def main(model=None, num_roles=3) -> None:
    task_prompt = "Develop a trading bot for the stock market."

    role_description_agent = RoleAssignmentAgent(model=model)

    role_description_dict = role_description_agent.run(
        task_prompt=task_prompt, num_roles=num_roles
    )

    if len(role_description_dict) != num_roles:
        raise ValueError(
            f"Length of role_names ({len(role_description_dict)}) "
            f"does not equal to num_roles ({num_roles})."
        )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(Fore.GREEN + f"List of {num_roles} roles with description:")
    for role_name in role_description_dict.keys():
        print(
            Fore.BLUE + f"{role_name}:\n"
            f"{role_description_dict[role_name]}\n"
        )


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: role_description\role_playing_with_role_description.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from colorama import Fore

from camel.agents import RoleAssignmentAgent
from camel.societies import RolePlaying
from camel.types import TaskType
from camel.utils import print_text_animated

AI_ASSISTANT_ROLE_INDEX = 0
AI_USER_ROLE_INDEX = 1


def main(
    model_for_role_generation=None, model=None, chat_turn_limit=50
) -> None:
    task_prompt = "Develop a trading bot for the stock market."

    role_description_agent = RoleAssignmentAgent(
        model=model_for_role_generation,
    )

    role_description_dict = role_description_agent.run(
        task_prompt=task_prompt, num_roles=2
    )

    ai_assistant_role = list(role_description_dict.keys())[
        AI_ASSISTANT_ROLE_INDEX
    ]
    ai_user_role = list(role_description_dict.keys())[AI_USER_ROLE_INDEX]
    ai_assistant_description = role_description_dict[ai_assistant_role]
    ai_user_description = role_description_dict[ai_user_role]

    sys_msg_meta_dicts = [
        dict(
            assistant_role=ai_assistant_role,
            user_role=ai_user_role,
            assistant_description=ai_assistant_description,
            user_description=ai_user_description,
        )
        for _ in range(2)
    ]

    role_play_session = RolePlaying(
        assistant_role_name=ai_assistant_role,
        user_role_name=ai_user_role,
        task_prompt=task_prompt,
        model=model,
        task_type=TaskType.ROLE_DESCRIPTION,  # Score for role description
        with_task_specify=True,
        task_specify_agent_kwargs=dict(model=model),
        extend_sys_msg_meta_dicts=sys_msg_meta_dicts,
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )
    print(
        Fore.GREEN + f"Role description of AI Assistant:\n"
        f"{role_play_session.assistant_sys_msg.role_name}\n"
        f"{role_description_dict[ai_assistant_role]}\n"
    )
    print(
        Fore.BLUE + f"Role description of AI User:\n"
        f"{role_play_session.user_sys_msg.role_name}\n"
        f"{role_description_dict[ai_user_role]}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    n = 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. "
                    f"Reason: {assistant_response.info['termination_reasons']}"
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        print_text_animated(
            Fore.BLUE
            + f"AI User: {ai_user_role}\n\n{user_response.msg.content}\n"
        )
        print_text_animated(
            Fore.GREEN
            + f"AI Assistant:{ai_assistant_role}\n\n"
            + f"{assistant_response.msg.content}\n"
        )

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: runtime\code_execution_with_docker_runtime.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from colorama import Fore

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.runtime import DockerRuntime
from camel.toolkits.code_execution import CodeExecutionToolkit
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated

# tools
toolkit = CodeExecutionToolkit(verbose=True)

# change to your own docker image
runtime = DockerRuntime("xukunliu/camel").add(
    toolkit.get_tools(),
    "camel.toolkits.CodeExecutionToolkit",
    dict(verbose=True),
    redirect_stdout=True,
)

tools = runtime.get_tools()

# set up LLM model
assistant_model_config = ChatGPTConfig(
    temperature=0.0,
)

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.GPT_4O,
    model_config_dict=assistant_model_config.as_dict(),
)


# set up agent

assistant_sys_msg = (
    "You are a personal math tutor and programmer. "
    "When asked a math question, "
    "write and run Python code to answer the question."
)

agent = ChatAgent(
    assistant_sys_msg,
    model,
    tools=tools,
)
agent.reset()


# set up agent

with runtime as r:
    r.wait()
    prompt = (
        "Weng earns $12 an hour for babysitting. "
        "Yesterday, she just did 51 minutes of babysitting. How much did she earn?"
    )
    print(Fore.YELLOW + f"user prompt:\n{prompt}\n")

    response = agent.step(prompt)
    for msg in response.msgs:
        print_text_animated(Fore.GREEN + f"Agent response:\n{msg.content}\n")


# ruff: noqa: E501
"""
===============================================================================
user prompt:
Weng earns $12 an hour for babysitting. Yesterday, she just did 51 minutes of babysitting. How much did she earn?

Executed the code below:
```py
hourly_rate = 12
minutes_worked = 51
hourly_earnings = hourly_rate / 60 * minutes_worked
hourly_earnings
```
> Executed Results:
10.200000000000001
Agent response:
Weng earned $10.20 for babysitting for 51 minutes at a rate of $12 per hour.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: runtime\code_execution_with_llm_guard_runtime.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from colorama import Fore

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.runtime import LLMGuardRuntime
from camel.toolkits.code_execution import CodeExecutionToolkit
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated

# tools
toolkit = CodeExecutionToolkit(verbose=False)


runtime = LLMGuardRuntime(verbose=True).add(
    *CodeExecutionToolkit().get_tools()
)

tools = runtime.get_tools()

print("Tools:")
for tool in tools:
    print(tool.get_function_name())

# set up LLM model
assistant_model_config = ChatGPTConfig(
    temperature=0.0,
)

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=assistant_model_config.as_dict(),
)


# set up agent

assistant_sys_msg = (
    "You are a personal math tutor and programmer. "
    "When asked a math question, "
    "write and run Python code to answer the question."
)

agent = ChatAgent(
    assistant_sys_msg,
    model,
    tools=tools,
)
agent.reset()


# set up agent

prompt = (
    "Weng earns $12 an hour for babysitting. "
    "Yesterday, she just did 51 minutes of babysitting. How much did she earn?"
)
print(Fore.YELLOW + f"user prompt:\n{prompt}\n")

response = agent.step(prompt)
for msg in response.msgs:
    print_text_animated(Fore.GREEN + f"Agent response:\n{msg.content}\n")


# ruff: noqa: E501
"""
Tools:
ignore_risk
execute_code
user prompt:
Weng earns $12 an hour for babysitting. Yesterday, she just did 51 minutes of babysitting. How much did she earn?

Risk assessment not passed for function execute_code.Score: 3 > Threshold: 2
Reason: The function 'execute_code' is designed to execute arbitrary code snippets, which inherently poses a significant risk. Although the provided code snippet appears harmless and simply calculates earnings based on hourly wage and minutes worked, the function itself allows for potentially dangerous operations if different code were to be executed. Therefore, the risk level is assessed as 3.
Ignoring risk for function execute_code: The code is a simple arithmetic calculation that poses no risk.
Agent response:
Weng earned $10.20 for 51 minutes of babysitting.
"""



--------------------------------------------------------------------------------
# File: runtime\docker_runtime.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.runtime import DockerRuntime
from camel.toolkits import CodeExecutionToolkit, MathToolkit

if __name__ == "__main__":
    runtime = (
        DockerRuntime("xukunliu/camel")  # change to your own docker image
        .add(MathToolkit().get_tools(), "camel.toolkits.MathToolkit")
        .add(
            CodeExecutionToolkit().get_tools(),
            "camel.toolkits.CodeExecutionToolkit",
            dict(verbose=True),
        )
    )

    with (
        runtime as r
    ):  # using with statement to automatically close the runtime
        print("Waiting for runtime to be ready...")
        r.wait()
        print("Runtime is ready.")

        tools = r.get_tools()

        add, sub, mul = tools[:3]
        code_exec = tools[3]

        # without kwargs
        print(f"Add 1 + 2: {add.func(1, 2)}")
        print(f"Subtract 5 - 3: {sub.func(5, 3)}")
        print(f"Multiply 2 * 3: {mul.func(2, 3)}")
        print(f"Execute code: {code_exec.func('1 + 2')}")

        # with kwargs
        print(f"Add 1 + 2: {add.func(a=1, b=2)}")
        print(f"Subtract 5 - 3: {sub.func(a=5, b=3)}")
        print(f"Multiply 2 * 3: {mul.func(a=2, b=3)}")
        print(f"Execute code: {code_exec.func(code='1 + 2')}")

        print("Documents: ", r.docs)
        # you can open this url in browser to see the API Endpoints
        # before the runtime is stopped.

    # you can also use the runtime without the with statement
    # runtime.build()
    # runtime.stop()

"""
Add 1 + 2: 3
Subtract 5 - 3: 2
Multiply 2 * 3: 6
Execute code: Executed the code below:
```py
1 + 2
```
> Executed Results:
3
Documents:  http://localhost:8000/docs
"""



--------------------------------------------------------------------------------
# File: runtime\docker_runtime_with_tasks.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from colorama import Fore

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.runtime import DockerRuntime, TaskConfig
from camel.toolkits.code_execution import CodeExecutionToolkit
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated

# tools
toolkit = CodeExecutionToolkit(verbose=True)

runtime = (
    DockerRuntime("xukunliu/camel")  # change to your own docker image
    .add(
        toolkit.get_tools(),
        "camel.toolkits.CodeExecutionToolkit",
        {"unsafe_mode": True, "import_white_list": ["os", "sys"]},
        True,
    )
    .add_task(
        TaskConfig(
            cmd="mkdir /home/test",
        )
    )
)

tools = runtime.get_tools()

# set up LLM model
assistant_model_config = ChatGPTConfig(
    temperature=0.0,
)

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=assistant_model_config.as_dict(),
)


# set up agent
assistant_sys_msg = (
    "You are a personal assistant and programmer. "
    "When asked a question, "
    "write and run Python code to answer the question."
    "Your code will be executed using eval()."
)

agent = ChatAgent(
    assistant_sys_msg,
    model,
    tools=tools,
)
agent.reset()


# set up agent

with runtime as r:
    r.wait()
    prompt = "List all directories in /home"
    print(Fore.YELLOW + f"user prompt:\n{prompt}\n")

    response = agent.step(prompt)
    for msg in response.msgs:
        print_text_animated(Fore.GREEN + f"Agent response:\n{msg.content}\n")


# TODO: unlock unsafe mode
# This example can not be run in the current version of CAMEL because
# the InternalPythonInterpreter does not support
# most of the built-in functions.



--------------------------------------------------------------------------------
# File: runtime\remote_http_runtime.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.runtime import RemoteHttpRuntime
from camel.toolkits import MathToolkit

if __name__ == "__main__":
    runtime = (
        RemoteHttpRuntime("localhost")
        .add(MathToolkit().get_tools(), "camel.toolkits.MathToolkit")
        .build()
    )
    print("Waiting for runtime to be ready...")
    runtime.wait()
    print("Runtime is ready.")
    add, sub, mul = runtime.get_tools()
    print(f"Add 1 + 2: {add.func(1, 2)}")
    print(f"Subtract 5 - 3: {sub.func(5, 3)}")
    print(f"Multiply 2 * 3: {mul.func(2, 3)}")

    print("Documents: ", runtime.docs)
    # you can open this url in browser to see the API Endpoints
    # before the runtime is stopped.
    # time.sleep(60)

    # call runtime.stop() if you want to stop the runtime manually
    # atherwise it will be stopped automatically when the program ends


"""
Waiting for runtime to be ready...
Runtime is ready.
Add 1 + 2: 3
Subtract 5 - 3: 2
Multiply 2 * 3: 6
Documents:  http://localhost:8000/docs
"""



--------------------------------------------------------------------------------
# File: schema_outputs\openai_converter_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from pydantic import BaseModel

from camel.schemas import OpenAISchemaConverter


def get_temperature(location: str, date: str, temperature: float):
    print(f"Temperature in {location} on {date} is {temperature} degrees.")


class Temperature(BaseModel):
    location: str
    date: str
    temperature: float


temperature_template = (
    '{"location": "Beijing", "date": "2023-09-01", "temperature": 30.0}'
)


model = OpenAISchemaConverter()

print(
    model.convert(
        "Today is 2023-09-01, the temperature in Beijing is 30 degrees.",
        output_schema=temperature_template,
    )
)

print(
    model.convert(
        "Today is 2023-09-01, the temperature in Beijing is 30 degrees.",
        output_schema=get_temperature,
    )
)

print(
    model.convert(
        "Today is 2023-09-01, the temperature in Beijing is 30 degrees.",
        output_schema=Temperature,
    )
)
"""
location='Beijing' date='2023-09-01' temperature=30.0
location='Beijing' date='2023-09-01' temperature=30.0
location='Beijing' date='2023-09-01' temperature=30.0
"""



--------------------------------------------------------------------------------
# File: schema_outputs\outlines_converter_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from pydantic import BaseModel

from camel.schemas import OutlinesConverter

# Define the model using OutlinesConverter
model = OutlinesConverter(
    model_type="microsoft/Phi-3-mini-4k-instruct", platform="transformers"
)

######## Regex conversion #########

time_regex_pattern = r"(0?[1-9]|1[0-2]):[0-5]\d\s?(am|pm)?"
output = model.convert_regex(
    "The the best time to visit a dentist is at ", time_regex_pattern
)

print(output)
"""
===============================================================================
6:00 pm
===============================================================================
"""


######## Pydantic conversion #########


# Using a Pydantic model
class Temperature(BaseModel):
    location: str
    date: str
    temperature: float


output = model.convert_pydantic(
    "Today is 2023-09-01, the temperature in Beijing is 30 degrees.",
    output_schema=Temperature,
)

print(type(output))
"""
===============================================================================
<class '__main__.Temperature'>
===============================================================================
"""
print(output)
"""
===============================================================================
location='Beijing' date='2023-09-01' temperature=30.0
===============================================================================
"""


######## JSON conversion #########

# 1. Using a JSON schema

schema = """
{
  "title": "User",
  "type": "object",
  "properties": {
    "name": {"type": "string"},
    "last_name": {"type": "string"},
    "id": {"type": "integer"}
  },
  "required": ["name", "last_name", "id"]
}
"""

output = model.convert_json(
    "Create a user profile with the fields name, last_name and id",
    output_schema=schema,
)
print(type(output))
"""
===============================================================================
<class 'dict'>
===============================================================================
"""
print(output)
"""
===============================================================================
{'name': 'John', 'last_name': 'Doe', 'id': 123456}
===============================================================================
"""

# 2. Using a function (Callable)


def get_temperature(location: str, date: str, temperature: float):
    print(f"Temperature in {location} on {date} is {temperature} degrees.")


output = model.convert_json(
    "Today is 2023-09-01, the temperature in Beijing is 30 degrees.",
    output_schema=get_temperature,
)

print(type(output))
"""
===============================================================================
<class 'dict'>
===============================================================================
"""
print(output)
"""
===============================================================================
{'location': 'Beijing', 'date': '2023-09-01', 'temperature': 30}
===============================================================================
"""


######## Type constraints #########

output = model.convert_type(
    "When I was 6 my sister was half my age. Now I'm 70 how old is my sister?",
    int,
)

print(output)
"""
===============================================================================
35
===============================================================================
"""


######## Multiple choices #########

output = model.convert_choice(
    "What is the capital of Spain?",
    ["Paris", "London", "Berlin", "Madrid"],
)

print(output)
"""
===============================================================================
Madrid
===============================================================================
"""


######## Grammar #########

arithmetic_grammar = """
    ?start: expression

    ?expression: term (("+" | "-") term)*

    ?term: factor (("*" | "/") factor)*

    ?factor: NUMBER
           | "-" factor
           | "(" expression ")"

    %import common.NUMBER
"""

output = model.convert_grammar(
    "Alice had 4 apples and Bob ate 2. "
    + "Write an expression for Alice's apples:",
    arithmetic_grammar,
)

print(output)
"""
===============================================================================
(8-2)
===============================================================================
"""



--------------------------------------------------------------------------------
# File: single_agent.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.prompts import PromptTemplateGenerator
from camel.types import TaskType


def main(key: str = 'generate_users', num_roles: int = 50, model=None):
    prompt_template = PromptTemplateGenerator().get_prompt_from_key(
        TaskType.AI_SOCIETY, key
    )
    prompt = prompt_template.format(num_roles=num_roles)
    print(prompt)
    agent = ChatAgent("You are a helpful assistant.", model=model)
    agent.reset()

    assistant_response = agent.step(prompt)
    print(assistant_response.msg.content)


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: storages\nebular_graph.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from unstructured.documents.elements import Element

from camel.storages.graph_storages import NebulaGraph
from camel.storages.graph_storages.graph_element import (
    GraphElement,
    Node,
    Relationship,
)

# Step 2: Initialize the NebulaGraph client
host = '127.0.0.1'
username = 'root'
password = 'nebula'
space = 'space_name'

nebula_graph = NebulaGraph(host, username, password, space)

# Ensure necessary tags (node types) exist
nebula_graph.ensure_tag_exists("CAMEL_AI")
nebula_graph.ensure_tag_exists("Agent_Framework")

# Show existing tags
query = 'SHOW TAGS;'
print(nebula_graph.query(query))

"""
==============================================================================
ResultSet(keys: ['Name'], values: ["CAMEL_AI"],["Agent_Framework"])
==============================================================================
"""

# Add triplet
nebula_graph.add_triplet(
    subj="CAMEL_AI", obj="Agent_Framework", rel="contribute_to"
)

# Check structured schema
print(nebula_graph.get_structured_schema)

"""
==============================================================================
{'node_props': {'CAMEL_AI': [], 'Agent_Framework': []}, 'rel_props': 
{'contribute_to': []}, 'relationships': ['contribute_to'], 'metadata': 
{'index': []}}
==============================================================================
"""

# Delete triplet
nebula_graph.delete_triplet(
    subj="CAMEL_AI", obj="Agent_Framework", rel="contribute_to"
)

# Create and add graph element
node_camel = Node(
    id="CAMEL_AI",
    type="Agent_Framework",
)
node_nebula = Node(
    id="Nebula",
    type="Graph_Database",
)

graph_elements = [
    GraphElement(
        nodes=[node_camel, node_nebula],
        relationships=[
            Relationship(
                subj=node_camel,
                obj=node_nebula,
                type="Supporting",
            )
        ],
        source=Element(element_id="a05b820b51c760a41415c57c1eef8f08"),
    )
]

# Add this graph element to graph db
nebula_graph.add_graph_elements(graph_elements)

# Get structured schema
print(nebula_graph.get_structured_schema)

"""
==============================================================================
{'node_props': {'Agent_Framework': [], 'CAMEL_AI': [], 'Graph_Database': [], 
'Nebula': [], 'agent_framework': []}, 'rel_props': {'Supporting': [], 
'contribute_to': []}, 'relationships': ['Supporting', 'contribute_to'], 
'metadata': {'index': []}}
==============================================================================
"""



--------------------------------------------------------------------------------
# File: storages\redis_storage.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import logging
from typing import Any, Dict, List

from camel.storages import RedisStorage


def main():
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    sid = "example_sid"
    url = "redis://localhost:6379"
    storage = RedisStorage(sid=sid, url=url)

    with storage:
        records: List[Dict[str, Any]] = [
            {"id": 1, "name": "Record1"},
            {"id": 2, "name": "Record2"},
        ]

        storage.save(records)
        logger.info("Records saved successfully.")

        loaded_records = storage.load()
        logger.info(f"Loaded records: {loaded_records}")
        """
        Loaded records: [{'id': 1, 'name': 'Record1'}, {'id': 2, 'name': 
        'Record2'}]
        """

        storage.clear()
        logger.info("Records cleared successfully.")
        """
        Records cleared successfully.
        """

        loaded_records_after_clear = storage.load()
        logger.info(
            f"Loaded records after clear: {loaded_records_after_clear}"
        )
        """
        Loaded records after clear: []
        """


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: storages\s3_storage.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from pathlib import Path

from camel.storages.object_storages import AmazonS3Storage


def get_file():
    s3_storage = AmazonS3Storage(bucket_name="camel-ai-bucket")
    print(s3_storage._get_file(Path("folder1/example.txt")))


def upload_file():
    s3_storage = AmazonS3Storage(bucket_name="camel-ai-bucket")
    s3_storage.upload_file(
        local_file_path=Path("./redis_storage.py"),
        s3_file_path=Path("folder1/redis_storage.py"),
    )


if __name__ == "__main__":
    upload_file()



--------------------------------------------------------------------------------
# File: storages\tidb_vector_storage.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.storages.vectordb_storages import (
    TiDBStorage,
    VectorDBQuery,
    VectorRecord,
)

"""
Before the DATABASE_URL, you can setup the a TiDB database cluster first:

(Option 1): TiDB Serverless

1. Go to [TiDB Cloud](https://tidbcloud.com/console/clusters) to create 
    a serverless cluster
2. Click the **Connect** button
3. Select "SQLAlchemy" > "PyMySQL" for the **Connect With** option, then 
    you can get the DATABASE_URL like:

DATABASE_URL="mysql+pymysql://<USERNAME>:<PASSWORD>@<HOST>:4000/test&ssl_verify_cert=true&ssl_verify_identity=true"

(Option 2): TiDB playground cluster on local

1. Install TiUP via command:

```
curl --proto '=https' --tlsv1.2 -sSf \
    https://tiup-mirrors.pingcap.com/install.sh | sh
```

2. Deploy a playground cluster via command: `tiup playground`
3. The DATABASE_URL should be like: "mysql+pymysql://root:@localhost:4000/test"
"""

DATABASE_URL = "mysql+pymysql://root:@localhost:4000/test"


def main():
    # Create an instance of TiDBStorage with dimension = 4
    tidb_storage = TiDBStorage(
        url_and_api_key=(DATABASE_URL, ''),
        vector_dim=4,
        collection_name="my_collection",
    )

    # Add two vector records
    tidb_storage.add(
        [
            VectorRecord(
                vector=[-0.1, 0.1, -0.1, 0.1],
                payload={'key1': 'value1'},
            ),
            VectorRecord(
                vector=[-0.1, 0.1, 0.1, 0.1],
                payload={'key2': 'value2'},
            ),
        ]
    )

    # Query similar vectors
    query_results = tidb_storage.query(
        VectorDBQuery(query_vector=[0.1, 0.2, 0.1, 0.1], top_k=1)
    )
    for result in query_results:
        print(result.record.payload, result.similarity)

    """
    Output:
    {'key2': 'value2'} 0.5669466755703252
    """

    # Clear all vectors
    tidb_storage.clear()


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: structured_response\json_format_reponse_with_tools.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from pydantic import BaseModel, Field

from camel.agents import ChatAgent
from camel.configs.openai_config import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import (
    MathToolkit,
    SearchToolkit,
)
from camel.types import ModelPlatformType, ModelType

tools_list = [
    *MathToolkit().get_tools(),
    *SearchToolkit().get_tools(),
]
assistant_model_config = ChatGPTConfig(
    temperature=0.0,
)

# Define system message
assistant_sys_msg = "You are a helpful assistant."

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=assistant_model_config.as_dict(),
)

# Set agent
camel_agent = ChatAgent(
    assistant_sys_msg,
    model=model,
    tools=tools_list,
)


# pydantic basemodel as input params format
class Schema(BaseModel):
    current_age: str = Field(
        description=" the current age of University of Oxford"
    )
    calculated_age: str = Field(description="the add more years of age")


user_msg = "Assume now is 2024 in the Gregorian calendar, "
"estimate the current age of University of Oxford "
"and then add 10 more years to this age, "

# Get response information
response = camel_agent.step(user_msg, response_format=Schema)
print(response.msgs[0].content)
"""
{'current_age': '928', 'calculated_age': '938'}
"""



--------------------------------------------------------------------------------
# File: structured_response\json_format_response.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from pydantic import BaseModel, Field

from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

# Define system message
assistant_sys_msg = "You are a helpful assistant."

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
)

# Set agent
camel_agent = ChatAgent(assistant_sys_msg, model=model)


# pydantic basemodel as input params format
class JokeResponse(BaseModel):
    joke: str = Field(description="a joke")
    funny_level: str = Field(description="Funny level, from 1 to 10")


# Get response information
response = camel_agent.step("Tell me a joke.", response_format=JokeResponse)
print(response.msgs[0].content)
"""
{'joke': "Why couldn't the bicycle find its way home? It lost its bearings!"
, 'funny_level': '8'}
"""



--------------------------------------------------------------------------------
# File: structured_response\structure_response_prompt_engineering.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from pydantic import BaseModel

from camel.agents import ChatAgent
from camel.configs import QwenConfig
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType


# Define Pydantic models
class Student(BaseModel):
    name: str
    age: str
    email: str


class StudentList(BaseModel):
    studentList: list[Student]


# Define Qwen model
qwen_model = ModelFactory.create(
    model_platform=ModelPlatformType.QWEN,
    model_type=ModelType.QWEN_TURBO,
    model_config_dict=QwenConfig().as_dict(),
)

qwen_agent = ChatAgent(
    model=qwen_model,
)

user_msg = """give me 1 student info."""

# Get response information
response0 = qwen_agent.step(user_msg, response_format=None)
print(response0.msgs[0].content)
"""
===============================================================================
Certainly! Below is an example of a student's information:

**Student Name:** Emily Johnson  
**Date of Birth:** March 12, 2005  
**Grade:** 10th Grade  
**School:** Lincoln High School  
**Address:** 456 Oak Street, Springfield, IL 62704  
**Phone Number:** (555) 123-4567  
**Email:** emily.johnson@student.lincolnhs.edu  
**Emergency Contact:** John Johnson (Father) - (555) 987-6543  

Is there anything specific you need or any changes you'd like to make?
===============================================================================
"""

# Get response information
response1 = qwen_agent.step(user_msg, response_format=Student)
print(response1.msgs[0].content)
"""
===============================================================================
{
  "name": "Emily Johnson",
  "age": "18",
  "email": "emily.johnson@student.lincolnhs.edu"
}
===============================================================================
"""
print(response1.msgs[0].parsed)
"""
===============================================================================
name='Emily Johnson' age='18' email='emily.johnson@student.lincolnhs.edu'
===============================================================================
"""
print(type(response1.msgs[0].parsed))
"""
===============================================================================
<class '__main__.Student'>
===============================================================================
"""

user_msg = """give me a list of student infos."""

# Get response information
response2 = qwen_agent.step(user_msg, response_format=StudentList)
print(response2.msgs[0].content)
"""
===============================================================================
{
  "studentList": [
    {
      "name": "Emily Johnson",
      "age": "18",
      "email": "emily.johnson@student.lincolnhs.edu"
    }
  ]
}
===============================================================================
"""

print(response2.msgs[0].parsed)
"""
===============================================================================
studentList=[Student(name='Emily Johnson', age='18', email='emily.johnson@student.lincolnhs.edu')] 
===============================================================================
"""  # noqa: E501

print(type(response2.msgs[0].parsed))
"""
===============================================================================
<class '__main__.StudentList'>
===============================================================================
"""



--------------------------------------------------------------------------------
# File: summarization\gpt_solution_extraction.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import argparse
import concurrent.futures
import itertools
import json
import os
import random
from typing import Dict, Tuple

import numpy as np

from camel.agents import ChatAgent
from camel.prompts import SolutionExtractionPromptTemplateDict
from camel.types import RoleType

parser = argparse.ArgumentParser(
    description='Arguments for conversation summarization.'
)
parser.add_argument(
    '--json_dir',
    type=str,
    help='Directory containing original json files',
    default='../camel/camel_data/ai_society',
)
parser.add_argument(
    '--solution_dir',
    type=str,
    help='Directory for solution json files',
    default='../camel/camel_data/ai_society_solution_extraction',
)
parser.add_argument(
    '--seed', type=int, help='Seed for reproducibility', default=10
)


def flatten_conversation(conversation: Dict) -> str:
    r"""Format a conversation into a string.

    Args:
        conversation (Dict): A dictionary containing
            information about the conversation.

    Returns:
        str: A string containing the specified task and
            all messages in the conversation.

    Raises:
        ValueError: If an unknown role name is encountered
            in the conversation.

    The conversation is formatted in the following format:
    Task: <specified_task>
    User (<role_1>): <message_1>
    Assistant (<role_2>): <message_2>
    ...

    Example:
        >>> conversation = {
        ...     'num_messages': 2,
        ...     'message_1': {'role_name': 'Engineer', 'content': 'Hello'},
        ...     'message_2': {'role_name': 'Programmer',
                              'content': 'Hi there!'},

        ...     'specified_task': 'Answer a greeting'
        ... }
        >>> flatten_conversation(conversation)
        'Task: Answer a greeting
            User (Engineer): Hello
            Assistant (Programmer): Hi there!'

    """

    num_messages = conversation['num_messages']
    assert num_messages >= 2
    role_1 = conversation['message_1']['role_name']
    role_2 = conversation['message_2']['role_name']
    task = conversation['specified_task']

    messages = []
    for i in range(1, num_messages + 1):
        if conversation[f'message_{i}']['role_name'] == role_1:
            message = (
                f"User ({role_1}): " + conversation[f'message_{i}']['content']
            )
        elif conversation[f'message_{i}']['role_name'] == role_2:
            message = (
                f"Assistant ({role_2}): "
                + conversation[f'message_{i}']['content']
            )
        else:
            raise ValueError(
                "Unknown role name: "
                f"{conversation[f'message_{i}']['role_name']}"
            )
        messages.append(message)

    joined_messages = '\n'.join(messages)
    formatted_data = f"Task: {task}\n{joined_messages}"

    return formatted_data


def format_combination(combination: Tuple[int, int, int]):
    assistant_role, user_role, task = combination
    assistant_role_str = str(assistant_role).zfill(3)
    user_role_str = str(user_role).zfill(3)
    task_str = str(task).zfill(3)
    return f"{assistant_role_str}_{user_role_str}_{task_str}"


def solution_extraction(
    conversation: Dict,
    flattened_conversation: str,
    file_name: str,
    args: argparse.Namespace,
) -> None:
    solution_extraction_template = SolutionExtractionPromptTemplateDict()
    assistant_sys_msg_prompt = solution_extraction_template[RoleType.ASSISTANT]

    # We use GPT4 because it has a longer context length
    agent = ChatAgent(assistant_sys_msg_prompt)
    agent.reset()

    prompt = "Here is the conversation:" + flattened_conversation

    assistant_response = agent.step(prompt)
    print(assistant_response.msg.content)

    # Create folder to write solution_extraction to
    if not os.path.exists(args.solution_dir):
        os.makedirs(args.solution_dir)

    # Append to the original JSON conversation file
    conversation['solution_extraction'] = assistant_response.msg.content

    # Save new dictionary as JSON file
    save_path = os.path.join(args.solution_dir, f'{file_name}.json')
    with open(save_path, "w") as f:
        json.dump(conversation, f, ensure_ascii=False)


def main():
    args = parser.parse_args()
    np.random.seed(args.seed)
    random.seed(args.seed)

    total_num_assistant_roles = 50
    total_num_user_roles = 50
    total_num_tasks = 1

    subsample_num_assistant_roles = 10
    subsample_num_user_roles = 10
    subsample_num_tasks = 1

    # Randomly subsample `subsample_num_assistant_roles`
    # of the total assistant roles
    subsampled_assistant_roles = random.sample(
        range(1, total_num_assistant_roles + 1), subsample_num_assistant_roles
    )

    # Randomly subsample `subsample_num_user_roles` of the total user roles
    subsampled_user_roles = random.sample(
        range(1, total_num_user_roles + 1), subsample_num_user_roles
    )

    # Randomly subsample `subsample_num_tasks` of the total tasks
    subsampled_tasks = random.sample(
        range(1, total_num_tasks + 1), subsample_num_tasks
    )

    file_names = list(
        itertools.product(
            subsampled_assistant_roles, subsampled_user_roles, subsampled_tasks
        )
    )

    # Formatting is needed to match the names of the original
    # generated JSON files xxx_xxx_xxx.json
    file_names = [
        format_combination(combination) for combination in file_names
    ]

    # Check that all files exist
    for file_name in file_names:
        json_file = os.path.join(args.json_dir, f"{file_name}.json")
        if not os.path.exists(json_file):
            raise ValueError(f"File {json_file} does not exist.")

    # Read in json files and extract solutions
    with concurrent.futures.ProcessPoolExecutor(max_workers=16) as executor:
        futures = []
        for file_name in file_names:
            json_file = os.path.join(args.json_dir, f"{file_name}.json")
            with open(json_file) as f:
                conversation = json.load(f)
            flattened_conversation = flatten_conversation(conversation)
            futures.append(
                executor.submit(
                    solution_extraction,
                    conversation,
                    flattened_conversation,
                    file_name,
                    args,
                )
            )

        for future in concurrent.futures.as_completed(futures):
            try:
                future.result()
            except Exception as e:
                print(f"Exception: {e}")


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: summarization\gpt_solver.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import concurrent.futures
import json
import os
from typing import Dict

from camel.agents import ChatAgent

# Directory containing your json files of CAMEL conversations
# This code will append a new key called "gpt_solution" to each json file
# Containing GPT solution to the specified task in the json file

# dir_files = "./camel_data/ai_society_solution_extraction_plus_gpt_solution"
data_dir = "./camel_data/ai_society_solution_extraction"
save_dir = "./camel_data/ai_society_solution_extraction_save"


def process_file(data: Dict[str, str]) -> None:
    print(data["id"])
    assistant_sys_msg = "You are a helpful assistant."
    agent = ChatAgent(assistant_sys_msg)
    agent.reset()

    prompt = "Solve the following task:\n" + data["specified_task"]
    assistant_response = agent.step(prompt)
    print(assistant_response.msg.content)

    # Append solution to JSON file as "gpt_solution"
    data["gpt_solution"] = assistant_response.msg.content

    # create save_dir if not exists
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # save result as json file
    with open(os.path.join(save_dir, data["id"] + ".json"), 'w') as f:
        json.dump(data, f, ensure_ascii=False)


def main():
    # read all json files in data_dir
    files = [f for f in os.listdir(data_dir) if f.endswith('.json')]

    # load all json files as data list
    data_list = []
    for file in files:
        with open(os.path.join(data_dir, file)) as f:
            data_list.append(json.load(f))

    # Specify number of processes with max_workers argument (default: 16)
    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
        futures = []
        for data in data_list:
            futures.append(executor.submit(process_file, data))

        for future in concurrent.futures.as_completed(futures):
            try:
                future.result()
            except Exception as e:
                print(f"Exception occurred: {e}")


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: tasks\task_generation.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.tasks import (
    Task,
    TaskManager,
)
from camel.types import (
    ModelPlatformType,
    ModelType,
)

# set up LLM model
assistant_model_config = ChatGPTConfig(
    temperature=0.0,
)

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=assistant_model_config.as_dict(),
)

# set up agent
assistant_sys_msg = "You are a personal math tutor and programmer."
agent = ChatAgent(assistant_sys_msg, model)
agent.reset()

task = Task(
    content="Weng earns $12 an hour for babysitting. Yesterday, she just did 51 minutes of babysitting. How much did she earn?",
    id="0",
)
print(task.to_string())


task_manager = TaskManager(task)

evolved_task = task_manager.evolve(task, agent=agent)
if evolved_task is not None:
    print(evolved_task.to_string())
else:
    print("Evolved task is None.")


new_tasks = task.decompose(agent=agent)
for t in new_tasks:
    print(t.to_string())

# ruff: noqa: E501
"""
===============================================================================
Task 0: Weng earns $12 an hour for babysitting. Yesterday, she just did 51 
minutes of babysitting. How much did she earn?

Task 0.0: Weng earns $12 an hour for babysitting. However, her hourly rate 
increases by $2 for every additional hour worked beyond the first hour. 
Yesterday, she babysat for a total of 3 hours and 45 minutes. How much did she 
earn in total for her babysitting services?

Task 0.0: Convert 51 minutes to hours.

Task 0.1: Calculate the proportion of 51 minutes to an hour.

Task 0.2: Multiply the proportion by Weng's hourly rate to find out how much 
she earned for 51 minutes of babysitting.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: test\test_ai_society_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from mock import patch

import examples.ai_society.role_playing
import examples.toolkits.role_playing_with_functions
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

test_model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.STUB,
)


def test_ai_society_role_playing_example():
    with patch('time.sleep', return_value=None):
        examples.ai_society.role_playing.main(
            model=test_model, chat_turn_limit=2
        )


def test_role_playing_with_function_example():
    with patch('time.sleep', return_value=None):
        examples.toolkits.role_playing_with_functions.main(
            model_platform=ModelPlatformType.DEFAULT,
            model_type=ModelType.STUB,
            chat_turn_limit=2,
        )



--------------------------------------------------------------------------------
# File: test\test_babyagi_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import pytest
from mock import patch

import examples.ai_society.babyagi_playing
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

parametrize = pytest.mark.parametrize(
    'model',
    [
        ModelFactory.create(
            model_platform=ModelPlatformType.OPENAI,
            model_type=ModelType.STUB,
        ),
        pytest.param(None, marks=pytest.mark.model_backend),
    ],
)


@parametrize
def test_ai_society_babyagi_playing_example(model):
    with patch('time.sleep', return_value=None):
        examples.ai_society.babyagi_playing.main(
            model=model, chat_turn_limit=2
        )



--------------------------------------------------------------------------------
# File: test\test_code_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from mock import patch

import examples.code.role_playing
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType


def test_code_role_playing_example():
    with patch('time.sleep', return_value=None):
        examples.code.role_playing.main(
            ModelFactory.create(
                model_platform=ModelPlatformType.OPENAI,
                model_type=ModelType.STUB,
            ),
            chat_turn_limit=2,
        )



--------------------------------------------------------------------------------
# File: test\test_role_description_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from mock import patch

import examples.role_description.role_generation
import examples.role_description.role_playing_with_role_description
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

model_gpt = ModelFactory.create(
    ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O,
)

model_stub = ModelFactory.create(
    ModelPlatformType.OPENAI,
    model_type=ModelType.STUB,
)


def test_role_generation_example():
    with patch('time.sleep', return_value=None):
        examples.role_description.role_generation.main(model_gpt)


def test_role_playing_with_role_description_example():
    with patch('time.sleep', return_value=None):
        examples.role_description.role_playing_with_role_description.main(
            model_gpt, model_stub, chat_turn_limit=2
        )



--------------------------------------------------------------------------------
# File: test\test_single_agent.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import pytest

import examples.code.generate_meta_data
import examples.code.task_generation
import examples.evaluation.single_agent
import examples.misalignment.single_agent
import examples.single_agent
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType

parametrize = pytest.mark.parametrize(
    'model',
    [
        ModelFactory.create(
            ModelPlatformType.OPENAI,
            model_type=ModelType.STUB,
        ),
        pytest.param(None, marks=pytest.mark.model_backend),
    ],
)


@parametrize
def test_single_agent(model):
    examples.single_agent.main(model=model)


@pytest.mark.parametrize(
    'model',
    [
        ModelFactory.create(
            ModelPlatformType.OPENAI,
            model_type=ModelType.STUB,
        )
    ],
)
def test_misalignment_single_agent(model):
    examples.misalignment.single_agent.main(model=model)


@parametrize
def test_evaluation_single_agent(model):
    examples.evaluation.single_agent.main(model=model)


@parametrize
def test_code_generate_metadata(model):
    examples.code.generate_meta_data.main(model=model)


@pytest.mark.parametrize(
    'model',
    [
        ModelFactory.create(
            ModelPlatformType.OPENAI,
            model_type=ModelType.STUB,
        )
    ],
)
def test_code_task_generation(model):
    examples.code.task_generation.main(model=model)



--------------------------------------------------------------------------------
# File: test\test_unstructured_io_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import os

import pytest

from examples.loaders.unstructured_io_example import (
    chunk_url_content_example,
    clean_text_example,
    extract_data_example,
    parse_file_example,
    parse_url_example,
    stage_data_example,
)


@pytest.fixture
def sample_url():
    return (
        "https://www.cnn.com/2023/01/30/sport/empire-state-building-green-"
        "philadelphia-eagles-spt-intl/index.html"
    )


@pytest.fixture
def sample_dirty_text():
    return "Some dirty text â€™ with extra spaces and – dashes."  # noqa: RUF001


@pytest.fixture
def sample_email_text():
    return "Contact me at example@email.com."


# Define test cases


def test_parse_file_example():
    # Setup: ensure any pre-existing 'mydoc.docx' is removed
    if os.path.exists("mydoc.txt"):
        os.remove("mydoc.txt")

    # Execution: call the function
    content = parse_file_example()

    # Assertion: check if the result is as expected
    expected_string = (
        "Important Analysis\n\nHere is my first "
        "thought.\n\nHere is my second thought."
    )
    assert content == expected_string

    # Cleanup: remove the created file after the test
    if os.path.exists("mydoc.txt"):
        os.remove("mydoc.txt")


def test_parse_url_example(sample_url):
    content = parse_url_example(sample_url)
    assert isinstance(content, str)
    assert len(content) > 0


def test_clean_text_example(sample_dirty_text):
    cleaned_text = clean_text_example(sample_dirty_text)
    assert isinstance(cleaned_text, str)
    assert cleaned_text == "Some dirty text with extra spaces and dashes."


def test_extract_data_example(sample_email_text):
    extracted_data = extract_data_example(sample_email_text)
    assert isinstance(extracted_data, list)
    assert extracted_data == ["example@email.com"]


def test_stage_data_example(sample_url):
    staged_data = stage_data_example(sample_url)
    assert isinstance(staged_data, dict)
    assert staged_data['rows'][0] == {
        'data': {
            'type': 'NarrativeText',
            'element_id': '0aafb4e862cf2f95e55f76b641766e39',
            'text': 'Miles Sanders scores a touchdown against the San Francisco 49ers during the NFC Championship game at Lincoln Financial Field.',  # noqa: E501
        },
        'metadata': {
            'languages': ['eng'],
            'filetype': 'text/html',
            'url': 'https://www.cnn.com/2023/01/30/sport/empire-state-building-green-philadelphia-eagles-spt-intl/index.html',
        },
    }


def test_chunk_url_content_example(sample_url):
    chunked_sections = chunk_url_content_example(sample_url)
    assert len(chunked_sections) == 7



--------------------------------------------------------------------------------
# File: toolkits\arxiv_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs.openai_config import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import ArxivToolkit
from camel.types import ModelPlatformType, ModelType

# Define system message
sys_msg = "You are a helpful assistant"

# Set model config
tools = ArxivToolkit().get_tools()
model_config_dict = ChatGPTConfig(
    temperature=0.0,
).as_dict()

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=model_config_dict,
)

# Set agent
camel_agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)
camel_agent.reset()

# Define a user message
usr_msg = "Search paper 'attention is all you need' for me"

# Get response information
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])
'''
===============================================================================
[ToolCallingRecord(func_name='search_papers', args={'query': 'attention is 
all you need'}, result=[{'title': "Attention Is All You Need But You Don't 
Need All Of It For Inference of Large Language Models", 'published_date': 
'2024-07-22', 'authors': ['Georgy Tyukin', 'Gbetondji J-S Dovonon', 'Jean 
Kaddour', 'Pasquale Minervini'], 'entry_id': 'http://arxiv.org/abs/2407.
15516v1', 'summary': 'The inference demand for LLMs has skyrocketed in recent 
months, and serving\nmodels with low latencies remains challenging due to the 
quadratic input length\ncomplexity of the attention layers. In this work, we 
investigate the effect of\ndropping MLP and attention layers at inference time 
on the performance of\nLlama-v2 models. We find that dropping dreeper 
attention layers only marginally\ndecreases performance but leads to the best 
speedups alongside dropping entire\nlayers. For example, removing 33\\% of 
attention layers in a 13B Llama2 model\nresults in a 1.8\\% drop in average 
performance ove...
===============================================================================
'''


# Define a user message
usr_msg = """Download paper "attention is all you need" for me to my 
    local path '/Users/enrei/Desktop/camel0826/camel/examples/tool_call'"""

# Get response information
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])
'''
===============================================================================
[ToolCallingRecord(func_name='download_papers', args={'query': 'attention 
is all you need', 'output_dir': '/Users/enrei/Desktop/camel0826/camel/examples/
tool_call', 'paper_ids': ['2407.15516v1', '2107.08000v1', '2306.01926v1', 
'2112.05993v1', '1912.11959v2']}, result='papers downloaded successfully')]
===============================================================================
'''



--------------------------------------------------------------------------------
# File: toolkits\ask_news_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.toolkits import AskNewsToolkit

ask_news = AskNewsToolkit()

news_output = ask_news.get_news(query="President of United States")
print(news_output[:1000])

"""
===============================================================================
<doc>
[1]:
Title: Can Elon Musk Become President of the United States?
Summary: Elon Musk, the American billionaire, has been appointed to lead the 
Department of Government Efficiency in Donald Trump's upcoming administration, 
sparking speculation about his potential presidential ambitions. However, 
according to the US Constitution, the President must be a natural-born citizen 
of the United States. As Musk was born in South Africa and became a Canadian 
citizen through his mother, he does not meet this requirement. While he 
acquired US citizenship in 2002, this does not make him a natural-born 
citizen. Additionally, the Constitution requires the President to be at least 
35 years old and a resident of the United States for at least 14 years. Musk 
can, however, hold other government positions, as the requirement of being a 
natural-born citizen only applies to the President and Vice President. Many 
non-US-born citizens have held prominent government positions in the past, 
including Henry
===============================================================================
"""

story_output = ask_news.get_stories(
    query="camel-ai", categories=["Technology"]
)
print(story_output)

web_search_output = ask_news.get_web_search(queries=["camel-ai"])
print(web_search_output)

reddit_output = ask_news.search_reddit(keywords=["camel-ai", "multi-agent"])
print(reddit_output)

finance_output = ask_news.query_finance(asset="bitcoin")
print(finance_output)



--------------------------------------------------------------------------------
# File: toolkits\audio_analysis_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory, OpenAIAudioModels
from camel.toolkits import AudioAnalysisToolkit
from camel.types import ModelPlatformType, ModelType

audio_models = OpenAIAudioModels()

# Set example input
input = """CAMEL-AI.org is an open-source community dedicated to the study of 
autonomous and communicative agents. We believe that studying these agents on 
a large scale offers valuable insights into their behaviors, capabilities, and 
potential risks. To facilitate research in this field, we provide, implement, 
and support various types of agents, tasks, prompts, models, datasets, and 
simulated environments.

Join us via Slack, Discord, or WeChat in pushing the boundaries of building AI 
Society."""

# Set example local path to store the file
storage_path = "examples/openai_audio_models/example_audio.mp3"

# Convert the example input into audio and store it locally
audio_models.text_to_speech(input=input, storage_path=storage_path)

model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=ChatGPTConfig(
        temperature=0.0,
    ).as_dict(),
)


audio_reason_model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=ChatGPTConfig(
        temperature=0.0,
    ).as_dict(),
)

# Create the AudioAnalysisToolkit with our reasoning model
audio_toolkit = AudioAnalysisToolkit(audio_reasoning_model=audio_reason_model)

# Create a ChatAgent with the audio toolkit tools
agent = ChatAgent(
    system_message="You are an assistant specialized in audio analysis.",
    model=model,
    tools=[*audio_toolkit.get_tools()],
)

question = "What content can you hear in this audio?"
response = agent.step(
    f"I have an audio file at {storage_path}. Can you analyze it and tell "
    f"me {question}"
)
print(response.msgs[0].content)
print("\n")

response = agent.step(f"Please transcribe the audio file at {storage_path}")
print(response.msgs[0].content)
print("\n")

"""
==========================================================================
2025-03-09 22:54:55,822 - camel.camel.toolkits.audio_analysis_toolkit - 
WARNING - No audio transcription model provided. Using OpenAIAudioModels.

The audio content discusses Camel AI, an open-source community dedicated to 
the study of autonomous and communicative agents. It emphasizes the belief 
that large-scale research on these agents can yield valuable insights into 
their behaviors, capabilities, and potential risks. The community provides 
resources to support research, including various types of agents, tasks, 
prompts, models, datasets, and simulated environments. Additionally, it 
invites listeners to join the community through platforms like Slack, Discord, 
or WeChat to contribute to the development of AI society.


Here is the transcription of the audio:

"CamelAI.org is an open-source community dedicated to the study of autonomous 
and communicative agents. We believe that studying these agents on a large 
scale offers valuable insights into their behaviors, capabilities, and 
potential risks. To facilitate research in this field, we provide, implement, 
and support various types of agents, tasks, prompts, models, datasets, and 
simulated environments. Join us via Slack, Discord, or WeChat in pushing the 
boundaries of building AI society."
==========================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\browser_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========


from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import BrowserToolkit
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=ChatGPTConfig(
        temperature=0.0,
    ).as_dict(),
)

web_agent_model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=ChatGPTConfig(
        temperature=0.0,
    ).as_dict(),
)

planning_agent_model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=ChatGPTConfig(
        temperature=0.0,
    ).as_dict(),
)

web_toolkit = BrowserToolkit(
    headless=False,
    web_agent_model=web_agent_model,
    planning_agent_model=planning_agent_model,
    channel="chromium",
)

agent = ChatAgent(
    system_message="You are a helpful assistant.",
    model=model,
    tools=[*web_toolkit.get_tools()],
)

response = agent.step(
    "Navigate to Amazon.com and identify the current #1 best-selling product"
    " in the gaming category. Please provide the product name, price, and"
    " rating if available.",
)

print(response.msgs[0].content)
"""
==========================================================================
The current #1 best-selling product in the gaming category on Amazon is the 
**AutoFull C3 Gaming Chair**. 

- **Price:** $249.99
- **Rating:** 4.4 stars based on 5,283 ratings.
==========================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\code_execution_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from colorama import Fore

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits.code_execution import CodeExecutionToolkit
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated

# tools
toolkit = CodeExecutionToolkit(verbose=True)
tools = toolkit.get_tools()

# set up LLM model
assistant_model_config = ChatGPTConfig(
    temperature=0.0,
)

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=assistant_model_config.as_dict(),
)


# set up agent
assistant_sys_msg = (
    "You are a personal math tutor and programmer. "
    "When asked a math question, "
    "write and run Python code to answer the question."
)

agent = ChatAgent(
    assistant_sys_msg,
    model,
    tools=tools,
)
agent.reset()


# set up agent

prompt = (
    "Weng earns $12 an hour for babysitting. "
    "Yesterday, she just did 51 minutes of babysitting. How much did she earn?"
)
print(Fore.YELLOW + f"user prompt:\n{prompt}\n")

response = agent.step(prompt)
for msg in response.msgs:
    print_text_animated(Fore.GREEN + f"Agent response:\n{msg.content}\n")

# ruff: noqa: E501
"""
===============================================================================
user prompt:
Weng earns $12 an hour for babysitting. Yesterday, she just did 51 minutes of babysitting. How much did she earn?

Executed the code below:
```py
hourly_rate = 12
minutes_worked = 51
hourly_earnings = hourly_rate / 60 * minutes_worked
hourly_earnings
```
> Executed Results:
10.200000000000001
Agent response:
Weng earned $10.20 for babysitting for 51 minutes at a rate of $12 per hour.
===============================================================================
"""

agent_with_e2b = ChatAgent(
    assistant_sys_msg,
    model,
    tools=CodeExecutionToolkit(verbose=True, sandbox="e2b").get_tools(),
)
agent_with_e2b.reset()

print(Fore.YELLOW + f"user prompt:\n{prompt}\n")

response_with_e2b = agent_with_e2b.step(prompt)
for msg in response_with_e2b.msgs:
    print_text_animated(Fore.GREEN + f"Agent response:\n{msg.content}\n")

# ruff: noqa: E501
"""
===============================================================================
user prompt:
Weng earns $12 an hour for babysitting. Yesterday, she just did 51 minutes of babysitting. How much did she earn?

Executed the code below:
```py
hourly_wage = 12
minutes_worked = 51
# Convert minutes to hours
hours_worked = minutes_worked / 60
# Calculate earnings
earnings = hourly_wage * hours_worked
earnings
```
> Executed Results:
10.2
Agent response:
Weng earned $10.20 for 51 minutes of babysitting.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\dappier_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.toolkits import DappierToolkit, FunctionTool

real_time_data_response = DappierToolkit().search_real_time_data(
    query="camel-ai"
)

print(real_time_data_response)
"""
===============================================================================
CAMEL-AI is pretty cool! It's the first LLM (Large Language Model) multi-agent 
framework and an open-source community focused on exploring the scaling laws 
of agents. 🌟

Here are some highlights:

- **Purpose**: It aims to create highly customizable intelligent agents and 
    build multi-agent systems for real-world applications.
- **Features**: CAMEL provides a role-playing approach and inception prompting
    to help chat agents complete tasks aligned with human intentions.
- **Use Cases**: You can turn your database into an AI-powered data analyst,
    allowing you to ask questions in plain English and get instant insights.
    📊🤖
- **Community**: It's an open-source initiative, so developers can contribute
    and collaborate on building and using LLM-based agents.

If you want to dive deeper, check out their website:
[CAMEL-AI.org](https://www.camel-ai.org) 🚀!
===============================================================================
"""

# Use a different AI model which has access to real-time financial news.
real_time_data_response = DappierToolkit().search_real_time_data(
    query="Could you please provide the stock price for Google on 05/03/24?",
    ai_model_id="am_01j749h8pbf7ns8r1bq9s2evrh",
)
print(real_time_data_response)
"""
===============================================================================
The stock price for Google (GOOGL) on May 3rd, 2024, was $167.10.
===============================================================================
"""

# Example with ChatAgent using the Real Time Search.
agent = ChatAgent(
    system_message="""You are a helpful assistant that can use brave search 
        engine to answer questions.""",
    tools=[FunctionTool(DappierToolkit().search_real_time_data)],
)

usr_msg = "What is the temperature in Tokyo?"

response = agent.step(input_message=usr_msg, response_format=None)

print(response.msgs[0].content)
"""
===============================================================================
The current temperature in Tokyo is 50°F (about 10°C). It's a bit chilly, 
so you might want to grab a jacket! 🧥🌬️
===============================================================================
"""

ai_recommendations_response = DappierToolkit().get_ai_recommendations(
    query="latest sports news",
    data_model_id="dm_01j0pb465keqmatq9k83dthx34",
    similarity_top_k=3,
    ref="sportsnaut.com",
    num_articles_ref=2,
    search_algorithm="most_recent",
)
print(ai_recommendations_response)
"""
===============================================================================
{'author': 'Andrew Buller-Russ', 
'image_url': 'https://images.dappier.com/dm_01j0pb465keqmatq9k83dthx34/
Syndication-Detroit-Free-Press-25087075_.jpg?width=428&height=321', 
'pubdate': 'Thu, 02 Jan 2025 03:12:06 +0000', 
'source_url': 'https://sportsnaut.com/nick-bosa-detroit-lions-trade-rumors-49ers/', 
'summary': 'In a thrilling Monday night game, the Detroit Lions triumphed 
over the San Francisco 49ers 40-34, solidifying their status as a top NFL 
team. Despite a strong performance from Nick Bosa, who recorded eight tackles 
and two sacks, the 49ers\' playoff hopes were dashed. Bosa praised the Lions\' 
competitive spirit and resilience under Coach Dan Campbell, sparking 
about his interest in joining the team, although he remains under contract 
with the 49ers for four more seasons. Bosa\'s admiration for the Lions 
highlights the stark contrast between the two franchises\' fortunes, 
with the Lions celebrating a significant victory while the 49ers struggle.
Having experienced playoff success with the 49ers, Bosa values strong 
leadership from both Campbell and his own coach, Kyle Shanahan. His comments 
reflect a broader sentiment in the NFL about the importance of winning and 
the positive environment it fosters for players.', 
'title': 'Nick Bosa gushes about Detroit Lions, sparking 49ers trade rumors'}

{'author': 'Andrew Buller-Russ', 
'image_url': 'https://images.dappier.com/dm_01j0pb465keqmatq9k83dthx34/
Baseball-World-Baseball-Classic-Semifinal-Japan-vs-Mexico-20279015_.jpg?width=428&height=321', 
'pubdate': 'Thu, 02 Jan 2025 02:43:38 +0000', 
'source_url': 'https://www.lafbnetwork.com/los-angeles-dodgers/
los-angeles-dodgers-news/los-angeles-dodgers-meeting-roki-sasaki/', 
'summary': 'Roki Sasaki, a talented 23-year-old Japanese pitcher, is 
approaching a decision on his MLB free agency, with the Los Angeles Dodgers 
among the frontrunners to sign him. They are competing against teams like 
the Chicago Cubs, New York Mets, and others. The Dodgers are set to meet 
with Sasaki, emphasizing his signing as a top priority despite facing 
competition from around 20 other teams. Sasaki\'s status as a minor-league 
posting player may allow him to be signed at a more affordable price, 
increasing his appeal. As he gathers information and prepares for a second
round of meetings, the Dodgers are keen to secure him before the posting 
window closes on January 24, with the international signing period beginning 
on January 15.', 'title': 'Los Angeles Dodgers Take Another Step Toward 
Signing Roki Sasaki'}

{'author': 'Andrew Buller-Russ', 
'image_url': 'https://images.dappier.com/dm_01j0pb465keqmatq9k83dthx34/
NFL-Detroit-Lions-at-Kansas-City-Chiefs-24020812_.jpg?width=428&height=321', 
'pubdate': 'Thu, 02 Jan 2025 02:08:34 +0000', 
'source_url': 'https://sportsnaut.com/detroit-lions-cut-jamal-adams/', 
'summary': 'The Detroit Lions, with a strong 14-2 record, have released 
former All-Pro safety Jamal Adams from their practice squad ahead of a crucial 
Week 18 game against the Minnesota Vikings. Adams, who joined the Lions on 
December 1, 2024, played in two games but recorded only three tackles in 
20 defensive snaps, representing a mere 17% of the team\'s defensive plays. 
This marks Adams\' second release this season, having previously been cut 
by the Tennessee Titans after three appearances. The Lions\' decision to part 
ways with Adams comes as they focus on their playoff positioning for the 
upcoming game.', 
'title': 'Detroit Lions cut bait with All-Pro ahead of Week 18 matchup with 
Vikings'}
===============================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\data_commons_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.toolkits.data_commons_toolkit import DataCommonsToolkit

# Initialize the DataCommonsToolkit
dc_toolkit = DataCommonsToolkit()

# Example 1: Query Data Commons
geoId06_name_query = '''
SELECT ?name ?dcid 
WHERE {
    ?a typeOf Place .
    ?a name ?name .
    ?a dcid ("geoId/06" "geoId/21" "geoId/24") .
    ?a dcid ?dcid
}
'''
result = dc_toolkit.query_data_commons(geoId06_name_query)
print("Query Result:")
print(result)

'''
===============================================================================
Query Result:
[{'?name': 'Kentucky', '?dcid': 'geoId/21'}, 
 {'?name': 'California', '?dcid': 'geoId/06'}, 
 {'?name': 'Maryland', '?dcid': 'geoId/24'}]
===============================================================================
'''

# Example 2: Get Triples
dcids = ["geoId/06", "geoId/21", "geoId/24"]
triples = dc_toolkit.get_triples(dcids)
print("\nTriples for California, Kentucky, and Maryland:")
print(triples)

'''
===============================================================================
Triples for California, Kentucky, and Maryland:
{
    "geoId/06": [
        ("name", "California"),
        ("containedInPlace", "country/USA"),
        ...
    ],
    "geoId/21": [
        ("name", "Kentucky"),
        ("containedInPlace", "country/USA"),
        ...
    ],
    "geoId/24": [
        ("name", "Maryland"),
        ("containedInPlace", "country/USA"),
        ...
    ]
}
===============================================================================
'''

# Example 3: Get Statistical Time Series
place = "geoId/06"
stat_var = "Count_Person"
series = dc_toolkit.get_stat_time_series(place, stat_var)
print("\nPopulation Time Series for California:")
print(series)

'''
===============================================================================
Population Time Series for California:
{
    "2010": 37253956,
    "2011": 37594778,
    "2012": 37971427,
    ...
}
===============================================================================
'''

# Example 4: Get Property Values
dcids = ["geoId/06", "geoId/21", "geoId/24"]
prop = "containedInPlace"
values = dc_toolkit.get_property_values(dcids, prop)
print("\nContained In Place for California, Kentucky, and Maryland:")
print(values)

'''
===============================================================================
Contained In Place for California, Kentucky, and Maryland:
{
    "geoId/06": ["country/USA"],
    "geoId/21": ["country/USA"],
    "geoId/24": ["country/USA"]
}
===============================================================================
'''

# Example 5: Get Statistical Value
place = "geoId/06"
stat_var = "Count_Person"
date = "2021"
value = dc_toolkit.get_stat_value(place, stat_var, date)
print("\nPopulation of California in 2021:")
print(value)

'''
===============================================================================
Population of California in 2021:
39237836
===============================================================================
'''



--------------------------------------------------------------------------------
# File: toolkits\excel_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import os
import tempfile

import pandas as pd

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import ExcelToolkit
from camel.types import ModelPlatformType, ModelType

# Create a sample Excel file for demonstration
temp_file = tempfile.NamedTemporaryFile(suffix=".csv", delete=False)
sample_file_path = temp_file.name

# Create a sample DataFrame
df = pd.DataFrame(
    {
        'Name': ['Alice', 'Bob', 'Charlie'],
        'Age': [25, 30, 35],
        'City': ['New York', 'San Francisco', 'Seattle'],
        'Department': ['Engineering', 'Marketing', 'Finance'],
    }
)

# Save the DataFrame to the CSV file
df.to_csv(sample_file_path, index=False)
print(f"Created sample Excel file at: {sample_file_path}")

# Initialize the Excel toolkit
excel_toolkit = ExcelToolkit()

# Create a model using OpenAI
model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=ChatGPTConfig(
        temperature=0.0,
    ).as_dict(),
)

# Create a chat agent with the Excel toolkit
agent = ChatAgent(
    system_message=(
        "You are a helpful assistant that can analyze Excel files. "
        "Use the provided Excel toolkit to extract and analyze data."
    ),
    model=model,
    tools=[*excel_toolkit.get_tools()],
)

# Example: Ask the agent to analyze the Excel file
response = agent.step(
    f"Analyze the Excel file at {sample_file_path} and tell me what data "
    f"it contains."
)

print(response.msgs[0].content)

# Clean up the temporary file
if os.path.exists(sample_file_path):
    os.remove(sample_file_path)
    print(f"Removed temporary file: {sample_file_path}")

'''
===============================================================================
Created sample Excel file at: /var/folders/93/f_71_t957cq9cmq2gsybs4_40000gn/T/
tmpqweue66k.csv
The Excel file contains the following data:

| Name    | Age | City          | Department   |
|---------|-----|---------------|--------------|
| Alice   | 25  | New York      | Engineering   |
| Bob     | 30  | San Francisco | Marketing     |
| Charlie | 35  | Seattle       | Finance       |

### Summary:
- **Total Records**: 3
- **Columns**:
  - **Name**: Names of individuals
  - **Age**: Ages of individuals
  - **City**: Cities where individuals reside
  - **Department**: Departments where individuals work
Removed temporary file: /var/folders/93/f_71_t957cq9cmq2gsybs4_40000gn/T/
tmpqweue66k.csv
===============================================================================
'''



--------------------------------------------------------------------------------
# File: toolkits\file_write_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# ruff: noqa: E501
import os

from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.toolkits import FileWriteToolkit
from camel.types import ModelPlatformType
from camel.types.enums import ModelType

# Create a model instance
model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict={"temperature": 0},
)

# Define system message for the agent
sys_msg = "You are a helpful assistant that can create and modify files."

# Set up output directory
output_dir = "./file_write_outputs"
os.makedirs(output_dir, exist_ok=True)

# Initialize the FileWriteToolkit with the output directory
file_toolkit = FileWriteToolkit(output_dir=output_dir)

# Get the tools from the toolkit
tools_list = file_toolkit.get_tools()

# Initialize a ChatAgent with the tools
camel_agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools_list,
)

# Example 1: Write a Python script to a file
python_query = """Please generate a Python script that creates a simple 
                  web server using Flask and save it to a file."""

camel_agent.reset()
response = camel_agent.step(python_query)
print("Example 1: Writing a Python script")
print(response.msgs[0].content)
print("Tool calls:", response.info['tool_calls'])
print("\n")
'''
===============================================================================
Example 1: Writing a Python script
The Python script for a simple web server using Flask has been created and saved as `simple_flask_server.py`. You can run this script to start the server, and it will return "Hello, Flask!" when accessed at the root URL.
Tool calls: [ToolCallingRecord(tool_name='write_to_file', args={'content': 'from flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\'/\')\ndef home():\n    return "Hello, Flask!"\n\nif __name__ == \'__main__\':\n    app.run(debug=True)', 'filename': 'simple_flask_server.py', 'encoding': 'utf-8'}, result='Content successfully written to file: /Users/enrei/Desktop/camel0209/camel/file_write_outputs/simple_flask_server.py', tool_call_id='call_hCCxkjNkx4HKN9q6fuIpe8Bn')]
===============================================================================
'''

# Example 2: Create a JSON data file
json_query = """Generate a JSON file containing information about 3 fictional
                books, including title, author, publication year, and genre."""
camel_agent.reset()
response = camel_agent.step(json_query)
print("Example 2: Creating a JSON file")
print(response.msgs[0].content)
print("Tool calls:", response.info['tool_calls'])
print("\n")
'''
===============================================================================
Example 2: Creating a JSON file
The JSON file containing information about three fictional books has been successfully created. You can find it at the following location: **books.json**. 

Here is the content of the file:

```json
[
  {
    "title": "The Whispering Shadows",
    "author": "Ava Sinclair",
    "publication_year": 2021,
    "genre": "Fantasy"
  },
  {
    "title": "Echoes of the Past",
    "author": "Liam Carter",
    "publication_year": 2019,
    "genre": "Historical Fiction"
  },
  {
    "title": "The Last Star",
    "author": "Maya Thompson",
    "publication_year": 2022,
    "genre": "Science Fiction"
  }
]
```
Tool calls: [ToolCallingRecord(tool_name='write_to_file', args={'content': '[  \n  {  \n    "title": "The Whispering Shadows",  \n    "author": "Ava Sinclair",  \n    "publication_year": 2021,  \n    "genre": "Fantasy"  \n  },  \n  {  \n    "title": "Echoes of the Past",  \n    "author": "Liam Carter",  \n    "publication_year": 2019,  \n    "genre": "Historical Fiction"  \n  },  \n  {  \n    "title": "The Last Star",  \n    "author": "Maya Thompson",  \n    "publication_year": 2022,  \n    "genre": "Science Fiction"  \n  }  \n]', 'filename': 'books.json', 'encoding': 'utf-8'}, result='Content successfully written to file: /Users/enrei/Desktop/camel0209/camel/file_write_outputs/books.json', tool_call_id='call_1ayRgujHhiWowz0jhtMCukgn')]
===============================================================================
'''

# Example 3: Create a CSV file with tabular data
csv_query = """Create a CSV file with data about 5 countries, including
               columns for name, capital, population, area, and continent."""
camel_agent.reset()
response = camel_agent.step(csv_query)
print("Example 3: Creating a CSV file")
print(response.msgs[0].content)
print("Tool calls:", response.info['tool_calls'])
print("\n")
'''
===============================================================================
Example 3: Creating a CSV file
The CSV file containing data about 5 countries has been successfully created. It includes the following columns: name, capital, population, area, and continent. If you need any further modifications or additional data, feel free to ask!
Tool calls: [ToolCallingRecord(tool_name='write_to_file', args={'content': [['Name', 'Capital', 'Population', 'Area (sq km)', 'Continent'], ['United States', 'Washington, D.C.', '331002651', '9833517', 'North America'], ['Brazil', 'Brasília', '212559417', '8515767', 'South America'], ['Germany', 'Berlin', '83783942', '357022', 'Europe'], ['Australia', 'Canberra', '25499884', '7692024', 'Oceania'], ['Japan', 'Tokyo', '126476461', '377975', 'Asia']], 'filename': 'countries_data.csv', 'encoding': 'utf-8'}, result='Content successfully written to file: /Users/enrei/Desktop/camel0209/camel/file_write_outputs/countries_data.csv', tool_call_id='call_yTgErI2TrV32ehs5LJCf6kW7')]
===============================================================================
'''

# Example 4: Create a Markdown document
md_query = """Write a markdown document that explains the basics of machine
              learning, including headings, bullet points, and code examples.
              """
camel_agent.reset()
response = camel_agent.step(md_query)
print("Example 4: Creating a Markdown document")
print(response.msgs[0].content)
print("Tool calls:", response.info['tool_calls'])
print("\n")
'''
===============================================================================
Example 4: Creating a Markdown document
The markdown document explaining the basics of machine learning has been successfully created. You can find it under the name **basics_of_machine_learning.md**. If you need any further modifications or additional information, feel free to ask!
Tool calls: [ToolCallingRecord(tool_name='write_to_file', args={'content': "# Basics of Machine Learning\n\nMachine Learning (ML) is a subset of artificial intelligence (AI) that focuses on building systems that learn from data and improve their performance over time without being explicitly programmed. Here are the key concepts and components of machine learning:\n\n## Key Concepts\n\n- **Data**: The foundation of machine learning. Data can be structured (like tables) or unstructured (like images or text).\n- **Model**: A mathematical representation of a process that is trained on data to make predictions or decisions.\n- **Training**: The process of feeding data into a model to help it learn patterns.\n- **Testing**: Evaluating the model's performance on unseen data to ensure it generalizes well.\n- **Features**: Individual measurable properties or characteristics used as input to the model.\n- **Labels**: The output or target variable that the model is trying to predict.\n\n## Types of Machine Learning\n\n1. **Supervised Learning**: The model is trained on labeled data.\n   - **Examples**: Classification, Regression\n   - **Use Cases**: Spam detection, House price prediction\n\n2. **Unsupervised Learning**: The model is trained on unlabeled data and tries to find patterns.\n   - **Examples**: Clustering, Dimensionality Reduction\n   - **Use Cases**: Customer segmentation, Anomaly detection\n\n3. **Reinforcement Learning**: The model learns by interacting with an environment and receiving feedback.\n   - **Examples**: Game playing, Robotics\n   - **Use Cases**: Self-driving cars, Game AI\n\n## Machine Learning Workflow\n\n1. **Data Collection**: Gather data from various sources.\n2. **Data Preprocessing**: Clean and prepare the data for analysis.\n3. **Model Selection**: Choose the appropriate algorithm for the task.\n4. **Training the Model**: Fit the model to the training data.\n5. **Model Evaluation**: Assess the model's performance using metrics like accuracy, precision, and recall.\n6. **Hyperparameter Tuning**: Optimize the model's parameters for better performance.\n7. **Deployment**: Implement the model in a production environment.\n\n## Code Example\n\nHere is a simple example of a supervised learning model using Python and the popular library `scikit-learn`:\n\n```python\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Load dataset\ndata = pd.read_csv('data.csv')\n\n# Define features and labels\nX = data[['feature1', 'feature2']]\nY = data['label']\n\n# Split the data into training and testing sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# Create a model\nmodel = LinearRegression()\n\n# Train the model\nmodel.fit(X_train, Y_train)\n\n# Make predictions\npredictions = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(Y_test, predictions)\nprint(f'Mean Squared Error: {mse}')\n```\n\n## Conclusion\n\nMachine learning is a powerful tool that can be applied to various fields, from healthcare to finance. Understanding the basics of machine learning is essential for anyone looking to leverage data for decision-making and predictive analytics.", 'filename': 'basics_of_machine_learning.md', 'encoding': 'utf-8'}, result='Content successfully written to file: /Users/enrei/Desktop/camel0209/camel/file_write_outputs/basics_of_machine_learning.md', tool_call_id='call_KKHcbPvmG8VWc4W2JeOWcg1B')]
===============================================================================
'''

# Example 5: Create a YAML configuration file
yaml_query = """Generate a YAML configuration file for a web application
                with settings for database connection, logging, and server
                parameters."""
camel_agent.reset()
response = camel_agent.step(yaml_query)
print("Example 5: Creating a YAML configuration file")
print(response.msgs[0].content)
print("Tool calls:", response.info['tool_calls'])
print("\n")
'''
===============================================================================
Example 5: Creating a YAML configuration file
The YAML configuration file for the web application has been successfully created. Here are the contents of the file:

```yaml
database:
  host: localhost
  port: 5432
  username: user
  password: password
  dbname: mydatabase

logging:
  level: info
  file: /var/log/myapp.log
  max_size: 10MB
  max_backups: 5

server:
  host: 0.0.0.0
  port: 8080
  timeout: 30s
  enable_https: true
```

The file is saved as `config.yaml`. If you need any modifications or additional settings, feel free to ask!
Tool calls: [ToolCallingRecord(tool_name='write_to_file', args={'content': 'database:\n  host: localhost\n  port: 5432\n  username: user\n  password: password\n  dbname: mydatabase\n\nlogging:\n  level: info\n  file: /var/log/myapp.log\n  max_size: 10MB\n  max_backups: 5\n\nserver:\n  host: 0.0.0.0\n  port: 8080\n  timeout: 30s\n  enable_https: true\n', 'filename': 'config.yaml', 'encoding': 'utf-8'}, result='Content successfully written to file: /Users/enrei/Desktop/camel0209/camel/file_write_outputs/config.yaml', tool_call_id='call_svQbTh8tl1diDDYwxNDWUp2U')]
===============================================================================
'''

# Example 6: Create an HTML file
html_query = """Create a simple HTML webpage with a header, navigation menu, 
                main content section, and footer."""
camel_agent.reset()
response = camel_agent.step(html_query)
print("Example 6: Creating an HTML file")
print(response.msgs[0].content)
print("Tool calls:", response.info['tool_calls'])
print("\n")
'''
===============================================================================
Example 6: Creating an HTML file
I have created a simple HTML webpage with a header, navigation menu, main content section, and footer. You can find the file named `simple_webpage.html` in the specified directory. If you need any modifications or additional features, feel free to ask!
Tool calls: [ToolCallingRecord(tool_name='write_to_file', args={'content': '<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <meta name="viewport" content="width=device-width, initial-scale=1.0">\n    <title>Simple Webpage</title>\n    <link rel="stylesheet" href="styles.css">\n</head>\n<body>\n    <header>\n        <h1>Welcome to My Simple Webpage</h1>\n    </header>\n    <nav>\n        <ul>\n            <li><a href="#home">Home</a></li>\n            <li><a href="#about">About</a></li>\n            <li><a href="#services">Services</a></li>\n            <li><a href="#contact">Contact</a></li>\n        </ul>\n    </nav>\n    <main>\n        <section id="home">\n            <h2>Home</h2>\n            <p>This is the home section of the webpage.</p>\n        </section>\n        <section id="about">\n            <h2>About</h2>\n            <p>This section contains information about us.</p>\n        </section>\n        <section id="services">\n            <h2>Services</h2>\n            <p>Details about our services can be found here.</p>\n        </section>\n        <section id="contact">\n            <h2>Contact</h2>\n            <p>Get in touch with us through this section.</p>\n        </section>\n    </main>\n    <footer>\n        <p>&copy; 2023 My Simple Webpage. All rights reserved.</p>\n    </footer>\n</body>\n</html>', 'filename': 'simple_webpage.html', 'encoding': 'utf-8'}, result='Content successfully written to file: /Users/enrei/Desktop/camel0209/camel/file_write_outputs/simple_webpage.html', tool_call_id='call_6FUwTx4gSAB8mtN7lety05SP')]
===============================================================================
'''



--------------------------------------------------------------------------------
# File: toolkits\generate_openai_tool_schema.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========


from camel.agents import ChatAgent
from camel.toolkits import FunctionTool


# Define a function which doesn't have a docstring
def get_perfect_square(n: int) -> int:
    return n**2


# Create a FunctionTool with the function
function_tool = FunctionTool(
    get_perfect_square,
    synthesize_schema=True,
)
print("\nGenerated OpenAI Tool Schema:")
print(function_tool.get_openai_tool_schema())

# Set system message for the assistant
assistant_sys_msg = "You are a helpful assistant."

# Create a ChatAgent with the tool
camel_agent = ChatAgent(
    system_message=assistant_sys_msg, tools=[function_tool]
)
camel_agent.reset()

# Define a user message
user_prompt = "What is the perfect square of 2024?"
user_msg = user_prompt

# Get response from the assistant
response = camel_agent.step(user_msg)
print("\nAssistant Response:")
print(response.msg.content)

"""
===============================================================================
Warning: No model provided. Use `gpt-4o-mini` to generate the schema.

Generated OpenAI Tool Schema:
{'type': 'function', 'function': {'name': 'get_perfect_square', 'description':
'Calculates the perfect square of a given integer.', 'parameters':
{'properties': {'n': {'type': 'integer', 'description': 'The integer to be
squared.'}}, 'required': ['n'], 'type': 'object'}}}

Assistant Response:
The perfect square of 2024 is 4,096,576.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\github_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.toolkits import GithubToolkit

gt = GithubToolkit(repo_name="camel-ai/camel")

# Retrieve a list of all file paths within the camel GitHub repository
paths = gt.get_all_file_paths()
print(paths)
"""
===============================================================================
['.container/.env.example', '.container/Dockerfile', '.container/README.md', '.
container/docker-compose.yaml', '.container/minimal_build/Dockerfile', '.
github/ISSUE_TEMPLATE/bug_report.yml', '.github/ISSUE_TEMPLATE/discussions.
yml', '.github/ISSUE_TEMPLATE/feature_request.yml', '.github/ISSUE_TEMPLATE/
questions.yml', '.github/PULL_REQUEST_TEMPLATE.md', '.github/actions/
camel_install/action.yml', '.github/workflows/build_package.yml', '.github/
workflows/documentation.yml', '.github/workflows/pre_commit.yml', '.github/
workflows/publish_release.yml', '.github/workflows/pytest_apps.yml', '.github/
workflows/pytest_package.yml', '.gitignore', '.pre-commit-config.yaml', '.
style.yapf', 'CONTRIBUTING.md', 'LICENSE', 'Makefile', 'README.md', 'apps/
agents/README.md', 'apps/agents/agents.py', 'apps/agents/test/test_agents.py', 
'apps/agents/test/test_text_utils.py', 'apps/agents/text_utils.py', 'apps/
common/auto_zip.py', 'apps/common/test/test_archive_1.zip', 'apps/common/test/
test_auto_zip.py', 'apps/data_explorer/.gitignore', 'apps/data_explorer/README.
md', 'apps/data_explorer/data_explorer.py', 'apps/data_explorer/downloader.
py', 'apps/data_explorer/loader.py', 'apps/data_explorer/test/
test_data_explorer.py', 'apps/data_explorer/test/test_loader.py', 'apps/
dilemma/database_connection.py', 'apps/dilemma/dilemma.py', 'apps/dilemma/
requirements.txt', 'camel/__init__.py', 'camel/agents/__init__.py', 'camel/
agents/base.py', 'camel/agents/chat_agent.py', 'camel/agents/critic_agent.py', 
'camel/agents/deductive_reasoner_agent.py',...
===============================================================================
"""

# Retrieve the content of a specific file in the repository
content = gt.retrieve_file_content("camel/agents/chat_agent.py")
print(content[:1000])
"""
===============================================================================
from __future__ import annotations

import json
import logging
import re
import uuid
from collections import defaultdict
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    List,
    Optional,
    Tuple,
    Type,
    Union,
)

from openai.types.chat import ChatCompletionMessageToolCall
f
===============================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\google_scholar_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs.openai_config import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import GoogleScholarToolkit
from camel.types import ModelPlatformType, ModelType

# Define system message
sys_msg = "You are a helpful assistant"

# Set model config
tools = GoogleScholarToolkit(
    author_identifier="https://scholar.google.com/citations?user=JicYPdAAAAAJ&hl=en&oi=ao"
).get_tools()

model_config_dict = ChatGPTConfig(
    temperature=0.0,
).as_dict()

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=model_config_dict,
)

# Set agent
camel_agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)
camel_agent.reset()

# Define a user message
usr_msg = "get the detailed information of this author"

# Get response information
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])
"""
===============================================================================
[ToolCallingRecord(func_name='get_author_detailed_info', args={}, result=
{'container_type': 'Author', 'filled': ['basics', 'indices', 'counts', 
'coauthors', 'publications', 'public_access'], 'scholar_id': 'JicYPdAAAAAJ', 
'source': <AuthorSource.AUTHOR_PROFILE_PAGE: 'AUTHOR_PROFILE_PAGE'>, 'name': 
'Geoffrey Hinton', 'url_picture': 'https://scholar.googleusercontent.com/
citations?view_op=view_photo&user=JicYPdAAAAAJ&citpid=2', 'affiliation': 
'Emeritus Prof. Computer Science, University of Toronto', 'organization': 
8515235176732148308, 'interests': ['machine learning', 'psychology', 
'artificial intelligence', 'cognitive science', 'computer science'], 
'email_domain': '@cs.toronto.edu', 'homepage': 'http://www.cs.toronto.edu/
~hinton', 'citedby': 853541, 'citedby5y': 560063, 'hindex': 186, 'hindex5y': 
137, 'i10index': 483, 'i10index5y': 368, 'cites_per_year': {1989: 2627, 1990: 
3589, 1991: 3766, 1992: 4091, 1993: 4573, 1994: 4499, 1995: 4090, 1996: 3935, 
1997: 3740, 1998: 3744, 1999: 3559, 2000: 3292, 2001: 3398, 2002: 3713, 2003: 
3670, 2004: 3393, 2005: 3813, 2006: 4168, 2007: 4558, 2008: 4349, 2009: 4784, 
2010: 5238, 2011: 5722, 2012: 6746, 2013: 9900, 2014: 12751, 2015: 18999, 
2016: 29932, 2017: 43675, 2018: 63544, 2019: 80800, 2020: 90523, 2021: 101735, 
2022: 104036, 2023: 106452, 2024: 76413}, 'coauthors': [{'container_type': 
'Author', 'filled': [], 'scholar_id': 'm1qAiOUAAAAJ', 'source': <AuthorSource.
CO_AUTHORS_LIST: 'CO_AUTHORS_LIST'>, 'name': 'Terrence Sejnowski', 
'affiliation': 'Francis Crick Professor, Salk Institute, Distinguished 
Professor, UC San Diego'}, {'container_type': 'Author', 'filled': [], 
'scholar_id': 'RnoIxUwAAAAJ', 'source': <AuthorSource.CO_AUTHORS_LIST: 
'CO_AUTHORS_LIST'>, 'name': 'Vinod Nair', 'affiliation': 'Research Scientist, 
DeepMind'}, {'container_type': 'Author', 'filled': [], 'scholar_id': 
'ghbWy-0AAAAJ', 'source': <AuthorSource.CO_AUTHORS_LIST: 'CO_AUTHORS_LIST'>, 
'name': 'George E. Dahl', 'affiliation': 'Google Inc.'}, {'container_
===============================================================================
"""

# Define a user message
usr_msg = "get the publications of this author"

# Get response information
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])
"""
===============================================================================
[ToolCallingRecord(func_name='get_author_publications', args={}, result=
['Imagenet classification with deep convolutional neural networks', 'Deep 
learning', 'Learning internal representations by error-propagation', 'Dropout: 
a simple way to prevent neural networks from overfitting', 'Visualizing data 
using t-SNE', 'Learning representations by back-propagating errors', 'Learning 
multiple layers of features from tiny images', 'Rectified linear units improve 
restricted boltzmann machines', 'Reducing the dimensionality of data with 
neural networks', 'A fast learning algorithm for deep belief nets', 
'Distilling the Knowledge in a Neural Network', 'A simple framework for 
contrastive learning of visual representations', 'Deep neural networks for 
acoustic modeling in speech recognition: The shared views of four research 
groups', 'Layer normalization', 'Speech recognition with deep recurrent neural 
networks', 'Improving neural networks by preventing co-adaptation of feature 
detectors', 'Lec
===============================================================================
"""

# ruff: noqa: E501
# Define a user message

usr_msg = """get the detailed information for publication with title: `Camel: Communicative agents for" mind" exploration of large language model society`"""

# Get response information
response = camel_agent.step(usr_msg)
print(response.info['tool_calls'])
"""
===============================================================================
[ToolCallingRecord(func_name='get_publication_by_title', args=
{'publication_title': 'Camel: Communicative agents for" mind" exploration of 
large language model society'}, result={'container_type': 'Publication', 
'source': <PublicationSource.AUTHOR_PUBLICATION_ENTRY: 
'AUTHOR_PUBLICATION_ENTRY'>, 'bib': {'title': 'Camel: Communicative agents 
for" mind" exploration of large language model society', 'pub_year': 2023, 
'citation': 'Advances in Neural Information Processing Systems 36, 2023', 
'author': 'Guohao Li and Hasan Hammoud and Hani Itani and Dmitrii Khizbullin 
and Bernard Ghanem', 'journal': 'Advances in Neural Information Processing 
Systems', 'volume': '36', 'abstract': 'The rapid advancement of chat-based 
language models has led to remarkable progress in complex task-solving. 
However, their success heavily relies on human input to guide the 
conversation, which can be challenging and time-consuming. This paper explores 
the potential of building scalable techniques to facilitate autonomous 
cooperation among communicative agents, and provides insight into their 
"cognitive" processes. To address the challenges of achieving autonomous 
cooperation, we propose a novel communicative agent framework named 
role-playing. Our approach involves using inception prompting to guide chat 
agents toward task completion while maintaining consistency with human 
intentions. We showcase how role-playing can be used to generate 
conversational data for studying the behaviors and capabilities of a society 
of agents, providing a valuable resource for investigating conversational 
language models. In particular, we conduct comprehensive studies on 
instruction-following cooperation in multi-agent settings. Our contributions 
include introducing a novel communicative agent framework, offering a scalable 
approach for studying the cooperative behaviors and capabilities of 
multi-agent systems, and open-sourcing our library to support research on 
communicative agents and beyond: https://github. com/camel-ai/camel.'}, 
'filled': True, 'author_pub_id': 'J9K-D0sAAAAJ:_Qo2XoVZTnwC', 'num_citations': 
364, 'citedby_url': '/scholar?hl=en&cites=3976259482297250805', 'cites_id': 
['3976259482297250805'], 'pub_url': 'https://proceedings.neurips.cc/
paper_files/paper/2023/hash/
a3621ee907def47c1b952ade25c67698-Abstract-Conference.html', 
'url_related_articles': '/scholar?oi=bibs&hl=en&q=related:9TMbme6CLjcJ:scholar.
google.com/', 'cites_per_year': {2023: 95, 2024: 269}})]
===============================================================================
"""

usr_msg = """get the full information for paper from link: `https://hal.science/hal-04206682/document`"""

# Get response information
response = camel_agent.step(usr_msg)
print((response.info['tool_calls'])[:1000])
"""
===============================================================================
[ToolCallingRecord(func_name='get_full_paper_content_by_link', args=
{'pdf_url': 'https://hal.science/hal-04206682/document'}, result='Deep 
learning\nYann Lecun, Yoshua Bengio, Geoffrey Hinton\n\nTo cite this 
version:\n\nYann Lecun, Yoshua Bengio, Geoffrey Hinton. Deep learning. Nature, 
2015, 521 (7553), pp.436-444.\n\uffff10.1038/nature14539\uffff. 
\uffffhal-04206682\uffff\n\nHAL Id: hal-04206682\n\nhttps://hal.science/
hal-04206682v1\n\nSubmitted on 14 Sep 2023\n\nHAL is a multi-disciplinary open 
access\narchive for the deposit and dissemination of sci-\nentific research 
documents, whether they are pub-\nlished or not. The documents may come 
from\nteaching and research institutions in France or\nabroad, or from public 
or private research centers.\n\nL'archive ouverte pluridisciplinaire HAL, 
est\ndestinée au dépôt et à la diffusion de documents\nscientifiques de niveau 
recherche, publiés ou non,\némanant des établissements d'enseignement et 
de\nrecherche français ou étrangers, des laboratoires\npublics ou privés.
\n\n\x0cDeep learning\n\nYann LeCun1,2, Yoshua Bengio3 & Geoffrey Hinton4,
5\n\n1Facebook AI Research, 770 Broadway, New York, New York 10003 USA\n\n2N..
===============================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\human_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========


from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import HumanToolkit
from camel.types import ModelPlatformType, ModelType

human_toolkit = HumanToolkit()

model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=ChatGPTConfig(
        temperature=0.0,
    ).as_dict(),
)

# Example 1: Test Agent with the human toolkit (ask_human_via_console)
print("\nExample 1: Using ask_human_via_console through an agent")
agent = ChatAgent(
    system_message="You are a helpful assistant.",
    model=model,
    tools=[*human_toolkit.get_tools()],
)

response = agent.step(
    "Test me on the capital of some country, and comment on my answer."
)

print(response.msgs[0].content)

"""
==========================================================================
What is the capital of France?
Your reply: Paris

That's correct! Paris is indeed the capital of France. Would you like to try
another one?
Your reply: yes

What is the capital of Japan?
Your reply: Tokyo

That's correct! Tokyo is the capital of Japan. Would you like to continue with
another question?
Your reply: no
==========================================================================
"""

# Example 2: Agent using send_message_to_user through tools
print("\nExample 2: Agent using send_message_to_user through tools")
agent_with_message = ChatAgent(
    system_message="You are an assistant that can send messages to the user.",
    model=model,
    tools=[*human_toolkit.get_tools()],
)

response = agent_with_message.step(
    "Send me a notification about an upcoming meeting."
)

print(response.msgs[0].content)

"""
==========================================================================
Agent Message:
🔔 Reminder: You have an upcoming meeting scheduled. Please check your 
calendar for details!

I've sent you a notification about your upcoming meeting. Please check your 
calendar for details!
==========================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\image_analysis_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.messages.base import BaseMessage
from camel.models import ModelFactory
from camel.toolkits import ImageAnalysisToolkit
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
)

image_analysis_toolkit = ImageAnalysisToolkit(model=model)

agent = ChatAgent(
    system_message="You are a helpful assistant.",
    model=model,
    tools=[*image_analysis_toolkit.get_tools()],
)


user_msg = BaseMessage.make_user_message(
    role_name="User",
    content='''
        The image link is: https://upload.wikimedia.org/wikipedia/commons/
        thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/
        2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg
        What's in this image? You must use image analysis to help me.
        ''',
)
response = agent.step(user_msg)
print(response.msgs[0].content)
""""
===========================================================================
The image depicts a serene landscape featuring a wooden boardwalk that leads 
through a lush, green marsh or meadow. The boardwalk is centrally positioned, 
extending into the distance and inviting viewers to imagine walking along it. 
On either side of the boardwalk, tall grass and various vegetation create a 
vibrant green expanse.

In the background, there are clusters of trees and shrubs, adding depth to the 
scene. The sky above is mostly clear with a few scattered clouds, showcasing a 
gradient of blue hues. The overall atmosphere is tranquil and natural, 
suggesting a peaceful outdoor setting, with soft lighting that likely 
indicates early morning or late afternoon."
============================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\mcp\mcp_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
"""MCP Server Example

This example demonstrates how to use the MCP (Managed Code Processing) server
with CAMEL agents for file operations.

Setup:
1. Install Node.js and npm

2. Install MCP filesystem server globally:
   ```bash
   npm install -g @modelcontextprotocol/server-filesystem
   ```

Usage:
1. Run this script to start an MCP filesystem server
2. The server will only operate within the specified directory
3. All paths in responses will be relative to maintain privacy
"""

import asyncio
from pathlib import Path

from camel.agents import ChatAgent
from camel.models import ModelFactory
from camel.toolkits import MCPToolkit
from camel.types import ModelPlatformType, ModelType


async def main():
    config_path = Path(__file__).parent / "mcp_servers_config.json"
    mcp_toolkit = MCPToolkit(config_path=str(config_path))

    # Connect to all MCP servers.
    await mcp_toolkit.connect()

    sys_msg = "You are a helpful assistant"
    model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
    )
    camel_agent = ChatAgent(
        system_message=sys_msg,
        model=model,
        tools=[*mcp_toolkit.get_tools()],
    )
    user_msg = "List 5 files in the project, using relative paths"
    response = await camel_agent.astep(user_msg)
    print(response.msgs[0].content)
    print(response.info['tool_calls'])

    # Disconnect from all MCP servers and clean up resources.
    await mcp_toolkit.disconnect()


if __name__ == "__main__":
    asyncio.run(main())
'''
===============================================================================
Here are 5 files in the project using relative paths:

1. `.env`
2. `.gitignore`
3. `.pre-commit-config.yaml`
4. `CONTRIBUTING.md`
5. `README.md`
===============================================================================
'''



--------------------------------------------------------------------------------
# File: toolkits\memory_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits.memory_toolkit import MemoryToolkit
from camel.types import ModelPlatformType, ModelType


def run_memory_toolkit_example():
    """
    Demonstrates a ChatAgent using the MemoryToolkit for
    function calling to manage memory.
    """

    # Create a Model
    model_config_dict = ChatGPTConfig(temperature=0.0).as_dict()
    model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
        model_config_dict=model_config_dict,
    )

    # Create a ChatAgent
    agent = ChatAgent(
        system_message="""You are an assistant that can manage 
        conversation memory using tools.""",
        model=model,
    )

    # Add MemoryToolkit to the Agent
    memory_toolkit = MemoryToolkit(agent=agent)
    for tool in memory_toolkit.get_tools():
        agent.add_tool(tool)

    # Have a conversation to populate memory
    print("\n--- Starting a Conversation ---")
    user_msg_1 = "Tell me about the moon."
    print(f"[User] {user_msg_1}")
    response_1 = agent.step(user_msg_1)
    print(f"[Agent] {response_1.msgs[0].content}")

    # Save the memory to a file via function calling
    print("\n--- Saving Memory ---")
    save_msg = "Please save the current memory to 'conversation_memory.json'."
    response_save = agent.step(save_msg)
    print(f"[Agent] {response_save.msgs[0].content}")
    print(f"[Tool Call Info] {response_save.info['tool_calls']}")

    # Clear the memory via function calling
    print("\n--- Clearing Memory ---")
    clear_msg = "Please clear the memory."
    response_clear = agent.step(clear_msg)
    print(f"[Agent] {response_clear.msgs[0].content}")
    print(f"[Tool Call Info] {response_clear.info['tool_calls']}")

    # Verify memory is cleared
    print("\n--- Checking Memory After Clear ---")
    check_msg = "What do you remember about the moon?"
    response_check = agent.step(check_msg)
    print(f"[Agent] {response_check.msgs[0].content}")

    # Load memory from the saved file via function calling
    print("\n--- Loading Memory from File ---")
    load_msg = "Please load the memory from 'conversation_memory.json'."
    response_load = agent.step(load_msg)
    print(f"[Agent] {response_load.msgs[0].content}")
    print(f"[Tool Call Info] {response_load.info['tool_calls']}")

    # Verify memory is restored
    print("\n--- Checking Memory After Load ---")
    check_msg = "What do you remember about the moon?"
    response_restored = agent.step(check_msg)
    print(f"[Agent] {response_restored.msgs[0].content}")


if __name__ == "__main__":
    run_memory_toolkit_example()



--------------------------------------------------------------------------------
# File: toolkits\meshy_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.toolkits import MeshyToolkit

toolkit = MeshyToolkit()

# Example data for testing
prompt = "A figuring of Tin tin the cartoon character"
art_style = "realistic"
negative_prompt = "low quality, low resolution, low poly, ugly"

# 3D model generation
print("Starting 3D model generation...")
final_response = toolkit.generate_3d_model_complete(
    prompt=prompt, art_style=art_style, negative_prompt=negative_prompt
)
print("\nFinal Response:", final_response)
# ruff: noqa: E501
"""
==========================================================================
Starting 3D model generation...
Status after 0s: PENDING
Status after 11s: IN_PROGRESS
Status after 22s: IN_PROGRESS
Status after 32s: SUCCEEDED
Status after 0s: PENDING
Status after 11s: IN_PROGRESS
Status after 21s: IN_PROGRESS
Status after 32s: IN_PROGRESS
Status after 43s: IN_PROGRESS
Status after 53s: IN_PROGRESS
Status after 64s: IN_PROGRESS
Status after 74s: IN_PROGRESS
Status after 85s: IN_PROGRESS
Status after 95s: IN_PROGRESS
Status after 106s: IN_PROGRESS
Status after 117s: SUCCEEDED

Final Response: {'id': '01939144-7dea-73c7-af06-efa79c83243f', 'mode': 
'refine', 'name': '', 'seed': 1733308970, 'art_style': 'realistic', 
'texture_richness': 'high', 'prompt': 'A figuring of Tin tin the cartoon 
character', 'negative_prompt': 'low quality, low resolution, low poly, ugly', 
'status': 'SUCCEEDED', 'created_at': 1733309005313, 'progress': 100, 
'started_at': 1733309006267, 'finished_at': 1733309113474, 'task_error': None, 
'model_urls': {'glb': 'https://assets.meshy.ai/5e05026a-0e91-4073-83fe-0263b1b4d348/tasks/01939144-7dea-73c7-af06-efa79c83243f/output/model.glb?Expires=4886870400&Signature=TEbWpN8sFZOf1FKWBVxKNdT2Ltm1Ma6vHuUUpBh6rZaAzfTBQPKvV2i7RmD~wwaebbQSBvVVagF4j587tNKNwHPqkGtpBjBu2q43n4lWM6W--RxSqbOCvVZ54PiAzzlVjM9PzPz-MasrWQtYipm5qJ5tsWd7XoxB6Wv2tZMZEWsftdLxmXdp9SuoBcu5NM~MRnyvhEYPmwU9uCAKfh4FZ14mhfx6TeDpCprYh1ngnlkLzDXk5Mdw0HJ1zuYpnkCOUtth84p6Oq5aU0HtWtUVd2tLi53rqKn9QC0qxcH7QlPxxI1aoUtzqaMXXiqCGylzZuPTZILhdFWeAoiEtCOLZw__&Key-Pair-Id=KL5I0C8H7HX83', 'fbx': 'https://assets.meshy.ai/5e05026a-0e91-4073-83fe-0263b1b4d348/tasks/01939144-7dea-73c7-af06-efa79c83243f/output/model.fbx?Expires=4886870400&Signature=jGOPhF8FL1wa9mVbodNoq1jMVzi2gklWRrrl2qWAZvWAhadc4wgjmgTweBKsNiO~KMTGzCiey7iqSIGm6dDEYAMv72HExpIO7I8HwAVPp4KwhzORzwr6OcEoY9-7fFR9yEg~WqnWewmdkrcnUVHHx2Z9imkDkIhISn1IOERkum48mTemlyejb87CXGV14uX3hxIVKle4at6S8tMUfpXhCdZ3aIxmgt9Dpsobol92XtQbpC-JhJSOgMNBWtAH3OUXAgECsYrRRZND9gcZZkUZlXHHZt439JsU8MPoXZd4RQ0OGn~vb6W51rvQ904ErsYZf47dLYNswaxb6Se3oKm~zw__&Key-Pair-Id=KL5I0C8H7HX83', 'usdz': 'https://assets.meshy.ai/5e05026a-0e91-4073-83fe-0263b1b4d348/tasks/01939144-7dea-73c7-af06-efa79c83243f/output/model.usdz?Expires=4886870400&Signature=ICOOIH6EDVdy9LYCk-azYqBWtl6t9v2xZpRk8C8kQKa38jUXdukjoLLN469VP5a7rdIKinLA~I5-hMr-kd-MEmwJzE3JKw2ojNimYPa5Uvnr3R~4S~2fQgCPWfn2xVkt6Cvfx~Qj8~ZNVxMj0jvnKkIySRHYaqvCxMfASHCB7Kz9LN3lBWuT709pEnQ6mtwLJWybLlIJkMFOVoapw~epIgWBtJjhMNwPCzXswUddKSdirOHIm8JRoN3~Ha99oxo4nSN5tyf3u2fWLxGOTeAyp7Hcq97gMkdqjuNc14k2n7fPULgbSCkHepLIG8GQrNLMfA6hkphkIj0LdjC6AQ7pvg__&Key-Pair-Id=KL5I0C8H7HX83', 'obj': 'https://assets.meshy.ai/5e05026a-0e91-4073-83fe-0263b1b4d348/tasks/01939144-7dea-73c7-af06-efa79c83243f/output/model.obj?Expires=4886870400&Signature=a53mEQASL7jRU8Xz5WhN-~d3~74BlBlqDMufryX-j1~jXTgbMEEhY2dC5R7dHHHJbJ4ns9GQ8cbjxcCImVvjkiLvPYZ-lraLaqMnbG~hatsZNv6wDZxTson8jsiqTSHaLnamp83zycLotM~zrUW0DIHGoFWvf9DPTKqy4Z0ZAOxOsA9qfAmJI6k2CVHLu0hMRLAjm3f8KA4j90kJBBVuYvABZi27hP-aURhD09zoAMp~AsrXSKxFjd5wcYqKko78qch2K2H5NaAUGhsKbuNmBMFaxc0C5dKgSlKufWmib86vUOe1dYLQyqGTS85u5dVQSwFrDY5gyugGJ4TH-aVQVw__&Key-Pair-Id=KL5I0C8H7HX83', 'mtl': 'https://assets.meshy.ai/5e05026a-0e91-4073-83fe-0263b1b4d348/tasks/01939144-7dea-73c7-af06-efa79c83243f/output/model.mtl?Expires=4886870400&Signature=FnY3cNMqEymvBw~33riU~HkVIifWKEUh0ndV90VaWMnKczU~Wqng7AYTqwywr6PNQDuFL~iQFw-y6qvklsV9I0aTg8OoYQ3dfCaRqydwUbN80aonk~fwpAJUwBxqbhhN4n9T~7WTX-pyo0w5vQ09wte4G-4yAIUEM7qlOwZohdfK2a~EIhnq9WiV92TuGtm0c4x5n6png9ZjX5pHnp~a77UCBJlIQ1teN5Rb3I9HFh4sbUGdcXUas7B9EIq4YiabjO9vf5FGwicb2XQ-YxJFJJdEJwbBp6l6iZCbSk-WijmIWmyD~8A~jhTNwlG9UHR5qTsnprntgoRyLdTRSXvDzg__&Key-Pair-Id=KL5I0C8H7HX83'}, 
'thumbnail_url': 'https://assets.meshy.ai/5e05026a-0e91-4073-83fe-0263b1b4d348/tasks/01939144-7dea-73c7-af06-efa79c83243f/output/preview.png?Expires=4886870400&Signature=B16evi199mig4RTFq0FVPrHGkpjpPudRpLfpcY2FxJIkIFYg42-v8BfsL3eWAM-XDlDqahPSXtqqm6emVkSu550iPqo2yy-URoDifiIl5petEy~42nHtc1-dZB1HcEvtcyycHOjmk1y8zQfZBgQ8cjGq0Ds19xSdOXIo7-~QDPWhUGUTreJvBNg17GitgvcfYbGj2g6gibYJWjwatM7A6yHhq3d53N8eDcmO5L6dBH3VwUFTxDWBQXwUT7aXkS7dsQ7Wz5CkIbbH~T-4Pn5KpdJy1Kf1Lrh1YpOUN4T7JI8Ot5urYKYRj4cZ96xpDD9gicPGvgrRaicFyb1sSwW2ow__&Key-Pair-Id=KL5I0C8H7HX83', 
'video_url': 'https://assets.meshy.ai/5e05026a-0e91-4073-83fe-0263b1b4d348/tasks/01939144-7dea-73c7-af06-efa79c83243f/output/output.mp4?Expires=4886870400&Signature=r8t11N9~XGzNfuW3KowxarSpr7hC8eQb0kzkKADOz3hdTA9vAqBdv5AVdMGDlMmH9IP4R60UCnVNT6scA1EeN3FZLXaHaWbsxHDuc4XdUk7DE7AbwUKSbl~YdUSu5-RkNu6vaMHTiB55XubUJhv9ReB25a6Ifee0rf1ulGs-amFSMlL~eNPq6HTUI6NGAqi1p~VeFzE53JV5sWvU2JYnbGe8kzruC705z1LiCU-9isWzJGuOIy~RpiVfYzSmgh4xeILaYKpxR2ZM2uVtbi6snl~aYsqiKMIIMxMg-aZDWn-f5voiWaCL1OUV5fxbI82ZRJNd5DSlVjI~umqZZIl-iw__&Key-Pair-Id=KL5I0C8H7HX83', 
'texture_urls': [{'base_color': 'https://assets.meshy.ai/5e05026a-0e91-4073-83fe-0263b1b4d348/tasks/01939144-7dea-73c7-af06-efa79c83243f/output/texture_0.png?Expires=4886870400&Signature=Q8SGRrnE00-mGHCAcIOUUAig~YtTJqVx1n2IqFFbXBNUPvf~hsTYzcKgC2wQjF25tj0D6yQ8BiIktN9WjsKu0SnbeED~ofHIA0quheMjwHL~hfdj63LGWkMumVEjE2ZVwDv-DdlROF3ayw5hQxzlRbcHwXLq0n2xMHmj-WetyiYBKCcJbXbZMOAtlo8e40d21CGMnjImduCvdwhpqwNKUx4MwHeM2W0GW4OC94AoSF8AccHJeQPD2gdu7JHoTuZFjcqS-9YCjmHT7Y5Xg7rmeNYz40O21sYci0b54NvBDzX-6HvydjqtY-ofudppaxlC77Zd~FaVcCz5rH2J43cdLg__&Key-Pair-Id=KL5I0C8H7HX83'}]}
(camel-ai-py3.12) 
==========================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\mineru_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import MinerUToolkit
from camel.types import ModelPlatformType, ModelType


def main():
    # Initialize the toolkit
    mineru_toolkit = MinerUToolkit()

    print("Example 1: Extracting content from a single URL...")
    try:
        # Extract and wait for results
        result = mineru_toolkit.extract_from_urls(
            urls="https://arxiv.org/pdf/2311.10993.pdf"
        )
        print("\nExtraction completed successfully:")
        print(f"Download URL: {result['full_zip_url']}\n")

    except Exception as e:
        print(f"Extraction failed: {e}\n")

    print("Example 2: Extracting content from multiple URLs...")
    try:
        urls = [
            "https://arxiv.org/pdf/2311.10993.pdf",
            "https://arxiv.org/pdf/2310.07298.pdf",
        ]

        # Batch extract and wait for results
        results = mineru_toolkit.extract_from_urls(urls=urls)

        print("\nBatch extraction completed successfully:")
        for result in results['extract_result']:
            print(f"\nDocument: {result['file_name']}")
            print(f"Download URL: {result['full_zip_url']}")

    except Exception as e:
        print(f"Batch extraction failed: {e}\n")

    print("\nExample 3: Using MinerU with ChatAgent...")
    # TODO: implement this example with loader toolkit to get information from
    # the zip url
    try:
        # Set up the ChatAgent with MinerU capabilities
        sys_msg = (
            "You are a helpful assistant that can extract and analyze "
            "content from documents using MinerU's document extraction. "
            "You can handle PDFs and extract text, formulas, and tables. When "
            "processing documents, inform users that it may take time."
        )

        # Initialize the model with specific configuration
        model = ModelFactory.create(
            model_platform=ModelPlatformType.DEFAULT,
            model_type=ModelType.DEFAULT,
            model_config_dict=ChatGPTConfig(temperature=0.0).as_dict(),
        )

        # Create the agent with MinerU toolkit
        agent = ChatAgent(
            system_message=sys_msg,
            model=model,
            tools=mineru_toolkit.get_tools(),
        )

        # Example document analysis request
        usr_msg = (
            "Please extract and analyze this research paper,"
            "focusing on mathematical formulas and tables: "
            "https://arxiv.org/pdf/2311.10993.pdf "
        )

        response = agent.step(usr_msg)
        print("\nAgent Response:")
        print(response.msg.content)

    except Exception as e:
        print(f"\nAgent interaction failed: {e}\n")


if __name__ == "__main__":
    main()

"""
Example output:

Example 1: Extracting content from a single URL...

Extraction completed successfully:
Download URL: https://cdn-mineru.openxlab.org.cn/pdf/690a7956-eaaa-4fb2-ad7d-6056d1d4e316.zip

Example 2: Extracting content from multiple URLs...

Batch extraction completed successfully:

Document: 2311.10993.pdf
Download URL: https://cdn-mineru.openxlab.org.cn/pdf/690a7956-eaaa-4fb2-ad7d-6056d1d4e316.zip

Document: 2310.07298.pdf
Download URL: https://cdn-mineru.openxlab.org.cn/pdf/250a3762-406e-4279-aa80-47e5ea934509.zip

Example 3: Using MinerU with ChatAgent...

Agent Response:
The extraction of the research paper has been completed. You can download the
extracted content, including mathematical formulas and tables,
from the following link:

[Download Extracted Content]
(https://cdn-mineru.openxlab.org.cn/pdf/690a7956-eaaa-4fb2-ad7d-6056d1d4e316.zip)

If you need any specific analysis or further assistance with the content,
please let me know!

"""



--------------------------------------------------------------------------------
# File: toolkits\networkx_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs.openai_config import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import NetworkXToolkit
from camel.types import ModelPlatformType, ModelType

# Define system message
sys_msg = "You are a helpful assistant for graph analysis"

# Set model config and initialize toolkit
tools = NetworkXToolkit(graph_type='digraph').get_tools()
model_config_dict = ChatGPTConfig(
    temperature=0.0,
).as_dict()

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=model_config_dict,
)

# Set agent
camel_agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)

# Example: Create a directed graph and perform some analysis
usr_msg = """Create a directed graph with the following edges:
- A -> B (weight: 2)
- B -> C (weight: 3)
- C -> A (weight: 1)
Then find the shortest path from A to C and calculate the graph density."""

# Get response information
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls']))

'''
===============================================================================
[ToolCallingRecord(tool_name='add_edge', args={'source': 'A', 'target': 'B'}, 
result=None, tool_call_id='call_iewKMXQd2GKwKWy7XJ5e5d8e'), ToolCallingRecord
(tool_name='add_edge', args={'source': 'A', 'target': 'B'}, result=None, 
tool_call_id='call_Xn8wq22oKeKekuPEqcSj5HuJ'), ToolCallingRecord
(tool_name='add_edge', args={'source': 'B', 'target': 'C'}, result=None, 
tool_call_id='call_bPeCvUBk1iQ6vv5060Zd7nbi'), ToolCallingRecord
(tool_name='add_edge', args={'source': 'C', 'target': 'A'}, result=None, 
tool_call_id='call_inCnY60iSBVghsrrHEDh7hNw'), ToolCallingRecord
(tool_name='get_shortest_path', args={'source': 'A', 'target': 'C', 'weight': 
'weight', 'method': 'dijkstra'}, result=['A', 'B', 'C'], 
tool_call_id='call_Gwy3Ca8RDQCZFuiy2h0Z6SSF'), ToolCallingRecord
(tool_name='get_edges', args={}, result=[('A', 'B'), ('B', 'C'), ('C', 'A')], 
tool_call_id='call_LU2xhb2W4h5a6LOx4U8gLuxa'), ToolCallingRecord
(tool_name='get_nodes', args={}, result=['A', 'B', 'C'], 
tool_call_id='call_WLuB1nBrhFeGj4FKrbwfnCrG')]
===============================================================================
'''



--------------------------------------------------------------------------------
# File: toolkits\notion_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs.openai_config import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import NotionToolkit
from camel.types import ModelPlatformType, ModelType

# Define system message
sys_msg = "You are a helpful assistant"

# Set model config
tools = NotionToolkit().get_tools()
model_config_dict = ChatGPTConfig(
    temperature=0.0,
).as_dict()

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=model_config_dict,
)

# Set agent
camel_agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)
camel_agent.reset()

# Define a user message
usr_msg = "Lists all pages in the Notion workspace"

# Get response information
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])
"""
==========================================================================
[ToolCallingRecord(func_name='list_all_pages', args={}, result=[{'id': 
'12684f56-4caa-8080-be91-d7fb1a5834e3', 'title': 'test page'}, 
{'id': '47a4fb54-e34b-4b45-9928-aa2802982eb8', 'title': 'Aigentbot'}])]
"""

usr_msg = "Retrieves the text content of a Notion block which id is"
"'12684f56-4caa-8080-be91-d7fb1a5834e3'"

# Get response information
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])
"""
==========================================================================
[ToolCallingRecord(func_name='get_notion_block_text_content', args=
{'block_id': '12684f56-4caa-8080-be91-d7fb1a5834e3'}, result='hellonihao 
buhao this is a test par [Needs case added] another par [Needs case added]
A cute cat: https://www.google.com/imgres?q=cat&imgurl=https%3A%2F%2Fi.
natgeofe.com%2Fn%2F548467d8-c5f1-4551-9f58-6817a8d2c45e%2FNationalGeographic
_2572187_square.jpg&imgrefurl=https%3A%2F%2Fwww.nationalgeographic.com%2F
animals%2Fmammals%2Ffacts%2Fdomestic-cat&docid=K6Qd9XWnQFQCoM&tbnid=eAP24
4UcF5wdYM&vet=12ahUKEwir9rf3oKGJAxVsFTQIHYsrMYkQM3oECBkQAA..i&w=3072&h=307
2&hcb=2&ved=2ahUKEwir9rf3oKGJAxVsFTQIHYsrMYkQM3oECBkQAA')]
"""

usr_msg = "List names of users via the Notion integration"

# Get response information
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])
"""
==========================================================================
[ToolCallingRecord(func_name='list_all_users', args={}, result=[{'type':
'person', 'name': 'user a', 'workspace': ''}, {'type': 'bot', 'name':
'test', 'workspace': "user a's Notion"}])]
==========================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\openai_agent_toolkit_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import os

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import OpenAIAgentToolkit
from camel.types import ModelPlatformType, ModelType

# Define system message
sys_msg = """You are a helpful AI assistant that can use OpenAI's agent tools 
including web search and file search."""

# Set model config
model_config_dict = ChatGPTConfig(
    temperature=0.0,
).as_dict()

model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=model_config_dict,
)

# Initialize toolkit and get tools
toolkit = OpenAIAgentToolkit(model=model)
tools = toolkit.get_tools()

# Set agent
agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)

# Example 1: Web Search
print("\n=== Using Web Search with Agent ===")
response = agent.step(
    "What was a positive news story from today? Use web search to find out."
)
print("Web Search Response:", response.msg.content)

# Example 2: Direct Web Search
print("\n=== Direct Web Search Usage ===")
web_result = toolkit.web_search(
    "What are the latest developments in renewable energy?"
)
print("Direct Web Search Result:", web_result)

# Example 3: File Search (if configured)
vector_store_id = os.getenv("OPENAI_VECTOR_STORE_ID")

if vector_store_id:
    print("\n=== Using File Search with Agent ===")
    response = agent.step(
        f"Search through my documents for information about climate change. "
        f"Use file search with vector store ID: {vector_store_id}"
    )
    print("File Search Response:", response.msg.content)

    print("\n=== Direct File Search Usage ===")
    file_result = toolkit.file_search(
        query="What are the key points about climate change?",
        vector_store_id=vector_store_id,
    )
    print("Direct File Search Result:", file_result)
else:
    print("\n=== File Search Examples Skipped ===")
    print("Set OPENAI_VECTOR_STORE_ID env var to run file search examples")


"""
=== Using Web Search with Agent ===
Web Search Response: Here are some uplifting news stories from today:

1. **Historic Marriage Milestone**: A Brazilian couple has set a new Guinness
World Record for the longest marriage of a living couple, celebrating 84 
years and 85 days together.

2. **Lottery Jackpot Win**: A Michigan man who consistently played the same 
lottery numbers for four years has won a $1.3 million jackpot in the state's 
Lotto 47 game.

3. **Miraculous Recovery**: A 23-year-old British woman, initially given only 
a 5% chance of survival after a life-threatening skiing accident, shared her 
inspiring story of recovery.

4. **Marathon for a Cause**: A Wisconsin doctor, who previously battled 
testicular cancer, ran seven marathons on seven continents—from 
Antarctica to North America—to raise awareness for the disease.

5. **Kindness in the Skies**: An airline passenger shared a story about 
a fellow flyer who offered $100 to switch their window seat for an aisle
seat, highlighting unexpected acts of kindness.

6. **Community Support**: A North Carolina mother is honoring the victims of 
an American Airlines flight that collided with an Army helicopter by 
supporting the skating community, demonstrating resilience and unity.

7. **Positive Outlook During Pandemic**: A study from Oregon State University 
confirms that individuals with positive and playful attitudes navigated the 
COVID-19 pandemic more effectively, underscoring the power of optimism.

8. **Random Act of Kindness**: An emergency room doctor and father of three in 
Fort Worth, Texas, was touched when a stranger left a heartfelt note and paid 
his family's breakfast bill at a local café.

9. **Mountain Rescue**: Two experienced hikers were rescued from the tallest 
mountain in the Northeast after a whiteout snowstorm stranded them at about 
5,000 feet, showcasing the dedication of rescue teams.

10. **Pilot's Reassurance**: An American Airlines pilot went viral after 
assuring passengers that he has "no higher calling" than "carefully" 
transporting them to their destination, following a recent tragic 
crash in D.C.

These stories highlight the resilience, kindness, and positive spirit 
present in our communities.

=== Direct Web Search Usage ===
Direct Web Search Result: Recent developments in renewable energy have marked 
significant progress across various sectors:

**Record Growth in Renewable Energy Capacity**

In 2023, the global renewable energy sector experienced unprecedented growth, 
adding 473 gigawatts (GW) new capacity—a 54% increase from the previous year. 
This surge was predominantly driven by solar photovoltaic and onshore wind 
installations, which together accounted for over 95% of the new capacity. 
Notably, solar PV alone contributed 346 GW, reflecting a 73% year-on-year 
increase. China played a pivotal role in this expansion, leading in solar PV, 
onshore wind, offshore wind, and hydropower installations. This dominance has 
contributed to a decline in the global weighted average cost of electricity 
(LCOE) for these technologies, making renewable power generation increasingly 
competitive. ([greenearth.news](https://greenearth.news/record-growth-in-
renewable-energy-473-gw-added-in-2023-marks-a-turning-point-in-global-power-
transition-report/?utm_source=openai))

**Advancements in Energy Storage Technologies**

The integration of renewable energy sources has been bolstered by significant 
advancements in energy storage solutions. In 2023, battery storage capacity 
expanded dramatically, with 95.9 GWh added, up from just 0.1 GWh in 2010. This 
growth has been accompanied by substantial reduction in costs, with price of 
battery storage projects decreasing by 89% between 2010 and 2023. These 
developments enhance reliability and scalability of renewable energy systems, 
addressing the intermittent nature of sources like wind and solar. 
([greenearth.news](https://greenearth.news/record-growth-in-renewable-energy-
473-gw-added-in-2023-marks-a-turning-point-in-global-power-transition-report/?
utm_source=openai))

**Innovations in Renewable Energy Technologies**

The renewable energy sector has witnessed several technological breakthroughs:

- **Solar Power**: The introduction of high-efficiency solar panels utilizing 
materials such as perovskite and gallium arsenide has significantly increased 
energy conversion rates. Additionally, building-integrated photovoltaics
are enabling the seamless integration of solar technology into architectural 
designs, transforming buildings into self-sustaining power generators. 
([toxigon.com](https://toxigon.com/renewable-energy-technologies-2023?
utm_source=openai))

- **Wind Energy**: Advancements in wind turbine technology, including the 
deployment of larger and more efficient turbines, have enhanced energy capture 
Offshore wind farms, particularly those utilizing floating wind turbines, are 
expanding the potential for wind power generation in deeper waters previously 
inaccessible to traditional fixed-bottom turbines. ([toxigon.com]
(https://toxigon.com/renewable-energy-technologies-2023?utm_source=openai))

- **Energy Storage**: Innovations in battery technology, such as development 
of advanced lithium-ion, solid-state batteries, have improved energy density, 
charging times, and safety features. These advancements facilitate the 
integration of renewable energy into the power grid and support transition to 
a low-carbon economy. ([toxigon.com](https://toxigon.com/renewable-energy-
technologies-2023?utm_source=openai))

**Global Investment and Policy Support**

The clean energy transition is gaining momentum, with substantial investments 
and policy support driving the shift towards renewable energy:

- The International Energy Agency (IEA) projects that $2 trillion will be 
allocated to clean energy in 2024, with 85% of new power plants being 
renewable. 
([time.com](https://time.com/7022326/climate-leadership-forum-green-economy-
clean-energy-transition/?utm_source=openai))

- China's dominance in green technology manufacturing, including turbines, 
solar panels, electric vehicles, and lithium-ion batteries, positions it as a 
leader in the global renewable energy market. This leadership is expected to 
continue, with China projected to install 60% of world's renewable capacity 
between now and 2030. ([ft.com](https://www.ft.com/content/d3650b44-0313-44c9-
a7aa-495549b158b5?utm_source=openai))

These developments underscore the rapid advancements in renewable energy, 
driven by technological innovation, substantial investments, and supportive 
policies, collectively contributing to a more sustainable and resilient 
global energy landscape.


## Recent Developments in Renewable Energy:
- [Record renewables growth fuels cost competitiveness -IRENA report shows]
(https://www.reuters.com/business/energy/record-renewables-growth-fuels-cost-
competitiveness-irena-report-shows-2024-09-24/?utm_source=openai)
- [Cheap Solar Panels Are Changing the World](https://www.theatlantic.com/science/
archive/2024/10/solar-power-energy-revolution-global-south/680351/?
utm_source=openai)
- [China is winning the race for green supremacy](https://www.ft.com/content/
d3650b44-0313-44c9-a7aa-495549b158b5?utm_source=openai) 

=== Using File Search with Agent ===
File Search Response: It appears that there are no mentions of "climate 
change" in the documents you've uploaded. If you need info or insights on
that topic, please let me know how I can assist you further!

=== Direct File Search Usage ===
Direct File Search Result: It looks like there were no relevant results in the 
uploaded files regarding climate change. However, I can provide summary of key
points about climate change:

1. **Definition**: Climate change refers to significant changes in global 
temperatures and weather patterns over time.

2. **Causes**:
- **Greenhouse Gas Emissions**: Increased levels of carbon dioxide (CO2), 
methane (CH4), and nitrous oxide (N2O) from human activities (e.g., burning 
fossil fuels, deforestation).
- **Natural Factors**: Volcanic eruptions, solar radiation variations, and 
natural greenhouse gas emissions.

3. **Impacts**:
- **Temperature Increases**: Global average temperatures have risen, leading 
to heatwaves and extreme weather.
- **Sea Level Rise**: Melting ice caps and glaciers contribute to rising sea 
levels, which can lead to coastal flooding.
- **Ecosystem Disruption**: Affects biodiversity, leading to habitat loss, 
changes in species distribution, and increased extinction rates.

4. **Social and Economic Consequences**:
- **Food and Water Security**: Impact on agricultural productivity and water 
availability.
- **Health Risks**: Increased prevalence of heat-related illnesses and spread 
of diseases.
- **Economic Costs**: Damage to infrastructure, increased disaster recovery 
costs, and shifts in market dynamics.

5. **Mitigation Strategies**:
- **Renewable Energy Sources**: Transition to solar, wind, and other renewable 
energy resources.
- **Energy Efficiency**: Improving energy use in buildings, transportation, 
and industry.
- **Conservation Practices**: Protecting forests, wetlands, and other 
ecosystems that sequester carbon.

6. **International Agreements**: Efforts like the Paris Agreement aim to unite 
countries to limit global warming to well below 2 degrees Celsius above pre-
industrial levels.

If you need more specific information or details from the files, 
feel free to ask!
"""



--------------------------------------------------------------------------------
# File: toolkits\openapi_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.configs.openai_config import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import OpenAPIToolkit
from camel.types import ModelPlatformType, ModelType

# Define system message
sys_msg = "You are a helpful assistant"

# Set model config
tools = OpenAPIToolkit().get_tools()
model_config_dict = ChatGPTConfig(
    temperature=0.0,
).as_dict()

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=model_config_dict,
)

# Set agent
camel_agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)
camel_agent.reset()

# Define a user message
usr_msg = "help me to select a basketball in klarna."

# Get response information
response = camel_agent.step(usr_msg)
print(response.info['tool_calls'])
"""
===============================================================================
[ToolCallingRecord(func_name='klarna_productsUsingGET', args={
'q_in_query': 'basketball'}, result={'products': [{'name': 'Wilson Evolution'
, 'url': 'https://www.klarna.com/us/shopping/pl/cl1220/3203801266/Basketball
/Wilson-Evolution/?utm_source=openai&ref-site=openai_plugin', 'price':
'$65.00', 'attributes': ['Color:Brown,Blue,Black,Orange', 'Ball Size:6,7',
'Area of Use:Indoors,Outdoors', 'Material:Leather,Rubber']}, {'name':
'Wilson NBA Authentic', 'url': 'https://www.klarna.com/us/shopping/pl/cl1220/
3200358202/Basketball/Wilson-NBA-Authentic/?utm_source=openai&ref-site=openai
_plugin', 'price': '$24.99', 'attributes': ['Color:Orange', 'Ball Size:6,7',
'Area of Use: Indoors,Outdoors', 'Material:Leather']},]})]
===============================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\openbb_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.toolkits import OpenBBToolkit

# Initialize OpenBB toolkit with proper credentials
toolkit = OpenBBToolkit()

# Example 1: Stock quotes for multiple companies
print("\nExample 1: Stock quotes for multiple companies")
companies = ["AAPL", "MSFT", "GOOGL"]
for symbol in companies:
    res_quote = toolkit.get_stock_quote(symbol=symbol)
    print(f"\n{symbol} Stock Quote:")
    print(res_quote)
"""
===============================================================================
AAPL Stock Quote:
[YFinanceEquityQuoteData(symbol=AAPL, asset_type=EQUITY, name=Apple Inc.,
exchange=NMS, bid=249.26, bid_size=5000, bid_exchange=None, ask=250.02,
ask_size=400, ask_exchange=None, quote_conditions=None, quote_indicators=None,
sales_conditions=None, sequence_number=None, market_center=None,
participant_timestamp=None, trf_timestamp=None, sip_timestamp=None,
last_price=249.79, last_tick=None, last_size=None, last_timestamp=None,
open=247.46, high=251.85, low=247.0949, close=None, volume=58911560,
exchange_volume=None, prev_close=248.05, change=None, change_percent=None,
year_high=254.28, year_low=164.08, ma_50d=234.3126, ma_200d=210.4949,
volume_average=42989052.0, volume_average_10d=44930240.0, currency=USD)]

MSFT Stock Quote:
[YFinanceEquityQuoteData(symbol=MSFT, asset_type=EQUITY,
name=Microsoft Corporation, exchange=NMS, bid=418.95, bid_size=100,
bid_exchange=None, ask=437.26, ask_size=100, ask_exchange=None,
quote_conditions=None, quote_indicators=None, sales_conditions=None,
sequence_number=None, market_center=None, participant_timestamp=None,
trf_timestamp=None, sip_timestamp=None, last_price=437.03, last_tick=None,
last_size=None, last_timestamp=None, open=441.62, high=443.1834, low=436.33,
close=None, volume=21207330, exchange_volume=None, prev_close=437.39,
change=None, change_percent=None, year_high=468.35, year_low=366.5,
ma_50d=426.4474, ma_200d=424.4953, volume_average=20174949.0,
volume_average_10d=21000220.0, currency=USD)]

GOOGL Stock Quote:
[YFinanceEquityQuoteData(symbol=GOOGL, asset_type=EQUITY, name=Alphabet Inc.,
exchange=NMS, bid=188.33, bid_size=300, bid_exchange=None, ask=188.56,
ask_size=200, ask_exchange=None, quote_conditions=None, quote_indicators=None,
sales_conditions=None, sequence_number=None, market_center=None,
participant_timestamp=None, trf_timestamp=None, sip_timestamp=None,
last_price=188.51, last_tick=None, last_size=None, last_timestamp=None,
open=191.625, high=193.03, low=188.38, close=None, volume=31130881,
exchange_volume=None, prev_close=188.4, change=None, change_percent=None,
year_high=201.42, year_low=130.67, ma_50d=173.84, ma_200d=167.58334,
volume_average=27155819.0, volume_average_10d=38432110.0, currency=USD)]
===============================================================================
"""

# Example 2: Historical data for Apple stock
print("\nExample 2: Historical data for Apple stock")
res_hist = toolkit.get_historical_data(
    symbol="AAPL",
    start_date="2023-12-01",
    end_date="2023-12-31",
    interval="1d",
)
print("\nRecent Historical Data for AAPL:")
print(res_hist)
"""
===============================================================================
Recent Historical Data for AAPL:
{'results': [YFinanceEquityHistoricalData(date=2023-12-01,
open=190.3300018310547, high=191.55999755859375, low=189.22999572753906,
close=191.24000549316406, volume=45679300, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-04,
open=189.97999572753906, high=190.0500030517578, low=187.4499969482422,
close=189.42999267578125, volume=43389500, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-05,
open=190.2100067138672, high=194.39999389648438, low=190.17999267578125,
close=193.4199981689453, volume=66628400, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-06,
open=194.4499969482422, high=194.75999450683594, low=192.11000061035156,
close=192.32000732421875, volume=41089700, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-07,
open=193.6300048828125, high=195.0, low=193.58999633789062,
close=194.27000427246094, volume=47477700, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-08,
open=194.1999969482422, high=195.99000549316406, low=193.6699981689453,
close=195.7100067138672, volume=53377300, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-11,
open=193.11000061035156, high=193.49000549316406, low=191.4199981689453,
close=193.17999267578125, volume=60943700, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-12,
open=193.0800018310547, high=194.72000122070312, low=191.72000122070312,
close=194.7100067138672, volume=52696900, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-13,
open=195.08999633789062, high=198.0, low=194.85000610351562,
close=197.9600067138672, volume=70404200, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-14,
open=198.02000427246094, high=199.6199951171875, low=196.16000366210938,
close=198.11000061035156, volume=66831600, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-15,
open=197.52999877929688, high=198.39999389648438, low=197.0,
close=197.57000732421875, volume=128256700, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-18,
open=196.08999633789062, high=196.6300048828125, low=194.38999938964844,
close=195.88999938964844, volume=55751900, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-19,
open=196.16000366210938, high=196.9499969482422, low=195.88999938964844,
close=196.94000244140625, volume=40714100, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-20,
open=196.89999389648438, high=197.67999267578125, low=194.8300018310547,
close=194.8300018310547, volume=52242800, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-21,
open=196.10000610351562, high=197.0800018310547, low=193.5,
close=194.67999267578125, volume=46482500, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-22,
open=195.17999267578125, high=195.41000366210938, low=192.97000122070312,
close=193.60000610351562, volume=37122800, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-26,
open=193.61000061035156, high=193.88999938964844, low=192.8300018310547,
close=193.0500030517578, volume=28919300, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-27,
open=192.49000549316406, high=193.5, low=191.08999633789062,
close=193.14999389648438, volume=48087700, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-28,
open=194.13999938964844, high=194.66000366210938, low=193.1699981689453,
close=193.5800018310547, volume=34049900, vwap=None, split_ratio=None,
dividend=None), YFinanceEquityHistoricalData(date=2023-12-29,
open=193.89999389648438, high=194.39999389648438, low=191.72999572753906,
close=192.52999877929688, volume=42628800, vwap=None, split_ratio=None,
dividend=None)]}
===============================================================================
"""

# Example 3: Company Information
print("\nExample 3: Company Information")
company_info = toolkit.get_company_profile(symbol="AAPL", provider="fmp")
print("\nApple Inc. Company Information:")
print(company_info)
"""
===============================================================================
Apple Inc. Company Information:
[FMPEquityProfileData(symbol=AAPL, name=Apple Inc., cik=0000320193, 
cusip=037833100, isin=US0378331005, lei=None, legal_name=None, 
stock_exchange=NASDAQ Global Select, sic=None, short_description=None, 
long_description=Apple Inc. designs, manufactures, and markets smartphones, 
personal computers, tablets, wearables, and accessories worldwide. The company 
offers iPhone, a line of smartphones; Mac, a line of personal computers; iPad, 
a line of multi-purpose tablets; and wearables, home, and accessories 
comprising AirPods, Apple TV, Apple Watch, Beats products, and HomePod. It 
also provides AppleCare support and cloud services; and operates various 
platforms, including the App Store that allow customers to discover and 
download applications and digital content, such as books, music, video, games, 
and podcasts. In addition, the company offers various services, such as Apple 
Arcade, a game subscription service; Apple Fitness+, a personalized fitness 
service; Apple Music, which offers users a curated listening experience with 
on-demand radio stations; Apple News+, a subscription news and magazine 
service; Apple TV+, which offers exclusive original content; Apple Card, a 
co-branded credit card; and Apple Pay, a cashless payment service, as well as 
licenses its intellectual property. The company serves consumers, and small 
and mid-sized businesses; and the education, enterprise, and government 
markets. It distributes third-party applications for its products through the 
App Store. The company also sells its products through its retail and online 
stores, and direct sales force; and third-party cellular network carriers, 
wholesalers, retailers, and resellers. Apple Inc. was founded in 1976 and is 
headquartered in Cupertino, California., ceo=Mr. Timothy D. Cook, 
company_url=https://www.apple.com, business_address=None, 
mailing_address=None, business_phone_no=408 996 1010, hq_address1=One Apple 
Park Way, hq_address2=None, hq_address_city=Cupertino, 
hq_address_postal_code=95014, hq_state=CA, hq_country=US, inc_state=None, 
inc_country=None, employees=164000, entity_legal_form=None, 
entity_status=None, latest_filing_date=None, irs_number=None, 
sector=Technology, industry_category=Consumer Electronics, 
industry_group=None, template=None, standardized_active=None, 
first_fundamental_date=None, last_fundamental_date=None, 
first_stock_price_date=1980-12-12, last_stock_price_date=None, is_etf=False, 
is_actively_trading=True, is_adr=False, is_fund=False, image=https://images.
financialmodelingprep.com/symbol/AAPL.png, currency=USD, 
market_cap=3785298636000, last_price=250.42, year_high=260.1, year_low=164.08, 
volume_avg=43821504, annualized_dividend_amount=0.99, beta=1.24)]
===============================================================================
"""

# Example 4: Financial Statements
print("\nExample 4: Financial Statements")
balance_sheet = toolkit.get_financial_statement(
    symbol="MSFT",
    statement_type="balance",
    period="annual",
    provider="fmp",
    limit=5,
)
income_stmt = toolkit.get_financial_statement(
    symbol="MSFT",
    statement_type="income",
    period="annual",
    provider="fmp",
    limit=5,
)
cash_flow = toolkit.get_financial_statement(
    symbol="MSFT",
    statement_type="cash",
    period="annual",
    provider="fmp",
    limit=5,
)
print("\nMicrosoft Financial Statements Overview:")
print(f"Balance Sheet: {balance_sheet}")
print(f"Income Statement: {income_stmt}")
print(f"Cash Flow Statement: {cash_flow}")
"""
===============================================================================
Microsoft Financial Statements Overview:
Balance Sheet: [FMPBalanceSheetData(period_ending=2024-06-30, 
fiscal_period=FY, fiscal_year=2024, filing_date=2024-07-30, 
accepted_date=2024-07-30 16:06:22, reported_currency=USD, 
cash_and_cash_equivalents=18315000000.0, short_term_investments=57216000000.0, 
cash_and_short_term_investments=75531000000.0, net_receivables=56924000000.0, 
inventory=1246000000.0, other_current_assets=26033000000.0, 
total_current_assets=159734000000.0, plant_property_equipment_net=154552000000.
0, goodwill=119220000000.0, intangible_assets=27597000000.0, 
goodwill_and_intangible_assets=146817000000.0, 
long_term_investments=14600000000.0, tax_assets=None, 
other_non_current_assets=36460000000.0, non_current_assets=352429000000.0, ..
===============================================================================
"""

# Example 5: Financial Analysis with ChatAgent
print("\nExample 5: Financial Analysis with ChatAgent")

# Initialize agent with toolkit tools
tech_agent = ChatAgent(
    system_message="""You are a financial analysis expert. Analyze the provided
    financial data and provide insights about the company's financial 
    health.""",
    tools=toolkit.get_tools(),
)

# Get company data
symbol = "AAPL"

# Get financial statements using correct provider
balance_sheet = toolkit.get_financial_statement(
    symbol=symbol,
    statement_type="balance",
    period="annual",
    provider="fmp",
    limit=3,
)
print(f"\n{symbol} Balance Sheet:")
print(balance_sheet)

income_stmt = toolkit.get_financial_statement(
    symbol=symbol,
    statement_type="income",
    period="annual",
    provider="fmp",
    limit=3,
)
print(f"\n{symbol} Income Statement:")
print(income_stmt)

# Get financial metrics
metrics = toolkit.get_financial_metrics(
    symbol=symbol, period="annual", provider="fmp", limit=3
)
print(f"\n{symbol} Financial Metrics:")
print(metrics)

# Get company profile
profile = toolkit.get_company_profile(symbol=symbol, provider="fmp")
print(f"\n{symbol} Company Profile:")
print(profile)

# Example analysis prompt
analysis_prompt = f"""
Analyze {symbol}'s financial health based on:
1. Balance sheet strength
2. Profitability trends
3. Key financial metrics
4. Business profile

Provide a concise summary of strengths and potential concerns.
"""

response = tech_agent.step(input_message=analysis_prompt)
print("\nFinancial Analysis:")
print(response.msgs[0].content)
"""
===============================================================================
Financial Analysis:
### Financial Health Analysis of Apple Inc. (AAPL)

#### 1. Balance Sheet Strength
- **Total Debt**: The total debt has shown a decreasing trend from 
approximately $132.48 billion in 2022 to $106.63 billion in 2024. This 
indicates a reduction in leverage and improved financial stability.
- **Net Debt**: Similarly, net debt has decreased from about $108.83 billion 
in 2022 to $76.69 billion in 2024, suggesting that the company is managing its 
debt effectively and has sufficient cash reserves to cover its liabilities.

#### 2. Profitability Trends
- **Revenue Growth**: AAPL has consistently generated significant revenue, 
with a notable increase in profitability over the years. The income statement 
shows a healthy profit margin, indicating effective cost management.
- **Operating Income**: The operating income has remained strong, reflecting 
the company's ability to generate profit from its core operations.
- **Interest Expenses**: Interest expenses have been relatively stable, which 
is a positive sign as it indicates that the company is not over-leveraged.

#### 3. Key Financial Metrics
- **Market Capitalization**: As of 2024, AAPL's market cap is approximately 
$3.50 trillion, making it one of the most valuable companies in the world.
- **P/E Ratio**: The P/E ratio has increased from 24.44 in 2022 to 37.29 in 
2024, indicating that the stock may be overvalued relative to its earnings, 
which could be a concern for investors.
- **Dividend Yield**: The dividend yield has decreased slightly, reflecting a 
focus on reinvesting profits for growth rather than returning cash to 
shareholders.
- **Graham Number**: The Graham number indicates that the stock may be 
overvalued, as the calculated value is negative, suggesting that the stock 
price exceeds its intrinsic value based on earnings and book value.

#### 4. Business Profile
- **Industry Position**: AAPL is a leader in the technology sector, 
particularly in consumer electronics, software, and services. Its strong brand 
loyalty and innovative product offerings contribute to its competitive 
advantage.
- **Growth Potential**: The company continues to invest in research and 
development, positioning itself for future growth in emerging technologies and 
services.

### Summary of Strengths and Potential Concerns
**Strengths:**
- Strong balance sheet with decreasing total and net debt.
- Consistent revenue and operating income growth.
- Leading market capitalization and brand recognition.

**Potential Concerns:**
- Increasing P/E ratio may indicate overvaluation.
- Decreasing dividend yield could concern income-focused investors.
- Negative Graham number suggests potential overvaluation based on intrinsic 
value metrics.

Overall, AAPL demonstrates robust financial health, but investors should be 
cautious of valuation metrics that may indicate a correction in stock price.
===============================================================================
"""

# Example 6: ChatAgent using OpenBB toolkit
print("\nExample 6: ChatAgent using OpenBB toolkit")
agent = ChatAgent(
    system_message="""You are a helpful financial analyst that can use
    OpenBB toolkit to analyze stocks and market data. When comparing stock
    prices with moving averages, use the data directly from the stock quotes
    including the volume_average_10d field for accurate analysis.""",
    tools=toolkit.get_tools(),
)

usr_msg = (
    "Compare the current stock prices of AAPL and MSFT with their trading "
    "volumes and 10-day average volumes using the quote data"
)
response = agent.step(input_message=usr_msg)
print("\nAI Analysis:")
print(response.msgs[0].content)
"""
===============================================================================
AI Analysis:
Here is the comparison of the current stock prices, trading volumes,
and 10-day average volumes for Apple Inc. (AAPL) and Microsoft Corporation
(MSFT):

### Apple Inc. (AAPL)
- **Current Price:** $244.75
- **Current Trading Volume:** 44,476,711
- **10-Day Average Volume:** 56,030,030

### Microsoft Corporation (MSFT)
- **Current Price:** $424.63
- **Current Trading Volume:** 20,960,541
- **10-Day Average Volume:** 25,580,240

### Summary
- **AAPL** has a higher current trading volume compared to its 10-day
average volume, indicating increased trading activity.
- **MSFT** has a lower current trading volume than its 10-day average
volume, suggesting less trading activity relative to the past 10 days.

If you need further analysis or details, feel free to ask!
===============================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\post_weather_on_twitter.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from colorama import Fore

from camel.agents import ChatAgent
from camel.toolkits import SearchToolkit, TwitterToolkit, WeatherToolkit
from camel.utils import print_text_animated


def main():
    """
    To run this, you need to set the following environment variables:
    - TWITTER_CONSUMER_KEY
    - TWITTER_CONSUMER_SECRET
    - TWITTER_ACCESS_TOKEN
    - TWITTER_ACCESS_TOKEN_SECRET
    - OPENWEATHERMAP_API_KEY
    - GOOGLE_API_KEY
    - SEARCH_ENGINE_ID
    """

    sys_msg = "You are a helpful agent with multiple tools."

    agent = ChatAgent(
        system_message=sys_msg,
        tools=[
            *TwitterToolkit().get_tools(),
            *WeatherToolkit().get_tools(),
            *SearchToolkit().get_tools(),
        ],
    )

    usr_msg = "I'm in Chicago and want to travel to Oxford today. Make a "
    "travel plan for me, considering the weather today. Also announce my "
    "plan on Twitter from my perspective."

    response = agent.step(usr_msg)

    for tool_call in response.info["tool_calls"]:
        print(f"{Fore.YELLOW}{tool_call}{Fore.RESET}\n======")

    print_text_animated(
        f"{Fore.GREEN}{response.msg.content}{Fore.RESET}", delay=0.005
    )


if __name__ == '__main__':
    main()



--------------------------------------------------------------------------------
# File: toolkits\pubmed_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs.openai_config import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import PubMedToolkit
from camel.types import ModelPlatformType, ModelType

# Initialize PubMed toolkit and get tools
tools = PubMedToolkit().get_tools()

# Set up model configuration
model_config_dict = ChatGPTConfig(
    temperature=0.0,
).as_dict()

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=model_config_dict,
)

# Create chat agent
system_msg = (
    "You are a research assistant specialized in medical literature. "
    "Help researchers find and analyze scientific papers from PubMed."
)
camel_agent = ChatAgent(
    system_message=system_msg,
    model=model,
    tools=tools,
)
camel_agent.reset()

# Example 1: Search for recent papers about mRNA vaccine technology
print("\nExample 1: Search for recent papers about mRNA vaccine technology")
print("=" * 80)

usr_msg = (
    "Find recent review papers about mRNA vaccine technology published "
    "in 2024, with a focus on therapeutic applications and clinical trials. "
    "Limit to 3 papers."
)

response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:2000])

"""
===============================================================================
ToolCallingRecord(
    tool_name='search_papers',
    args={
        'query': 'mRNA vaccine tech therapeutic applications trials',
        'max_results': 10,
        'sort': 'date',
        'date_range': {'from': '2024/01/01', 'to': '2024/12/31'},
        'publication_type': ['Review'],
    },
    result=[
        {
            'id': '39601789',
            'title': 'Example Title',
            'authors': 'First Author, Second Author',
            'journal': 'Example Journal',
            'pub_date': '2025 Jan 6',
            'abstract': 'Abstract of the paper',
===============================================================================
"""


# Example 2: Get detailed information about a specific paper
print("\nExample 2: Get detailed paper information")
print("=" * 80)

usr_msg = (
    "Get detailed information about PubMed ID 39601789 "
    "(a key paper about mRNA vaccine technology)."
)
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:2000])

"""
===============================================================================
[ToolCallingRecord(
    tool_name='get_paper_details',
    args={'paper_id': 37840631, 'include_references': True},
    result={
        'id': '37840631',
        'title': 'Chinese guideline for lipid management (2023):
                  a new guideline rich in domestic elements for 
                  controlling dyslipidemia.',
        'authors': 'Li JJ',
        'journal': 'J Geriatr Cardiol',
        'pub_date': '2023 Sep 28',
        'abstract': '1. J Geriatr Cardiol. 
                     2023 Sep 28;20(9):618-620. 
                     doi: 10.26599/1671-5411.2023.09.007.
                     Chinese guideline for lipid management (2023):
                     a new guideline rich in domestic elements for 
                     controlling dyslipidemia.Li JJ(1).\Author information:
                     (1)Division of Cardio-Metabolic Center,
                     State Key Laboratory of Cardiovascular 
                     Disease, Fu Wai Hospital, National Center 
                     for Cardiovascular Disease, Chinese Academy
                     of Medical Sciences, Peking Union Medical College,
                     Beijing, China.DOI: 10.26599/1671-5411.2023.09.007
                     PMCID: PMC10568543\nPMID: 37840631',
        'doi': 'doi: 10.26599/1671-5411.2023.09.007',
        'keywords': [],
        'mesh_terms': [],
        'publication_types': ['Journal Article'],
        'references': ['35729555', '34734202', '34404993', 
                       '31172370', '30586774', '30526649', 
                       '29434622', '20350253']
    },
    tool_call_id='call_k8s7oFcRvDBKuEKvk48uoWXZ'
)]
===============================================================================
"""

# Example 3: Find related papers and citation metrics
print("\nExample 3: Find related papers and citation metrics")
print("=" * 80)

usr_msg = (
    "Find papers related to PubMed ID 39601789 (limit to 3 papers) and "
    "show its citation count."
)
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:2000])

"""
===============================================================================
[ToolCallingRecord(
    tool_name='get_related_papers',
    args={'paper_id': 37840631, 'max_results': 5},
    result=[
        {'id': '37840631',
         'title': 'Chinese guideline for lipid management (2023):
                   a new guideline rich in domestic elements for 
                   controlling dyslipidemia.',
         'authors': 'Li JJ',
         'journal': 'J Geriatr Cardiol',
         'pub_date': '2023 Sep 28',
         'abstract': (
             '1. J Geriatr Cardiol. 2023 Sep 28;20(9):618-620. doi: '
             '10.26599/1671-5411.2023.09.007.'
             'Chinese guideline for lipid management (2023): a new guideline'
             'rich in domestic elements for controlling dyslipidemia.'
             'Li JJ(1).Author information:(1)Division of Cardio-Metabolic '
             'Center, State Key Laboratory of Cardiovascular Disease, Fu Wai '
             'Hospital, National Center for Cardiovascular Disease, Chinese '
             'Academy of Medical Sciences, Peking Union Medical College, '
             'Beijing, China.DOI: 10.26599/1671-5411.2023.09.007'
             'PMCID: PMC10568543  PMID: 37840631'
         ),
         'doi': 'doi: 10.26599/1671-5411.2023.09.007',
         'keywords': [],
         'mesh_terms': [],
         'publication_types': ['Journal Article'],
         'references': None},
        {'id': '22801311',
         'title': (
             '[Short-term impact of modified blood-lipid reports on physicians'
             'lipid lowering drug prescribing behavior and knowledge '
             'improvement on dyslipidemia].'
         ),
         'authors': 'Li JH, Jiang H, Sun XH, Li CC, Ke YN, Yan SK, Wu YF',
         'journal': 'Zhonghua Xin Xue Guan Bing Za Zhi',
         'pub_date': '2012 Apr',
         'abstract': (
             '1. Zhonghua Xin Xue Guan Bing Za Zhi. 2012 Apr;40(4):318-22.'
             '[Short-term impact modified blood-lipid reports on physicians'
             'lipid lowering drug prescribing behavior and knowledge '
             'improvement on dyslipidemia].Article in Chinese]'
             'Li JH(1), Jiang H, Sun XH, Li CC, Ke YN, Yan SK, Wu YF.'
             'Author information:(1)Department of Cardiology, China-Japan'
===============================================================================
"""

# Example 4: Advanced search with multiple filters
print("\nExample 4: Advanced search with multiple filters")
print("=" * 80)

usr_msg = (
    "Find clinical trial papers about mRNA-based cancer vaccines published "
    "between 2023/01/01 and 2024/03/01, focusing on phase III trials. "
    "Limit to 3 papers."
)
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:2000])

"""
===============================================================================
[ToolCallingRecord(
    tool_name='search_papers',
    args={
        'query': 'mRNA cancer vaccine phase III clinical trial',
        'max_results': 10,
        'sort': 'date',
        'date_range': {'from': '2023/01/01', 'to': '2024/03/01'},
        'publication_type': ['Clinical Trial']
    },
    result=[
        {
            'id': '37820782',
            'title': 'Stochastic interventional approach to assessing immune '
                      'correlates of protection: Application to the COVE '
                      'RNA-1273 vaccine trial.',
            'authors': (
                'Hejazi NS, Shen X, Carpp LN, Benkeser D, Follmann D, 
                Janes HE, Baden LR, El Sahly HM, Deng W, Zhou H, 
                Leav B, Montefiori DC, 'Gilbert PB'
            ),
            'journal': 'Int J Infect Dis',
            'pub_date': '2023 Dec',
            'abstract': Abstract of the paper
===============================================================================
"""

# Example 5: Get abstract and analyze citations
print("\nExample 5: Get abstract and analyze citations")
print("=" * 80)

usr_msg = (
    "Get the abstract of PubMed ID 39601789 and find out how many times "
    "it has been cited."
)
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:2000])

"""
===============================================================================
[
    ToolCallingRecord(
        tool_name='get_abstract',
        args={'paper_id': 37840631},
        result='''
            1. J Geriatr Cardiol. 2023 Sep 28;20(9):618-620. doi: 
            10.26599/1671-5411.2023.09.007.
            
            Chinese guideline for lipid management (2023):a new guideline 
            rich in domestic elements for controlling dyslipidemia.
            
            Li JJ(1).
            
            Author information:
            (1)Division of Cardio-Metabolic Center, State Key Laboratory
            of Cardiovascular Disease, Fu Wai Hospital, National Center 
            for Cardiovascular Disease, Chinese Academy of Medical Sciences,
            Peking Union Medical College, Beijing, China.
            
            DOI: 10.26599/1671-5411.2023.09.007
            PMCID: PMC10568543
            PMID: 37840631
        ''',
        tool_call_id='call_AFG6jLkdvWidaVGrj9UblTci'
    ),
    ToolCallingRecord(
        tool_name='get_citation_count',
        args={'paper_id': 37840631},
        result=0,
        tool_call_id='call_ZM3p59gtYmeR9DPdONNHV4Qw'
    )
]
===============================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\reddit_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.toolkits import RedditToolkit

reddit_toolkit = RedditToolkit()

# Example usage of collect_top_posts
subreddit_name = "python"
post_limit = 2
comment_limit = 3

print(f"Collecting top {post_limit} posts from r/{subreddit_name}...\n")
top_posts_data = reddit_toolkit.collect_top_posts(
    subreddit_name, post_limit, comment_limit
)

# Output the top posts data
for post in top_posts_data:
    print(f"Post Title: {post['Post Title']}")
    for comment in post["Comments"]:
        print(f"  Comment: {comment['Comment Body']}")
        print(f"  Upvotes: {comment['Upvotes']}")
    print()

'''
===============================================================================
Collecting top 2 posts from r/python...

Post Title: Lad wrote a Python script to download 
            Alexa voice recordings, 
            he didn't expect this email.
  Comment: I will be honest, I was expecting a Cease 
           and Desist from Amazon.
  Upvotes: 1857
  Comment: Very cool. That is the beauty of sharing. 
           You never know who or how it will help someone, 
           but you post it anyway because that is just being awesome. 

Thanks for sharing.
  Upvotes: 264
  Comment: This was posted publicly by [Michael Haephrati]
            (https://www.facebook.com/photo.php?fbid=10220524693682311)
            on Facebook.

Update: The lad is a Redditor! [**u/haephrati**]
        (https://www.reddit.com/user/haephrati/)

Update: The lad turned it into a [Software]
        (https://www.facebook.com/pg/accexa2020/about/?ref=page_internal)!
  Upvotes: 219

Post Title: This post has:
  Comment: scale tap piquant quiet advise salt languid abundant dolls long
           -- mass edited with redact.dev
  Upvotes: 1325
  Comment: Good job. But honestly, add a sleep timer of a few seconds. 
           This will eventually get your IP banned on reddit 
           if you bombard them with too many requests.
  Upvotes: 408
  Comment: Cool! Could you share it?
  Upvotes: 113
===============================================================================
'''

# Track keyword discussions
keywords = [
    "python",
    "programming",
    "coding",
    "software",
    "development",
    "machine learning",
    "artificial intelligence",
    "AI",
    "deep learning",
    "data science",
    "analytics",
    "automation",
    "tech",
    "technology",
    "engineering",
    "developer",
    "algorithm",
    "API",
    "framework",
    "library",
    "tool",
    "debug",
    "optimization",
    "performance",
    "security",
    "privacy",
    "database",
    "cloud",
    "server",
    "network",
    "startup",
    "entrepreneur",
    "innovation",
    "research",
    "science",
]

subreddits = [
    "python",
    "learnprogramming",
    "datascience",
    "machinelearning",
]
keyword_data = reddit_toolkit.track_keyword_discussions(
    subreddits, keywords, post_limit=2, comment_limit=5
)


# Perform sentiment analysis on collected data
sentiment_data = reddit_toolkit.perform_sentiment_analysis(keyword_data)

# Output the results
for item in sentiment_data:
    print(f"Subreddit: {item['Subreddit']}")
    print(f"Post Title: {item['Post Title']}")
    print(f"Comment Body: {item['Comment Body']}")
    print(f"Upvotes: {item['Upvotes']}")
    if 'Sentiment Score' in item:
        print(f"Sentiment Score: {item['Sentiment Score']}")
    print()

'''
===============================================================================

Subreddit: learnprogramming
Post Title: I ran a 100% free full stack web development bootcamp
            for those laid off by the pandemic. 
            65 people got jobs and we are doing it again! 
            I would love to have you join us!
Comment Body: If you want to learn to code, this will change your life.
Can't make it to class? Recorded classes are on Twitch and YouTube
Never touched code before? He starts from square 1!
Shy/introvert/don't like talking? Stick to the chat
Don't have support in real life? 
Join the discord and get more support and hype than your family
Don't have money? It's free!
Not in the US? Leon is Mr. Worldwide when it comes to teaching!
100Devs isn't just a free online bootcamp, 
it's a whole support network that will be there for you to cheer you on, 
help you out, and give you a shoulder to cry on. 
If you're on the fence, give it a try. You won't regret it.
Upvotes: 518
Sentiment Score: 0.385

Subreddit: learnprogramming
Post Title: I ran a 100% free full stack web development bootcamp 
            for those laid off by the pandemic. 
            65 people got jobs and we are doing it again! 
            I would love to have you join us!
Comment Body: If you need any free dev help let me know

I was also a teacher for 15 years before coding if that's helpful too
Upvotes: 533
Sentiment Score: 0.4

Subreddit: datascience
Post Title: data siens
Comment Body: I was once reading this article that went as: 
              "The AI already predicted how many goals Cavani 
              will score at Manchester United". 
It was a linear regression.
Upvotes: 345
Sentiment Score: 0.5

Subreddit: machinelearning
Post Title: [D] A Demo from 1993 of 32-year-old Yann LeCun 
            showing off the World's first Convolutional 
            Network for Text Recognition
Comment Body: The fact that they also had to know the 
              location of the numbers and that the algorithm 
              was robust to scale changes is impressive for 1993

It's not like they just solved MNIST in 1993, it's one step above that
Upvotes: 412
Sentiment Score: 0.5
===============================================================================
'''



--------------------------------------------------------------------------------
# File: toolkits\role_playing_with_functions.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from typing import List

from colorama import Fore

from camel.agents.chat_agent import ToolCallingRecord
from camel.models import ModelFactory
from camel.societies import RolePlaying
from camel.toolkits import (
    MathToolkit,
    SearchToolkit,
)
from camel.types import ModelPlatformType, ModelType
from camel.utils import print_text_animated


def main(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    chat_turn_limit=10,
) -> None:
    task_prompt = (
        "Assume now is 2024 in the Gregorian calendar, "
        "estimate the current age of University of Oxford "
        "and then add 10 more years to this age, "
        "and get the current weather of the city where "
        "the University is located. You must use tool to solve the task."
    )

    tools_list = [
        *MathToolkit().get_tools(),
        SearchToolkit().search_duckduckgo,
    ]

    role_play_session = RolePlaying(
        assistant_role_name="Searcher",
        user_role_name="Professor",
        assistant_agent_kwargs=dict(
            model=ModelFactory.create(
                model_platform=model_platform,
                model_type=model_type,
            ),
            tools=tools_list,
        ),
        user_agent_kwargs=dict(
            model=ModelFactory.create(
                model_platform=model_platform,
                model_type=model_type,
            ),
        ),
        task_prompt=task_prompt,
        with_task_specify=False,
    )

    print(
        Fore.GREEN
        + f"AI Assistant sys message:\n{role_play_session.assistant_sys_msg}\n"
    )
    print(
        Fore.BLUE + f"AI User sys message:\n{role_play_session.user_sys_msg}\n"
    )

    print(Fore.YELLOW + f"Original task prompt:\n{task_prompt}\n")
    print(
        Fore.CYAN
        + "Specified task prompt:"
        + f"\n{role_play_session.specified_task_prompt}\n"
    )
    print(Fore.RED + f"Final task prompt:\n{role_play_session.task_prompt}\n")

    n = 0
    input_msg = role_play_session.init_chat()
    while n < chat_turn_limit:
        n += 1
        assistant_response, user_response = role_play_session.step(input_msg)

        if assistant_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI Assistant terminated. Reason: "
                    f"{assistant_response.info['termination_reasons']}."
                )
            )
            break
        if user_response.terminated:
            print(
                Fore.GREEN
                + (
                    "AI User terminated. "
                    f"Reason: {user_response.info['termination_reasons']}."
                )
            )
            break

        # Print output from the user
        print_text_animated(
            Fore.BLUE + f"AI User:\n\n{user_response.msg.content}\n"
        )

        # Print output from the assistant, including any function
        # execution information
        print_text_animated(Fore.GREEN + "AI Assistant:")
        tool_calls: List[ToolCallingRecord] = [
            ToolCallingRecord(**call.as_dict())
            for call in assistant_response.info['tool_calls']
        ]
        for func_record in tool_calls:
            print_text_animated(f"{func_record}")
        print_text_animated(f"{assistant_response.msg.content}\n")

        if "CAMEL_TASK_DONE" in user_response.msg.content:
            break

        input_msg = assistant_response.msg


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: toolkits\search_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from pydantic import BaseModel

from camel.agents import ChatAgent
from camel.toolkits import FunctionTool, SearchToolkit

res_simple = SearchToolkit().query_wolfram_alpha(
    query="solve 3x-7=11", is_detailed=False
)

print(res_simple)
'''
===============================================================================
x = 6
===============================================================================
'''

res_detailed = SearchToolkit().query_wolfram_alpha(
    query="solve 3x-7=11", is_detailed=True
)

print(res_detailed)
'''
===============================================================================
{'query': 'solve 3x-7=11', 'pod_info': [{'title': 'Input interpretation', 
'description': 'solve 3 x - 7 = 11', 'image_url': 'https://www6b3.wolframalpha.
com/Calculate/MSP/MSP37741a3dc67f338579ff00003fih94dg39300iaf?
MSPStoreType=image/gif&s=18'}, {'title': 'Result', 'description': 'x = 6', 
'image_url': 'https://www6b3.wolframalpha.com/Calculate/MSP/
MSP37751a3dc67f338579ff00001dg4gbdcd0f10i3f?MSPStoreType=image/gif&s=18'}, 
{'title': 'Plot', 'description': None, 'image_url': 'https://www6b3.
wolframalpha.com/Calculate/MSP/MSP37761a3dc67f338579ff0000374484g95bh3ah3e?
MSPStoreType=image/gif&s=18'}, {'title': 'Number line', 'description': None, 
'image_url': 'https://www6b3.wolframalpha.com/Calculate/MSP/
MSP37771a3dc67f338579ff00005573c3a87ahg8dbc?MSPStoreType=image/gif&s=18'}], 
'final_answer': 'x = 6', 'steps': {'step1': 'Isolate terms with x to the left 
hand side.\nAdd 7 to both sides:\n3 x + (7 - 7) = 7 + 11', 'step2': 'Look for 
the difference of two identical terms.\n7 - 7 = 0:\n3 x = 11 + 7', 'step3': 
'Evaluate 11 + 7.\n11 + 7 = 18:\n3 x = 18', 'step4': 'Divide both sides by a 
constant to simplify the equation.\nDivide both sides of 3 x = 18 by 3:\n(3 x)/
3 = 18/3', 'step5': 'Any nonzero number divided by itself is one.\n3/3 = 1:\nx 
= 18/3', 'step6': 'Reduce 18/3 to lowest terms. Start by finding the greatest 
common divisor of 18 and 3.\nThe greatest common divisor of 18 and 3 is 3, so 
factor out 3 from both the numerator and denominator: 18/3 = (3x6)/(3x1) = 3/3 
x 6 = 6\nAnswer: | \n | x = 6'}}
===============================================================================
'''

res_brave = SearchToolkit().search_brave(
    q="What is the weather in Tokyo?",
    search_lang="en",
)
print(res_brave)

# Example with ChatAgent using the Brave search engine

agent = ChatAgent(
    system_message="""You are a helpful assistant that can use brave search 
        engine to answer questions.""",
    tools=[FunctionTool(SearchToolkit().search_brave)],
)

usr_msg = "What is the temperature in Tokyo?"

response = agent.step(input_message=usr_msg, response_format=None)

print(response.msgs[0].content)
"""
===============================================================================
The current temperature in Tokyo can be found on various weather websites. 
Here are a couple of reliable sources where you can check the latest weather 
conditions:

1. [AccuWeather - Tokyo Current Weather](https://www.accuweather.com/en/jp/tokyo/226396/current-weather/226396)
2. [Time and Date - Tokyo Weather](https://www.timeanddate.com/weather/japan/tokyo)

You can visit these links to get the most up-to-date temperature and weather 
conditions in Tokyo.
===============================================================================
"""
search_linkup_response = SearchToolkit().search_linkup(
    query="Can you tell me which women were awarded the Physics Nobel Prize",
    depth="standard",
    output_type="searchResults",
)

print(search_linkup_response)
"""
===============================================================================
{'results': [{'type': 'text', 'name': 'Physics Nobel Prizes awarded to women | 
Scientia News', 'url': 'https://www.scientianews.org/
physics-nobel-prize-winners', 'content': 'The next female Nobel Prize in 
Physics award winner wouldn't be until another half-century later, with Donna 
Strickland. Strickland was awarded the Prize for her work on chirped pulse 
amplification and its applications. Although the research itself was published 
in 1985, she didn't receive the award until 2018.'}, {'type': 'text', 'name': 
'The 60 Women Who Have Won the Nobel Prize - Stacker', 'url': 'https://stacker.
com/history/60-women-who-have-won-nobel-prize', 'content': '- Award: Nobel 
Prize in Physics - Year: 1963. Maria Goeppert-Mayer was born in Germany. After 
she married, she migrated to America, where she worked on an American atom 
bomb project during World War II. Her work uncovered important discoveries 
about nuclear structure, and Goeppert-Mayer is one of only four women to win 
the Nobel Prize in physics.'}, {'type': 'text', 'name': 'Nobel Prize awarded 
women - NobelPrize.org', 'url': 'https://www.nobelprize.org/prizes/lists/
nobel-prize-awarded-women/', 'content': 'The Nobel Prize and the Sveriges 
Riksbank Prize in Economic Sciences in Memory of Alfred Nobel have been 
awarded to women 66 times between 1901 and 2024. Only one woman, Marie Curie, 
has been honoured twice, with the Nobel Prize in Physics 1903 and the Nobel 
Prize in Chemistry 1911. This means that 65 women in total have been awarded 
the Nobel ...'}, {'type': 'text', 'name': 'Women who changed science - The 
Nobel Prize', 'url': 'https://www.nobelprize.org/womenwhochangedscience/
stories', 'content': 'Nobel Prize in Physics 1903 Nobel Prize in Chemistry 
1911. MARIE CURIE. Read her story. Nobel Prize in Physiology or Medicine 1988. 
GERTRUDE B. ELION. Read her story. Nobel Prize in Physiology or Medicine 1988. 
GERTRUDE B. ELION. Read her story. Nobel Prize in Physics 1963. MARIA GOEPPERT 
MAYER. Read her story.'}, {'type': 'text', 'name': 'List of female Nobel 
laureates - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/
List_of_female_Nobel_laureates', 'content': "The most recent women to be 
awarded a Nobel Prize were Han Kang in Literature (2024), Claudia Goldin in 
Economics, Narges Mohammadi for Peace, Anne L'Huillier in Physics and Katalin 
Karikó in Physiology or Medicine (2023), Annie Ernaux in Literature and 
Carolyn R. Bertozzi for Chemistry (2022), Maria Ressa for Peace (2021), Louise 
Glück in ..."}, {'type': 'text', 'name': 'Only 5 women have won the Nobel 
Prize in physics—recent winners share ...', 'url': 'https://phys.org/news/
2024-10-women-won-nobel-prize-physics.html', 'content': 'Out of 225 people 
awarded the Nobel Prize in physics, only five have been women. This is a very 
small number, and certainly smaller than 50%—the percent of women in the human 
population.'}, {'type': 'text', 'name': 'All These Women Won Science Nobel 
Prizes - The Stemettes Zine', 'url': 'https://stemettes.org/zine/articles/
nobel-prize-women/', 'content': 'Currently, only 17% of Nobel Prize winners 
are women in the Science categories. So here we are celebrating all the 
amazing women who have Nobel Prizes for their Science research. ... & Physics 
(1903) Marie and her husband were awarded the Nobel Prize for Physics in 1903, 
for their study into the spontaneous radiation discovered by Becquerel. In ...
'}, {'type': 'text', 'name': 'These Are the 57 Women Who Have Won the Nobel 
Prize', 'url': 'https://www.newsweek.com/
these-are-57-women-who-have-won-nobel-prize-1538702', 'content': 'Getty Images/
Hulton-Deutsch Collection/CORBIS Marie Curie (born Skłodowska) - Award: Nobel 
Prize in Physics - Year: 1903. Marie Curie, who was the first woman to win a 
Nobel Prize, coined the ...'}, {'type': 'text', 'name': 'Anne L'Huillier - 
Banquet speech - NobelPrize.org', 'url': 'https://www.nobelprize.org/prizes/
physics/2023/lhuillier/speech/', 'content': 'The Nobel Prize in Physics 2023 
was awarded to Pierre Agostini, Ferenc Krausz and Anne L'Huillier "for 
experimental methods that generate attosecond pulses of light for the study of 
electron dynamics in matter" ... 120 years ago, Marie Skłodowska Curie was the 
first woman to be awarded the Nobel Prize in Physics. I am the fifth. For 
more ...'}, {'type': 'text', 'name': 'Facts on the Nobel Prize in Physics - 
NobelPrize.org', 'url': 'https://www.nobelprize.org/prizes/facts/
facts-on-the-nobel-prize-in-physics/', 'content': 'List of all female Nobel 
Prize laureates. Multiple Nobel Prize laureates in physics. John Bardeen is 
the only person who has received the Nobel Prize in Physics twice, year 1956 
and 1972 . Marie Curie was awarded the Nobel Prize twice, once in physics 1903 
and once in chemistry 1911.. See the list of multiple Nobel Prize laureates 
within other Nobel Prize categories here'}]}
===============================================================================
"""

search_linkup_response = SearchToolkit().search_linkup(
    query="Can you tell me which women were awarded the Physics Nobel Prize",
    depth="standard",
    output_type="sourcedAnswer",
)

print(search_linkup_response)
"""
===============================================================================
{'answer': "The women who have been awarded the Nobel Prize in Physics are: 1. 
Marie Curie - 1903 2. Maria Goeppert Mayer - 1963 3. Donna Strickland - 2018 
4. Anne L'Huillier - 2023", 'sources': [{'name': 'Nobel Prize awarded women - 
NobelPrize.org', 'url': 'https://www.nobelprize.org/prizes/lists/
nobel-prize-awarded-women/', 'snippet': 'The Nobel Prize and the Sveriges 
Riksbank Prize in Economic Sciences in Memory of Alfred Nobel have been 
awarded to women 66 times between 1901 and 2024.'}, {'name': 'Physics Nobel 
Prizes awarded to women | Scientia News', 'url': 'https://www.scientianews.org/
physics-nobel-prize-winners', 'snippet': 'The next female Nobel Prize in 
Physics award winner wouldn't be until another half-century later, with Donna 
Strickland.'}, {'name': 'List of female Nobel laureates - Wikipedia', 'url': 
'https://en.wikipedia.org/wiki/List_of_female_Nobel_laureates', 'snippet': 
"The most recent women to be awarded a Nobel Prize were Han Kang in Literature 
(2024), Claudia Goldin in Economics, Narges Mohammadi for Peace, Anne 
L'Huillier in Physics and Katalin Karikó in Physiology or Medicine (2023)."}]}
===============================================================================
"""


class PersonInfo(BaseModel):
    # Basic company information
    name: str = ""  # Company name
    description: str = ""


search_linkup_response = SearchToolkit().search_linkup(
    query="Can you tell me which women were awarded the Physics Nobel Prize",
    depth="standard",
    output_type="structured",
    structured_output_schema=PersonInfo,
)
print(search_linkup_response)

"""
===============================================================================
{'name': 'Female Nobel Prize Winners in Physics', 'description': 'The women 
awarded the Nobel Prize in Physics include: 1. Marie Curie (1903) 2. Maria 
Goeppert-Mayer (1963) 3. Donna Strickland (2018) 4. (4th winner not mentioned 
in the provided data) 5. (5th winner not mentioned in the provided data). Less 
than 5 women have won the Nobel Prize in Physics out of 225 total laureates.'}
===============================================================================
"""

search_bocha_response = SearchToolkit().search_bocha(
    query="阿里巴巴2024年的esg报告",
    freshness="noLimit",
    summary=False,
    count=10,
)
print(search_bocha_response)

"""
===============================================================================
{"_type":"SearchResponse","queryContext":{"originalQuery":"阿里巴巴2024年的esg报
告"},"webPages":{"webSearchUrl":"","totalEstimatedMatches":8912791,"value":[
{"id":None,"name":"阿里巴巴发布2024年ESG报告持续推进减碳与数字化普惠","url":"ht
tps://www.alibabagroup.com/document-1752073403914780672","displayUrl":"htt
ps://www.alibabagroup.com/document-1752073403914780672","snippet":"阿里巴巴
集团发布《2024财年环境、社会和治理(ESG)报告》(下称"报告"),详细分享过去一年在ESG各方面取
得的进展。报告显示,阿里巴巴扎实推进减碳举措,全集团自身运营净碳排放和价值链碳...","siteName"
:"www.alibabagroup.com","siteIcon":"https://th.bochaai.com/favicon?domain_url=
https://www.alibabagroup.com/document-1752073403914780672","dateLastCrawled":
"2024-07-22T00:00:00Z","cachedPageUrl":None,"language":None,"isFamilyFriendly"
:None,"isNavigational":None},],"someResultsRemoved":true},"images":{"id":None,
"readLink":None,"webSearchUrl":None,"value":[{"webSearchUrl":None,"name":None,
"thumbnailUrl":"http://q7.itc.cn/q_70/images01/20240726/ee26d6fa8658472d8b4c5
e7236b1640a.png","datePublished":None,"contentUrl":"http://q7.itc.cn/q_70/im
ages01/20240726/ee26d6fa8658472d8b4c5e7236b1640a.png","hostPageUrl":"https://
m.sohu.com/a/796245119_121713887/?pvid=000115_3w_a","contentSize":None,"enco
dingFormat":None,"hostPageDisplayUrl":"https://m.sohu.com/a/796245119_121713887
/?pvid=000115_3w_a","width":1285,"height":722,"thumbnail":None}],"isFamilyFrien
dly":None},"videos":None}
===============================================================================
"""


agent = ChatAgent(
    system_message="""You are a helpful assistant that can use baidu search 
        engine to answer questions.""",
    tools=[FunctionTool(SearchToolkit().search_baidu)],
)

usr_msg = "今天北京的天气如何"

response = agent.step(input_message=usr_msg, response_format=None)

print(response.msgs[0].content)

"""
===============================================================================
今天北京的天气信息可以通过以下链接查看:

1. [中国天气网 - 北京天气预报](http://www.baidu.com/link?
url=AJhE9PhEO3TmkJ70CUcRsR3NVB3m6wxN5Imdp0ZVsEBK1t8YhtM6YMxrQy3_vRN6dJv4FLHkBCe
fZURnzHTm9gio-dS4-4MwGVgJe40m7prOoggce2eB0h-3DsllbKMm)
2. [中国天气网 - 北京天气预报](http://www.baidu.com/link?
url=1vhNOfl9tV65_104GMQbDnU_fdCZPXDV2BtTJelxdd6isdSZjAHvtoXqOWG3n7D1N-m9zAmOhQG
c-jEGqiXe9K)
3. [中国天气网 - 北京天气预报](http://www.baidu.com/link?
url=Q0URfpodXDpUe1TKBPpToKIyIuCcjSGUR5jorx81g8Pni5XH-Tbc6AXMa7EwCWjBG3jysTZb43S
6ZCsJOKvPw2EbIlQ_bMu42-5sCraqXlS)
4. [中国天气网 - 北京天气预报一周](http://www.baidu.com/link?
url=TtFe8QryJFuwX1kx50YF5WijRcd2TMJRhPudDQvqW7TG4siah68gUZd_frsVWPi1xkYvrxoYL87
QMH0wSjDYOq)

请点击链接查看详细的天气预报信息。
===============================================================================
"""

bing_call_agent = ChatAgent(
    system_message="""You are a helpful assistant that can use baidu search 
        engine to answer questions.""",
    tools=[FunctionTool(SearchToolkit().search_bing)],
)

bing_usr_msg = "帮忙查询巴黎圣母院最新修复进展"

response = bing_call_agent.step(
    input_message=bing_usr_msg, response_format=None
)

print(response.msgs[0].content)

"""
===============================================================================
以下是关于巴黎圣母院最新修复进展的一些信息:

1. **时隔4年,灾后余生的巴黎圣母院即将重生** -
[知乎](https://zhuanlan.zhihu.com/p/619405504)
   
2. **历时4年,耗资70亿,被烧塌的巴黎圣母院修好了!!** -
[腾讯网](https://news.qq.com/rain/a/20231018A0329F00)

3. **一票难求!巴黎圣母院重新开放!5年修复离不开来自东方的支持** -
[新浪财经](https://finance.sina.com.cn/wm/2024-12-08/doc-incyumnp3384392.shtml)

4. **巴黎圣母院浴火重生!建筑学者:勘探报告近3000页,修复工作复杂** -
[腾讯网](https://news.qq.com/rain/a/20241208A05Q9K00)

这些链接提供了关于巴黎圣母院修复的详细信息和最新进展。
===============================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\searxng_toolkit_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import os
from typing import Optional

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.logger import get_logger
from camel.models import ModelFactory
from camel.toolkits import SearxNGToolkit
from camel.types import ModelPlatformType, ModelType

logger = get_logger(__name__)


def get_searxng_instance() -> Optional[str]:
    r"""Get SearxNG instance URL from environment variable.

    Returns:
        Optional[str]: The SearxNG instance URL if set, None otherwise.
    """
    instance_url = os.getenv('SEARXNG_URL')
    if not instance_url:
        logger.warning(
            "SEARXNG_URL environment variable not set. "
            "Please set it to your SearxNG instance URL."
        )
    return instance_url


def main() -> None:
    r"""Run the SearxNG toolkit example."""
    # Get SearxNG instance URL
    instance_url = get_searxng_instance()
    if not instance_url:
        logger.error(
            "\nTo run this example:"
            "\n1. Find a SearxNG instance (self-host or use public instance)"
            "\n2. Set the SEARXNG_URL environment variable:"
            "\n   export SEARXNG_URL='https://your-searxng-instance.com'"
            "\n\nPublic instances can be found at: https://searx.space"
        )
        return

    # Initialize the toolkit
    searxng_toolkit = SearxNGToolkit(
        searxng_host=instance_url,
        language="en",
        categories=["general", "news"],
        time_range="month",
        safe_search=1,
    )

    # Initialize the model
    model = ModelFactory.create(
        model_type=ModelType.DEFAULT,
        model_platform=ModelPlatformType.DEFAULT,
        model_config_dict=ChatGPTConfig(temperature=0.0).as_dict(),
    )

    # Create chat agent
    system_message = (
        "You are a helpful assistant that can search the web using SearxNG."
    )
    agent = ChatAgent(
        system_message=system_message,
        model=model,
        tools=[*searxng_toolkit.get_tools()],
    )

    # Example search query
    query = "Tell me about the CAMEL AI framework"
    response = agent.step(query)

    # Print the response message content
    print("\nAgent Response:")
    print(response.msgs[0].content)

    # Print tool calls if needed
    print("\nTool Calls:")
    print(response.info['tool_calls'])


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: toolkits\semantic_scholar_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import SemanticScholarToolkit
from camel.types import ModelPlatformType, ModelType

# Define the model, here in this case we use gpt-4o
model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=ChatGPTConfig(temperature=0.0).as_dict(),
)


sys_msg = "You are a helpful assistant"

# Initialize a toolkit
toolkit = SemanticScholarToolkit()
# Get list of tools
tools = toolkit.get_tools()

# Initialize a ChatAgent with your custom tools
camel_agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)

# Description of the added tools
usr_msg = "Describe the tools you've added"

response = camel_agent.step(usr_msg)
print(response.msgs[0].content)

'''
================================================================
1. **fetch_paper_data_title**: This tool fetches a single paper
 based on its title. You can specify which fields to include in
 the response, such as the abstract, authors, year, citation
 count, and more.

2. **fetch_paper_data_id**: Similar to the previous tool,
 this one retrieves a single paper but uses a paper ID instead
 of the title. It also allows for specifying the fields to
 include in the response.

3. **fetch_bulk_paper_data**: This tool allows you to fetch
 multiple papers at once based on a query that can include
 various operators (like AND, OR, NOT). You can filter by
 year and specify which fields to return.

4. **fetch_recommended_papers**: This tool provides
 recommendations for papers based on a list of positively
 and negatively correlated paper IDs. You can specify the
 fields to include in the response and limit the number
 of papers returned.

5. **fetch_author_data**: This tool retrieves information
 about authors based on their IDs. You can specify which
 fields to include in the response, such as the author's name,
 URL, paper count, h-index, and their papers.

These tools can be used individually or in combination to
 gather comprehensive information about academic literature
 and authors.
================================================================
'''

# Search a paper through its id
usr_msg = """search the paper 'Construction of the Literature
    Graph in Semantic Scholar' for me including its paperid"""
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])

'''
================================================================
[FunctionCallingRecord(func_name='fetch_paper_data_title', 
args={'paperTitle': 'Construction of the Literature Graph in
Semantic Scholar', 'fields': 'title,abstract,authors,year,
citationCount,paperId'}, result={'total': 1, 'offset': 0,
'data': [{'paperId': '649def34f8be52c8b66281af98ae884c09aef38b',
'title': 'Construction of the Literature Graph in Semantic
 Scholar', 'abstract': 'We describe a deployed scalable system
for organizing published scientific literature into a 
heterogeneous graph to facilitate algorithmic manipulation and 
discovery. The resulting literature graph consists of more than
 280M nodes, representing papers, authors, entities and various
 interactions between them (e.g., authorships, citations, 
 entity mentions). We reduce literature graph construction into
 familiar NLP tasks (e.g., entity extraction and linking),
 point out research challenges due to differences from standard
 formulations of these tasks, and report empirical results for
 each task. The methods describe
================================================================
'''

# Search a paper through its title
usr_msg = """search the paper with paper id of 
    '649def34f8be52c8b66281af98ae884c09aef38b' for me"""
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])

'''
================================================================
[FunctionCallingRecord(func_name='fetch_paper_data_id', args=
{'paperID': '649def34f8be52c8b66281af98ae884c09aef38b', 
'fields': 'title,abstract,authors,year,citationCount,
publicationTypes,publicationDate,openAccessPdf'}, 
result={'paperId': '649def34f8be52c8b66281af98ae884c09aef38b',
'title': 'Construction of the Literature Graph in Semantic
 Scholar', 'abstract': 'We describe a deployed scalable system
 for organizing published scientific literature into a
 heterogeneous graph to facilitate algorithmic manipulation
 and discovery. The resulting literature graph consists of 
 more than 280M nodes, representing papers, authors, entities
 and various interactions between them (e.g., authorships,
 citations, entity mentions). We reduce literature graph
 construction into familiar NLP tasks (e.g., entity extraction
 and linking), point out research challenges due to differences
 from standard formulations of these tasks, and report
 empirical results for each task. The methods described
 in this paper ar
================================================================
'''

# Search papers through related topic
usr_msg = """search 3 papers with topic related to
    'generative ai' from 2024 for me"""
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])

'''
================================================================
[FunctionCallingRecord(func_name='fetch_bulk_paper_data', 
args={'query': 'generative ai', 'year': '2024-', 'fields':
'title,url,publicationTypes,publicationDate,openAccessPdf'},
result={'total': 9849, 'token': 'PCOA3RZZB2ADADAEYCX2BLJJRDEGL
PUCFA3I5XJAKEAB3YXPGDOTY2GU3WHI4ZMALUMAPUDPHP724CEUVEFKTYRZY5K
LUU53Y5MWWEINIKYZZRC3YT3H4AF7CTSQ', 'data': [{'paperId': 
'0008cd09c0449451b9e6e6de35c29009f0883cd9', 'url': 'https://www
.semanticscholar.org/paper/0008cd09c0449451b9e6e6de35c29009
f0883cd9', 'title': 'A Chitchat on Using ChatGPT for Cheating',
 'openAccessPdf': {'url': 'https://doi.org/10.34074/proc.240106'
 , 'status': 'BRONZE'}, 'publicationTypes': ['Conference'], 
 'publicationDate': '2024-07-24'}, {'paperId': '0013aecf813400
 174158e4f012918c5408f90962', 'url': 'https://www.semanticsc
 holar.org/paper/0013aecf813400174158e4f012918c5408f90962', 
 'title': 'Can novice teachers detect AI-generated texts in EFL
 writing?', 'openAccessPdf': None, 'publicationTypes':
 ['JournalArticle'], 'publicationDate'
================================================================
'''

# Search papers through related topic and operator
usr_msg = """search 2 papers with topic related to
    'ai and bio' from 2024 for me"""
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])

'''
================================================================
[FunctionCallingRecord(func_name='fetch_bulk_paper_data', 
args={'query': 'ai and bio', 'year': '2024-', 'fields': 'title,
url,publicationTypes,publicationDate,openAccessPdf'}, result=
{'total': 207, 'token': None, 'data': [{'paperId': '00c8477a9c
c28b85e4f6da13d2a889c94a955291', 'url': 'https://www.semantics
cholar.org/paper/00c8477a9cc28b85e4f6da13d2a889c94a955291', 
'title': 'Explaining Enterprise Knowledge Graphs with Large
 Language Models and Ontological Reasoning', 'openAccessPdf': 
 None, 'publicationTypes': ['JournalArticle'], 'publicationDate
 ': None}, {'paperId': '01726fbfc8ee716c82b9c4cd70696906d3a4
 46d0', 'url': 'https://www.semanticscholar.org/paper/01726fbfc
 8ee716c82b9c4cd70696906d3a446d0', 'title': 'Study Research 
 Protocol for Phenome India-CSIR Health Cohort Knowledgebase
 (PI-CHeCK): A Prospective multi-modal follow-up study on a 
 nationwide employee cohort.', 'openAccessPdf': {'url': 
 'https://www.medrxiv.org/content/medrxiv/early/2024/10/19/2024
 .10.17.24315252.full.pdf', 'status'
================================================================
'''

# Recommend papers through positive and negative paper id
usr_msg = """recommend 2 papers with positive paper id
    of "02138d6d094d1e7511c157f0b1a3dd4e5b20ebee",
    "018f58247a20ec6b3256fd3119f57980a6f37748" and negative
    paper id of "0045ad0c1e14a4d1f4b011c92eb36b8df63d65bc"
    for me"""
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])

'''
================================================================
[FunctionCallingRecord(func_name='fetch_recommended_papers',
args={'positive_paper_ids': ['02138d6d094d1e7511c157f0b1a3dd4e
5b20ebee', '018f58247a20ec6b3256fd3119f57980a6f37748'], 'negati
ve_paper_ids': ['0045ad0c1e14a4d1f4b011c92eb36b8df63d65bc'], 
'fields': 'title,url,citationCount,authors,publicationTypes,
publicationDate,openAccessPdf', 'limit': 20, 'save_to_file': F
alse}, result={'recommendedPapers': [{'paperId': '9cb202a72171
dc954f8180b42e08da7ab31e16a1', 'url': 'https://www.semanticsc
holar.org/paper/9cb202a72171dc954f8180b42e08da7ab31e16a1', 'tit
le': 'Embrace, Don't Avoid: Reimagining Higher Education with
 Generative Artificial Intelligence', 'citationCount': 0, 'op
 enAccessPdf': {'url': 'https://heca-analitika.com/jeml/arti
 cle/download/233/157', 'status': 'HYBRID'}, 'publicationT
 ypes': ['JournalArticle'], 'publicationDate': '2024-11-2
 8', 'authors': [{'authorId': '1659371967', 'name': 'T. R. N
 oviandy'}, {'authorId': '1657989613', 'name': 'A. Maulan
 a'}, {'authorId': '146805414', 'name
================================================================
'''

# Recommend papers and save the result in a file
usr_msg = """search the authors of author ids of "2281351310",
    "2281342663","2300302076","2300141520" for me"""
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])

'''
================================================================
[FunctionCallingRecord(func_name='fetch_recommended_papers', 
args={'positive_paper_ids': ['02138d6d094d1e7511c157f0b1a3dd4e5
b20ebee', '018f58247a20ec6b3256fd3119f57980a6f37748'], 'negativ
e_paper_ids': ['0045ad0c1e14a4d1f4b011c92eb36b8df63d65bc'],
 'fields': 'title,url,citationCount,authors,publicationTypes,
 publicationDate,openAccessPdf', 'limit': 20, 'save_to_file': T
 rue}, result={'recommendedPapers': [{'paperId': '9cb202a7217
 1dc954f8180b42e08da7ab31e16a1', 'url': 'https://www.semantics
 cholar.org/paper/9cb202a72171dc954f8180b42e08da7ab31e16a1', 
 'title': 'Embrace, Don't Avoid: Reimagining Higher Education
 with Generative Artificial Intelligence', 'citationCount':
 0, 'openAccessPdf': {'url': 'https://heca-analitika.com/jeml
 /article/download/233/157', 'status': 'HYBRID'}, 'publication
 Types': ['JournalArticle'], 'publicationDate': '2024-11-28',
 'authors': [{'authorId': '1659371967', 'name': 'T. R. Novia
 ndy'}, {'authorId': '1657989613', 'name': 'A. Maulana'}, 
 {'authorId': '146805414', 'name'
================================================================
'''

# Search author information through author id
usr_msg = """recommend 2 papers with positive paper id
    of "02138d6d094d1e7511c157f0b1a3dd4e5b20ebee", "018f5
    8247a20ec6b3256fd3119f57980a6f37748" and negative paper
    id of "0045ad0c1e14a4d1f4b011c92eb36b8df63d65bc" for me,
    and please save the result in a file."""
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])

'''
================================================================
[FunctionCallingRecord(func_name='fetch_recommended_papers', 
args={'positive_paper_ids': ['02138d6d094d1e7511c157f0b1a3dd4e5
b20ebee', '018f58247a20ec6b3256fd3119f57980a6f37748'], 'negat
ive_paper_ids': ['0045ad0c1e14a4d1f4b011c92eb36b8df63d65bc'],
 'fields': 'title,url,citationCount,authors,publicationTypes
 ,publicationDate,openAccessPdf', 'limit': 20, 'save_to_file
 ': True}, result={'recommendedPapers': [{'paperId': '9cb20
 2a72171dc954f8180b42e08da7ab31e16a1', 'url': 'https://www.se
 manticscholar.org/paper/9cb202a72171dc954f8180b42e08da7ab31e
 16a1', 'title': 'Embrace, Don't Avoid: Reimagining Higher 
 Education with Generative Artificial Intelligence', 'citat
 ionCount': 0, 'openAccessPdf': {'url': 'https://heca-anali
 tika.com/jeml/article/download/233/157', 'status': 'HYBR
 ID'}, 'publicationTypes': ['JournalArticle'], 'publicatio
 nDate': '2024-11-28', 'authors': [{'authorId': '165937196
 7', 'name': 'T. R. Noviandy'}, {'authorId': '1657989613',
 'name': 'A. Maulana'}, {'authorId': '146805414', 'name'
================================================================
'''

# Search author information and save the result in a file
usr_msg = """search the authors of author ids of "2281351310"
    ,"2281342663","2300302076","2300141520" for me, and please
    save the record in a file."""
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])

'''
================================================================
[FunctionCallingRecord(func_name='fetch_author_data', args=
{'ids': ['2281351310', '2281342663', '2300302076', '230014152
0'], 'fields': 'name,url,paperCount,hIndex,papers', 'save_to_
file': True}, result=[{'authorId': '2281351310', 'url': 'ht
tps://www.semanticscholar.org/author/2281351310', 'name': 'Tho
mas K. F. Chiu', 'paperCount': 3, 'hIndex': 1, 'papers': [{'p
aperId': '218b2e3d3418edff705336a6e0c7f2125be7c562', 'title': N
one}, {'paperId': '630642b7040a0c396967e4dab93cf73094fa4f8f
', 'title': None}, {'paperId': '833ff07d2d1be9be7b12e88487d5631
c141a2e95', 'title': None}]}, {'authorId': '2281342663', 'ur
l': 'https://www.semanticscholar.org/author/2281342663', 'nam
e': 'C. Chai', 'paperCount': 6, 'hIndex': 2, 'papers': [{'pape
rId': '0c70ca68c0239895b0d36abf7f11302cdcf01855', 'title': Non
e}, {'paperId': '218b2e3d3418edff705336a6e0c7f2125be7c562', 't
itle': None}, {'paperId': '7ce699e1cfb81cecf298df6be8eaac8f50
2e0fcc', 'title': None}, {'paperId': '4521b51a8465e69d20a3ae4
b770cf164a180f67b', 'ti
================================================================
'''



--------------------------------------------------------------------------------
# File: toolkits\sympy_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs.openai_config import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import SymPyToolkit
from camel.types import ModelPlatformType, ModelType

# Define system message
sys_msg = """You are a helpful math assistant that can perform symbolic 
computations"""

# Set model config
tools = SymPyToolkit().get_tools()
model_config_dict = ChatGPTConfig(
    temperature=0.0,
).as_dict()

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=model_config_dict,
)

# Set agent
camel_agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)
camel_agent.reset()

# Define a user message with a complex expression
usr_msg = """Simplify the expression: (x^4 - 16)/(x^2 - 4) + sin(x)^2 + cos(x)
^2 + (x^3 + 6*x^2 + 12*x + 8)/(x + 2)"""

# Get response information
response = camel_agent.step(usr_msg)
print(response.info['tool_calls'])
'''
===============================================================================
[ToolCallingRecord(tool_name='simplify_expression', args={'expression': '(x**4 
- 16)/(x**2 - 4) + sin(x)**2 + cos(x)**2 + (x**3 + 6*x**2 + 12*x + 8)/(x + 2)
'}, result='{"status": "success", "result": "2*x**2 + 4*x + 9"}', 
tool_call_id='call_CdoZsLWeagT0yBM13RYuz09W')]
===============================================================================
'''



--------------------------------------------------------------------------------
# File: toolkits\synthesize_function_execution.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from typing import Any, Dict

import requests
from pydantic import BaseModel, Field

from camel.agents import ChatAgent
from camel.toolkits import FunctionTool


# example function
def movie_data_by_id(id: int) -> Dict[str, Any]:
    r"""Fetch movie data by its ID from the IMDB Top 100 Movies API.

    Args:
        id (int): The ID of the movie to retrieve information for.

    Returns:
        Dict[str, Any]: A dictionary with the following keys:
            - rank (int): The rank of the movie in the top 100 list.
            - movie_title (str): The title of the movie.
            - rating (str): The movie's rating.
            - id (str): The unique identifier of the movie.
            - year (int): The release year of the movie.
            - description (str): A brief description of the movie.

    Raises:
        Exception: If an unexpected error occurs while fetching the data.
    """
    try:
        url = f"https://imdb-top-100-movies.p.rapidapi.com/{id}"
        headers = {
            "x-rapidapi-key": "Your API Key",
            "x-rapidapi-host": "imdb-top-100-movies.p.rapidapi.com",
        }
        response = requests.get(url, headers=headers)
        return response.json()
    except Exception as e:
        return {
            "error": str(e),
        }


# Define the response format for movie data
class MovieResponse(BaseModel):
    rating: str = Field(description="The movie's rating.")
    description: str = Field(description="A brief description of the movie.")
    movie_title: str = Field(description="The title of the movie.")


real_get_movie = FunctionTool(movie_data_by_id)
synthesized_get_movie = FunctionTool(movie_data_by_id, synthesize_output=True)

assistant_sys_msg = "You are a helpful assistant."
user_msg = (
    "What is the rating, description and movie_title of the movie with id 2048"
)

print("Synthesize output: False")
real_agent = ChatAgent(assistant_sys_msg, tools=[real_get_movie])
assistant_response = real_agent.step(user_msg)
print(assistant_response.msg.content)


print("\nSynthesize output: True")
synthesized_agent = ChatAgent(assistant_sys_msg, tools=[synthesized_get_movie])
assistant_response = synthesized_agent.step(
    user_msg, response_format=MovieResponse
)
print(assistant_response.msg.content)

"""
===============================================================================
Warning: No synthesize_output_model provided. Use `gpt-4o-mini` to synthesize 
the output.
Synthesize output: False
It seems that I'm unable to access the movie data at the moment due to a 
subscription issue with the API. However, if you provide me with the title of 
the movie or any other details, I can help you find information about it!
===============================================================================
"""

"""
===============================================================================
Synthesize output: True
{'rating': '8.8', 'description': 'A thief who steals corporate secrets through 
the use of dream-sharing technology is given the inverse task of planting an 
idea into the mind of a CEO.', 'movie_title': 'Inception'}
===============================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\terminal_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import os

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import TerminalToolkit
from camel.types import ModelPlatformType, ModelType

# Get current script directory
base_dir = os.path.dirname(os.path.abspath(__file__))
# Define workspace directory for the toolkit
workspace_dir = os.path.join(
    os.path.dirname(os.path.dirname(base_dir)), "workspace"
)

# Define system message
sys_msg = (
    "You are a System Administrator helping with log management tasks. "
    "You have access to terminal tools that can help you execute "
    "shell commands and search files. "
)

# Set model config
tools = TerminalToolkit(working_dir=workspace_dir).get_tools()

model_config_dict = ChatGPTConfig(
    temperature=0.0,
).as_dict()

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=model_config_dict,
)

# Set agent
camel_agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)
camel_agent.reset()

# Define a user message for creating logs directory
usr_msg = (
    f"Create a 'logs' directory in '{workspace_dir}' and list its contents"
)

# Get response information
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])
"""
===============================================================================
[ToolCallingRecord(tool_name='shell_exec', args={'id': 'session1', 'exec_dir': 
'/Users/enrei/Desktop/camel0302/camel/workspace', 'command': 'mkdir logs'}, 
result='', tool_call_id='call_ekWtDhrwxOg20lz55pqLEKvm'), ToolCallingRecord
(tool_name='shell_exec', args={'id': 'session2', 'exec_dir': '/Users/enrei/
Desktop/camel0302/camel/workspace/logs', 'command': 'ls -la'}, result='total 
0\ndrwxr-xr-x  2 enrei  staff   64 Mar 30 04:29 .\ndrwxr-xr-x  4 enrei  staff  
128 Mar 30 04:29 ..\n', tool_call_id='call_FNdkLkvUahtEZUf7YZiJrjfo')]
===============================================================================
"""

# Define a user message for creating log files
usr_msg = (
    f"Create 'app.log' in the logs directory at "
    f"'{os.path.join(workspace_dir, 'logs')}' with content: INFO: Application "
    f"started successfully at 2024-03-10 and show the file content"
)

# Get response information
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])
"""
===============================================================================
[ToolCallingRecord(tool_name='shell_exec', args={'id': 'create_log_file', 
'exec_dir': '/Users/enrei/Desktop/camel0302/camel/workspace/logs', 'command': 
"echo 'INFO: Application started successfully at 2024-03-10' > app.log"}, 
result='', tool_call_id='call_bctQQYnWgAuPp1ga7a7xM6bo'), ToolCallingRecord
(tool_name='shell_exec', args={'id': 'show_log_file_content', 'exec_dir': '/
Users/enrei/Desktop/camel0302/camel/workspace/logs', 'command': 'cat app.
log'}, result='INFO: Application started successfully at 2024-03-10\n', 
tool_call_id='call_wPYJBG3eYrUsjFJYIYYynxuz')]
===============================================================================
"""

# Define a user message for searching in logs
usr_msg = (
    f"Search for 'INFO' keyword in the log file at "
    f"'{os.path.join(workspace_dir, 'logs', 'app.log')}'"
)

# Get response information
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])
"""
===============================================================================
[ToolCallingRecord(tool_name='file_find_in_content', args={'file': '/Users/
enrei/Desktop/camel0302/camel/workspace/logs/app.log', 'regex': 'INFO', 
'sudo': False}, result='INFO: Application started successfully at 2024-03-10',
 tool_call_id='call_PpeRUsldHyg5jSPLZxiGoVfq')]
===============================================================================
"""

# Define a user message for cleaning up logs
usr_msg = (
    f"Remove the 'logs' directory and all its contents in '{workspace_dir}'"
)

# Get response information
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(response.info['tool_calls'])
"""
===============================================================================
[ToolCallingRecord(tool_name='shell_exec', args={'id': 'remove_logs', 
'exec_dir': '/Users/enrei/Desktop/camel0302/camel/workspace', 'command': 'rm 
-rf logs'}, result='', tool_call_id='call_A2kUkVIAhkD9flWmmpTlS9FA')]
===============================================================================
"""

# Define a user message for find the content of the log file
usr_msg = "Find all the files under path `examples/bots`"

# Get response information
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(response.info['tool_calls'])
"""
===============================================================================
[ToolCallingRecord(tool_name='file_find_by_name', args={'path': 'examples/
bots', 'glob': '*'}, result='examples/bots\nexamples/bots/discord_bot.
py\nexamples/bots/discord_bot_installation_management.py\nexamples/bots/
slack_bot_use_msg_queue.py\nexamples/bots/discord_bot_use_msg_queue.
py\nexamples/bots/slack_bot.py', tool_call_id='call_LzRjSotNqKOWwU4yHcstlnG9')]
===============================================================================
"""

# Define a user message for testing resource cleanup via __del__ method
print("\n\n================ Testing Resource Cleanup ================")
usr_msg = (
    "Start a long-running process that sleeps for 300 seconds in the "
    "background, then show me the list of running processes"
)


# Get response information for starting the process
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])
"""
===============================================================================
[ToolCallingRecord(tool_name='shell_exec', args={'id': 'session1', 'exec_dir': 
'/tmp', 'command': 'sleep 300 & echo $!'}, result='Operation restriction: 
Execution path /tmp must be within working directory /home/jjyaoao/openSource/
camel/workspace', tool_call_id='call_G7TcVUJs195Er6yocORHysXP'), 
ToolCallingRecord(tool_name='shell_exec', args={'id': 'session1', 'exec_dir': 
'/home/jjyaoao/openSource/camel/workspace', 'command': 'sleep 300 & echo $!'}, 
result='10804\n', tool_call_id='call_mncQosy3b4cuc1j5MGiltohH'), 
ToolCallingRecord(tool_name='shell_exec', args={'id': 'session2', 'exec_dir': 
'/home/jjyaoao/openSource/camel/workspace', 'command': 'ps aux'}, 
result='USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME 
COMMAND\nroot           1  0.0  0.2 170104 12368 ?        Ss   10:06   0:00 
/sbin/init\nroot           2  0.0  0.0   2776  1928 ?        Sl   10:06   0:00 
/init\nroot           8  0.0  0.0   2776     4 ?        Sl   10:06   0:00 
plan9 --control-socket 7 --log-level=debug --log-file=/dev/null ...',
tool_call_id='call_UvxQrsb1GpfDHTQQc6rLoQ3P')]
===============================================================================
"""
# Define a user message to check if the process was terminated by __del__
usr_msg = "Check if there are any sleep processes running on the system"

# Get response information for checking the processes
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls'])[:1000])
"""
===============================================================================
[ToolCallingRecord(tool_name='shell_exec', args={'id': 'check_sleep_processes',
'exec_dir': '/', 'command': 'ps aux | grep sleep'}, result='Operation 
restriction: Execution path / must be within working directory 
/home/jjyaoao/openSource/camel/workspace', tool_call_id=
'call_gbhmZ3mwpB07uPtVF3FxZaHu'), ToolCallingRecord(tool_name='shell_exec',
args={'id': 'check_sleep_processes', 'exec_dir': 
'/home/jjyaoao/openSource/camel/workspace', 'command': 'ps aux | grep sleep'}, 
result='root       11385  0.0  0.0   2620   532 pts/4    S+   11:16   0:00 
/bin/sh -c ps aux | grep sleep\nroot       11387  0.0  0.0   8172   656 pts/4  
S+   11:16   0:00 grep sleep\n', tool_call_id='call_gSZqRaqNAtYjUXOfvVuaObw2')]
===============================================================================
"""

usr_msg = "help me use uv pip install pptx, and create a ppt, and show me the"
" output of the terminal"

# Get response information for checking the processes
camel_agent.reset()
response = camel_agent.step(usr_msg)
print(str(response.info['tool_calls']))



--------------------------------------------------------------------------------
# File: toolkits\thinking_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import ThinkingToolkit
from camel.types import ModelPlatformType, ModelType

# Create a Model
model_config_dict = ChatGPTConfig(temperature=0.0).as_dict()
model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=model_config_dict,
)

# Initialize the ThinkingToolkit
thinking_toolkit = ThinkingToolkit()
tools = thinking_toolkit.get_tools()

# Set up the ChatAgent with thinking capabilities
sys_msg = (
    "You are an assistant that can break down complex problems and think "
    "through solutions step by step. Use the thinking toolkit to organize "
    "your thoughts, reflect on the problem, and create plans."
)

agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)

# Example: Problem solving with thinking toolkit
print("\nExample: Problem solving with thinking toolkit")
print("=" * 80)

usr_msg = """
Help me solve this math problem:
If a train travels at 60 mph and needs to cover 300 miles, 
with 3 stops of 15 minutes each, how long will the journey take?
"""

response = agent.step(usr_msg)
print(response.msgs[0].content)
print("\nTool calls:")
print(response.info['tool_calls'])

"""
Example: Problem Solving with Thinking Toolkit
===============================================================================
The train's total journey time for traveling 300 miles at 60 mph, with 
3 stops of 15 minutes each, is 5.75 hours. This consists of 5 hours of 
travel time and 0.75 hours (or 45 minutes) of stop time. The conversion 
of stop time from minutes to hours was explicitly noted for clarity.

Tool Calls:
[
    ToolCallingRecord(
        tool_name='plan',
        args={
            'plan': '1. Compute the travel time for 300 miles at 60 mph '
                    'without stops.\n'
                    '2. Determine the total stop time.\n'
                    '3. Sum the travel time and stop time to get the total '
                    'journey duration.'
        },
        result='Plan: 1. Compute the travel time for 300 miles at 60 mph '
               'without stops.\n'
               '2. Determine the total stop time.\n'
               '3. Sum the travel time and stop time to get the total journey '
               'duration.',
        tool_call_id='call_kKYeTFLMGPf0mhimAZ8hapFk'
    ),
    ToolCallingRecord(
        tool_name='think',
        args={
            'thought': 'Using the formula time = distance / speed, where '
                       'distance = 300 miles and speed = 60 mph, we can '
                       'determine the travel time.'
        },
        result='Thought: Using the formula time = distance / speed, where '
               'distance = 300 miles and speed = 60 mph, we can determine '
               'the travel time.',
        tool_call_id='call_t3DXWahikwhc8ps0y2GTE9ko'
    ),
    ToolCallingRecord(
        tool_name='think',
        args={
            'thought': 'The total stop time is calculated as: number of '
                       'stops * time per stop, which is 3 * 15 minutes.'
        },
        result='Thought: The total stop time is calculated as: number of '
               'stops * time per stop, which is 3 * 15 minutes.',
        tool_call_id='call_MM1YlTPmiMhhiy6HWqraKh8E'
    ),
    ToolCallingRecord(
        tool_name='hypothesize',
        args={
            'hypothesis': 'The travel time for 300 miles at 60 mph should '
                          'be 5 hours.'
        },
        result='Hypothesis: The travel time for 300 miles at 60 mph should '
               'be 5 hours.',
        tool_call_id='call_F16dfESrJmUDwieYDA2aCheB'
    ),
    ToolCallingRecord(
        tool_name='hypothesize',
        args={
            'hypothesis': 'The total stop time for 3 stops of 15 minutes '
                          'each should be 45 minutes.'
        },
        result='Hypothesis: The total stop time for 3 stops of 15 minutes '
               'each should be 45 minutes.',
        tool_call_id='call_coxWcLPATfKNdiqQDz853pm4'
    ),
    ToolCallingRecord(
        tool_name='synthesize',
        args={
            'synthesis': 'The total journey time for the train traveling '
                         '300 miles at 60 mph, with 3 stops of 15 minutes '
                         'each, is 5.75 hours. This includes 5 hours of '
                         'travel time and 0.75 hours (or 45 minutes) of '
                         'stop time. The conversion of stop time from '
                         'minutes to hours was explicitly noted for clarity.'
        },
        result='Synthesis: The total journey time for the train traveling '
               '300 miles at 60 mph, with 3 stops of 15 minutes each, is '
               '5.75 hours. This includes 5 hours of travel time and 0.75 '
               'hours (or 45 minutes) of stop time. The conversion of stop '
               'time from minutes to hours was explicitly noted for clarity.',
        tool_call_id='call_9AHg54snm17XN7Mj1UzgSV04'
    )
]
===============================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\toolkit_timeout.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========


# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import time
from typing import Optional

from camel.toolkits.base import BaseToolkit
from camel.utils import with_timeout


# Example 1: Basic function with timeout
@with_timeout(1.0)
def basic_function() -> str:
    r"""A basic function with a 1-second timeout."""
    time.sleep(0.5)  # Simulating some work
    return "Basic function completed successfully!"


# Example 2: Function that exceeds timeout
@with_timeout(1.0)
def slow_function() -> str:
    r"""A slow function that will exceed the timeout."""
    time.sleep(2.0)  # This will exceed the timeout
    return "This message will never be returned"


# Example 3: Class with configurable timeout
class TimeoutExample:
    def __init__(self, timeout: Optional[float] = None):
        self.timeout = timeout

    @with_timeout()  # Uses instance timeout
    def instance_timeout_method(self) -> str:
        r"""Method using the instance's timeout value."""
        time.sleep(0.5)
        return "Instance timeout method completed!"

    @with_timeout(0.1)  # Uses decorator-specific timeout
    def decorator_timeout_method(self) -> str:
        r"""Method using the decorator's timeout value."""
        time.sleep(0.5)
        return "This will timeout"


# Example 4: Toolkit with timeout
class TimeoutToolkit(BaseToolkit):
    def __init__(self, timeout: Optional[float] = None):
        super().__init__(timeout=timeout)

    @with_timeout()
    def fast_operation(self) -> str:
        r"""A fast operation that completes within timeout."""
        time.sleep(0.1)
        return "Fast operation completed!"

    @with_timeout()
    def slow_operation(self) -> str:
        r"""A slow operation that exceeds timeout."""
        time.sleep(1.0)
        return "Slow operation completed!"


def main():
    # Example 1: Basic function
    print("\nExample 1: Basic function")
    print(basic_function())

    # Example 2: Slow function
    print("\nExample 2: Slow function")
    print(slow_function())

    # Example 3: Class with timeout
    print("\nExample 3: Class with timeout")
    example = TimeoutExample(timeout=0.2)
    print(example.instance_timeout_method())
    print(example.decorator_timeout_method())

    # Example 4: Toolkit
    print("\nExample 4: Toolkit with timeout")
    toolkit = TimeoutToolkit(timeout=0.5)
    print(toolkit.fast_operation())
    print(toolkit.slow_operation())


if __name__ == "__main__":
    main()

"""
===============================================================================
Example 1: Basic function
Basic function completed successfully!

Example 2: Slow function
Function `slow_function` execution timed out, exceeded 1.0 seconds.

Example 3: Class with timeout
Function `instance_timeout_method` execution timed out, exceeded 0.2 seconds.
Function `decorator_timeout_method` execution timed out, exceeded 0.1 seconds.

Example 4: Toolkit with timeout
Fast operation completed!
Function `slow_operation` execution timed out, exceeded 0.5 seconds.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\video_analysis_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========


from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import VideoAnalysisToolkit
from camel.types import ModelPlatformType, ModelType

model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=ChatGPTConfig(
        temperature=0.0,
    ).as_dict(),
)

video_model = ModelFactory.create(
    model_platform=ModelPlatformType.OPENAI,
    model_type=ModelType.GPT_4O_MINI,
    model_config_dict=ChatGPTConfig(
        temperature=0.0,
    ).as_dict(),
)

# Initialize the VideoAnalysisToolkit with the model
# Note: Audio transcription is disabled for faster processing
video_toolkit = VideoAnalysisToolkit(
    model=video_model,
    use_audio_transcription=False,
)

# Create an agent with the video toolkit's tools
agent = ChatAgent(
    system_message="You are a helpful assistant that can analyze videos.",
    model=model,
    tools=[*video_toolkit.get_tools()],
)

# Example video URL (Very short sample video)
video_url = "https://www.youtube.com/watch?v=kQ_7GtE529M"
question = "What is shown in the first few seconds of this video?"

# Use the toolkit directly for faster processing with fewer frames
print("Analyzing video...")
result = video_toolkit.ask_question_about_video(
    video_path=video_url,
    question=question,
    num_frames=5,  # Extract only 5 frames for faster processing
)

print("Video Analysis Result:")
print("-" * 50)
print(result)
print("-" * 50)
"""
==========================================================================
Analyzing video...
[youtube] Extracting URL: https://www.youtube.com/watch?v=kQ_7GtE529M
[youtube] kQ_7GtE529M: Downloading webpage
[youtube] kQ_7GtE529M: Downloading ios player API JSON
[youtube] kQ_7GtE529M: Downloading mweb player API JSON
[youtube] kQ_7GtE529M: Downloading m3u8 information
[info] kQ_7GtE529M: Downloading 1 format(s): 247+251
[download] Destination: /private/var/folders/93/f_71_t957cq9cmq2gsybs4_40000gn/
T/tmp4plhd3s3/Douchebag Bison.f247.webm
[download] 100% of    1.95MiB in 00:00:01 at 1.18MiB/s
[download] Destination: /private/var/folders/93/f_71_t957cq9cmq2gsybs4_40000gn/
T/tmp4plhd3s3/Douchebag Bison.f251.webm
[download] 100% of  303.08KiB in 00:00:00 at 490.62KiB/s
[Merger] Merging formats into "/private/var/folders/93/
f_71_t957cq9cmq2gsybs4_40000gn/T/tmp4plhd3s3/Douchebag Bison.webm"
Deleting original file /private/var/folders/93/f_71_t957cq9cmq2gsybs4_40000gn/
T/tmp4plhd3s3/Douchebag Bison.f251.webm (pass -k to keep)
Deleting original file /private/var/folders/93/f_71_t957cq9cmq2gsybs4_40000gn/
T/tmp4plhd3s3/Douchebag Bison.f247.webm (pass -k to keep)
2025-03-09 21:17:08,036 - pyscenedetect - ERROR - VideoManager is deprecated 
and will be removed.
2025-03-09 21:17:08,060 - pyscenedetect - INFO - Loaded 1 video, framerate: 30.
000 FPS, resolution: 1280 x 720
2025-03-09 21:17:08,061 - pyscenedetect - INFO - Duration set, start: None, 
duration: None, end: None.
2025-03-09 21:17:08,061 - pyscenedetect - INFO - Detecting scenes...
2025-03-09 21:17:09,065 - camel.camel.toolkits.video_analysis_toolkit - 
WARNING - No scenes detected in video, capturing frames at regular intervals
Video Analysis Result:
--------------------------------------------------
### Visual Analysis

1. **Identified Entities**:
   - **Wolves**: Multiple wolves are visible in the frames, characterized by 
   their grayish fur, slender bodies, and bushy tails. They appear to be in a 
   pack, indicating social behavior.
   - **Bison**: A bison is present, identifiable by its large size, shaggy 
   brown fur, and distinctive hump on its back. The bison is significantly 
   larger than the wolves.

2. **Key Attributes**:
   - **Wolves**: 
     - Size: Smaller than the bison, typically around 26-32 inches tall at the 
     shoulder.
     - Color: Predominantly gray with some variations in fur color.
     - Behavior: The wolves are shown moving in a coordinated manner, 
     suggesting they are hunting or scavenging.
   - **Bison**:
     - Size: Much larger, can weigh up to 2,000 pounds.
     - Color: Dark brown, with a thick coat.
     - Behavior: The bison appears to be stationary or moving slowly, possibly 
     in a defensive posture.

3. **Groupings and Interactions**:
   - The wolves are seen surrounding the bison, indicating a predatory 
   behavior. The interaction suggests a hunting scenario, where the wolves are 
   attempting to take down or scavenge from the bison.

### Audio Integration
- **No audio transcription available**: Therefore, the analysis relies solely 
on visual observations.

### Detailed Reasoning and Justification
- **Identification of Species**:
  - The wolves are identified by their physical characteristics and social 
  behavior, which is typical of pack animals. Their movement patterns and 
  proximity to the bison indicate a hunting strategy.
  - The bison is easily distinguishable due to its size and unique physical 
  features, such as the hump and thick fur.

### Comprehensive Answer
- **Total Number of Distinct Species**: 2 (Wolves and Bison)
- **Defining Characteristics**:
  - **Wolves**: Gray fur, slender build, social behavior in a pack.
  - **Bison**: Large size, shaggy brown fur, distinctive hump.

### Important Considerations
- The wolves exhibit coordinated movement, which is crucial for hunting, while 
the bison's size and defensive posture highlight its role as prey in this 
scenario. The visual cues of size, color, and behavior effectively distinguish 
these two species in the context of a predatory interaction.
==========================================================================
"""



--------------------------------------------------------------------------------
# File: toolkits\zapier_toolkit.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.configs.openai_config import ChatGPTConfig
from camel.models import ModelFactory
from camel.toolkits import ZapierToolkit
from camel.types import ModelPlatformType, ModelType

# Define system message
sys_msg = """You are a helpful AI assistant that can use Zapier AI tools to 
perform various tasks. When using tools, first list the available tools using 
list_actions, then use the appropriate tool based on the task. Always provide 
clear explanations of what you're doing."""

# Set model config
tools = ZapierToolkit().get_tools()
model_config_dict = ChatGPTConfig(
    temperature=0.0,
).as_dict()


model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
    model_config_dict=model_config_dict,
)

# Set agent
camel_agent = ChatAgent(
    system_message=sys_msg,
    model=model,
    tools=tools,
)
camel_agent.reset()

# First, list available tools
usr_msg = "First, list all available Zapier tools."
response = camel_agent.step(usr_msg)
print("Available Tools:")
print(response.msg.content)
print("\n" + "=" * 80 + "\n")

# Now, use the translation tool
usr_msg = """Now that we can see the translation tool is available, please 
use it to translate 'hello camel' from en to zh. Use 
the tool ID from the list above and make sure to specify the language codes 
correctly in the instructions."""
response = camel_agent.step(usr_msg)
print("Translation Result:")
print(response.msg.content)

"""
===============================================================================
Here are the available Zapier tools:

1. **Gmail: Find Email**
   - **ID:** 0d82cfd3-2bd7-4e08-9f3d-692719e81a26
   - **Description:** This action allows you to find an email in Gmail based 
        on a search string.
   - **Parameters:**
     - `instructions`: Instructions for executing the action.
     - `Search_String`: The string to search for in the emails.

2. **Translate by Zapier: Translate Text**
   - **ID:** f7527450-d7c7-401f-a764-2f69f622e7f3
   - **Description:** This action translates text into a specified target 
        language.
   - **Parameters:**
     - `instructions`: Instructions for executing the action.
     - `Text`: The text to be translated.
     - `Target_Language`: The language to translate the text into.

If you need to perform a specific task using one of these tools, please let me 
know!

================================================================================

Translation Result:
The translation of "hello camel" from English to Chinese (zh) is:

**Translation:** 你好骆驼

- **Source Language:** English (en)
- **Target Language:** Chinese (zh)

If you need any further assistance or additional translations, feel free to 
ask!
===============================================================================
"""



--------------------------------------------------------------------------------
# File: translation\translator.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import argparse
import codecs
import json
import multiprocessing
import os
import os.path as osp
import warnings

from camel.agents import ChatAgent
from camel.configs import ChatGPTConfig
from camel.generators import SystemMessageGenerator
from camel.models import ModelFactory
from camel.types import (
    ModelPlatformType,
    ModelType,
    RoleType,
    TaskType,
)

warnings.filterwarnings("ignore")

language_list = [
    "arabic",
    "chinese",
    "french",
    "german",
    "hindi",
    "italian",
    "japanese",
    "korean",
    "russian",
    "spanish",
]

parser = argparse.ArgumentParser(description='Arguments for translation.')
parser.add_argument(
    '--directory_path',
    type=str,
    help='Directory that contains original json files',
    default='../camel_data/ai_society',
)
parser.add_argument(
    '--save_directory_path',
    type=str,
    help='Directory to save translated files',
    default='../camel_data/ai_society_translated',
)
parser.add_argument(
    '--single',
    action='store_true',
    help='Run translator in a non-parallel way.',
)
parser.add_argument(
    '--stream',
    action='store_true',
    help='Set OpenAI GPT model with the stream mode.',
)
parser.add_argument(
    '--language',
    type=str,
    help='Language you want to translated to. '
    'Notice that this is not used in the parallel mode, '
    'which uses SLURM_ARRAY_TASK_ID to indicate the '
    'language to be translated.',
    choices=language_list,
    default='arabic',
)


def translate_content(
    args: argparse.Namespace, file_path: str, language: str
) -> None:
    # Extract file name from the .json file path to be translated
    file_name = osp.splitext(osp.basename(file_path))[0]

    if not osp.exists(args.save_directory_path):
        os.makedirs(args.save_directory_path)

    save_lang_director_path = osp.join(args.save_directory_path, language)
    if not osp.exists(save_lang_director_path):
        os.makedirs(save_lang_director_path)

    # Check that file_name.json does not exist in the save directory
    save_path = osp.join(save_lang_director_path, f'{file_name}.json')
    if osp.exists(save_path):
        return

    # Load the json file
    with open(file_path, "r") as json_file:
        json_data = json.load(json_file)

    # Translate the content of each message in the json
    for i in range(json_data['num_messages']):
        msg_i_content = (
            "Sentence to translate: " + json_data[f"message_{i+1}"]["content"]
        )

        sys_msg_generator = SystemMessageGenerator(
            task_type=TaskType.TRANSLATION
        )

        assistant_sys_msg = sys_msg_generator.from_dict(
            meta_dict=dict(language=language.capitalize()),
            role_tuple=('Language Translator', RoleType.ASSISTANT),
        )

        if not args.stream:
            model_config = ChatGPTConfig(stream=False)
        else:
            model_config = ChatGPTConfig(stream=True)

        model = ModelFactory.create(
            model_platform=ModelPlatformType.DEFAULT,
            model_type=ModelType.DEFAULT,
            model_config=model_config,
        )

        assistant_agent = ChatAgent(
            system_message=assistant_sys_msg,
            model=model,
        )

        user_msg = msg_i_content

        assistant_response = assistant_agent.step(user_msg)
        assistant_msg = assistant_response.msg

        json_data[f"message_{i+1}"]["content"] = assistant_msg.content

    with codecs.open(save_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=4)


def main(args: argparse.Namespace) -> None:
    if not args.single:
        # Get the language to translate based on Slurm array index
        slum_id_env = "SLURM_ARRAY_TASK_ID"
        try:
            language_index = int(os.environ[slum_id_env])
        except KeyError:
            print(f"{slum_id_env} not found. Defaulting to 0 (i.e Arabic)")
            # Default to Arabic, you can change to any other language
            language_index = 0
        # List of languages to translate to
        language_list = [
            "arabic",
            "chinese",
            "french",
            "german",
            "hindi",
            "italian",
            "japanese",
            "korean",
            "russian",
            "spanish",
        ]
        language = language_list[language_index]
    else:
        language = args.language

    # Get list of all .json files paths
    json_file_paths = []

    for filename in os.listdir(args.directory_path):
        if filename.endswith(".json"):
            file_path = osp.join(args.directory_path, filename)
            json_file_paths.append(file_path)

    if not args.single:
        pool = multiprocessing.Pool()
        # Apply parallel translation to all .json files
        for file_path in json_file_paths:
            pool.apply_async(
                translate_content, args=(args, file_path, language)
            )
        pool.close()
        pool.join()
    else:
        for file_path in json_file_paths:
            translate_content(args, file_path, language)


if __name__ == "__main__":
    args = parser.parse_args()
    main(args=args)



--------------------------------------------------------------------------------
# File: verifier\math_verifier_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

import asyncio

from camel.logger import get_logger
from camel.verifiers import MathVerifier

logger = get_logger(__name__)

# Initialize verifier with configuration
verifier = MathVerifier(float_rounding=6, numeric_precision=15)


async def main():
    r"""Run test cases demonstrating different verification scenarios."""

    print("\nStarting Math Verifier Examples\n")
    await verifier.setup()

    try:
        # Test case 1: Basic numerical equivalence (should succeed)
        print("=== Test 1: Basic Numerical ===")
        result = await verifier.verify(
            solution="0.333333", reference_answer="1/3"
        )
        print("Input: 0.333333 ≈ 1/3")
        print(f"Result: {result.status}")
        print(
            f"err: {result.error_message if result.error_message else 'None'}"
        )

        # Test case 2: LaTeX expressions (should succeed)
        print("=== Test 2: LaTeX Expression ===")
        result = await verifier.verify(
            solution=r"$\frac{1}{2}$", reference_answer=r"0.5"
        )
        print("Input: \\frac{1}{2} = 0.5")
        print(f"Result: {result.status}")
        print(
            f"err: {result.error_message if result.error_message else 'None'}"
        )

        # Test case 3: Deliberate mismatch (should fail)
        print("=== Test 3: Expected Failure ===")
        result = await verifier.verify(
            solution="0.5", reference_answer="0.3333"
        )
        print("Input: 0.5 ≠ 0.3333")
        print(f"Result: {result.status}")
        print(
            f"err: {result.error_message if result.error_message else 'None'}"
        )

    finally:
        await verifier.cleanup()
        print("Math Verifier Examples Completed")


if __name__ == "__main__":
    asyncio.run(main())

"""
===============================================================================
Starting Math Verifier Examples

=== Test 1: Basic Numerical ===
Input: 0.333333 ≈ 1/3
Result: VerificationOutcome.SUCCESS
err: None

=== Test 2: LaTeX Expression ===
Input: \frac{1}{2} = 0.5
Result: VerificationOutcome.SUCCESS
err: None

=== Test 3: Expected Failure ===
Input: 0.5 ≠ 0.3333
Result: VerificationOutcome.FAILURE
err: Solution does not match ground truth

Math Verifier Examples Completed
===============================================================================
"""



--------------------------------------------------------------------------------
# File: verifier\python_verifier_example.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import asyncio

from camel.verifiers import PythonVerifier

verifier = PythonVerifier(required_packages=["numpy"])
asyncio.run(verifier.setup(uv=True))

numpy_test_code = """
import numpy as np
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
result = np.dot(a, b)
print(result)
"""


# Since the output of the above numpy code evaluates to 32,
# we expect the verification outcome to be a success.
result = asyncio.run(
    verifier.verify(solution=numpy_test_code, reference_answer="32")
)
print(f"Result: {result}")

result = asyncio.run(
    verifier.verify(solution=numpy_test_code, reference_answer="40")
)

# Now we expect the VerificationOutcome to be a failure,
# because the answer is wrong.
print(f"Result: {result}")

asyncio.run(verifier.cleanup())



--------------------------------------------------------------------------------
# File: vision\duckduckgo_video_object_recognition.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from typing import List

from PIL import Image

from camel.agents import ChatAgent
from camel.messages import BaseMessage
from camel.prompts import PromptTemplateGenerator
from camel.toolkits import SearchToolkit, VideoDownloaderToolkit
from camel.types import RoleType, TaskType


def detect_image_obj(image_list: List[Image.Image]) -> None:
    sys_msg = PromptTemplateGenerator().get_prompt_from_key(
        TaskType.OBJECT_RECOGNITION, RoleType.ASSISTANT
    )
    print("=" * 20 + " SYS MSG " + "=" * 20)
    print(sys_msg)
    print("=" * 49)
    agent = ChatAgent(sys_msg)

    user_msg = BaseMessage.make_user_message(
        role_name="User",
        content="Please start the object detection for the following images!",
        image_list=image_list,
        image_detail="high",
    )

    assistant_response = agent.step(user_msg)
    print("=" * 20 + " RESULT " + "=" * 20)
    print(assistant_response.msgs[0].content)
    print("=" * 48)


def main():
    # Create an instance of the SearchToolkit
    search_toolkit = SearchToolkit()

    # Example query for DuckDuckGo video search
    query = "The future of AI in education"

    # Perform a DuckDuckGo search with the query, setting source to 'videos'
    results = search_toolkit.search_duckduckgo(
        query=query, source="videos", max_results=5
    )

    # Try to download videos from the search results
    for result in results:
        video_url = result['embed_url']
        if not video_url:
            print(f"No valid video URL provided for result: {result}")
            continue

        print(f"Trying to download video from: {video_url}")
        downloader = VideoDownloaderToolkit()
        image_list = downloader.get_video_screenshots(video_url, 3)
        if image_list and len(image_list) > 0:
            print(
                f'''Successfully downloaded video and captured screenshots 
                from: {video_url}'''
            )
            detect_image_obj(image_list)
            print("Stopping further downloads as we found valid images.")
            break
        else:
            print(f"Failed to capture screenshots from video: {video_url}")

    print("Exited the video download loop.")


if __name__ == "__main__":
    main()

"""
===============================================================================
Successfully downloaded video and captured screenshots 
                from: https://www.youtube.com/embed/RRMVF0PPqZI?autoplay=1
==================== SYS MSG ====================
You have been assigned an object recognition task.
Your mission is to list all detected objects in following image.
Your output should always be a list of strings starting with `1.`, `2.` etc.
Do not explain yourself or output anything else.
=================================================
==================== RESULT ====================
1. Drone
2. Hangar
3. Person (in uniform)
4. Plants
5. Wall (brick)
6. Table
7. Electrical panels
8. Lights
9. Floor
================================================
Stopping further downloads as we found valid images.
Exited the video download loop.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: vision\image_crafting.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents.chat_agent import ChatAgent
from camel.models import ModelFactory
from camel.prompts import PromptTemplateGenerator
from camel.toolkits import DalleToolkit
from camel.types import (
    ModelPlatformType,
    ModelType,
    RoleType,
    TaskType,
)


def main():
    sys_msg = PromptTemplateGenerator().get_prompt_from_key(
        TaskType.IMAGE_CRAFT, RoleType.ASSISTANT
    )
    print("=" * 20 + " SYS MSG " + "=" * 20)
    print(sys_msg)
    print("=" * 49)

    model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
    )

    dalle_agent = ChatAgent(
        system_message=sys_msg,
        model=model,
        tools=DalleToolkit().get_tools(),
    )

    response = dalle_agent.step("Draw a picture of a camel.")

    print("=" * 20 + " RESULT " + "=" * 20)
    print(response.msg.content)
    print("=" * 48)


if __name__ == "__main__":
    main()

"""
===============================================================================
==================== SYS MSG ====================
You are tasked with creating an original image based on
        the provided descriptive captions. Use your imagination
        and artistic skills to visualize and draw the images and
        explain your thought process.
=================================================
==================== RESULT ====================
I have created an image of a camel standing in a desert oasis under the shade 
of a palm tree. You can see the realistic and detailed drawing of the camel in 
the image below. 

![Camel in a Desert Oasis](img/58a2a3fa-1e7e-407c-8cd6-4b99448b6a90.png) 

The scene captures the essence of the desert environment with the camel 
peacefully resting in the oasis.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: vision\multi_condition_image_crafting.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from PIL import Image

from camel.agents.chat_agent import ChatAgent
from camel.generators import PromptTemplateGenerator
from camel.messages.base import BaseMessage
from camel.models import ModelFactory
from camel.toolkits import DalleToolkit
from camel.types import (
    ModelPlatformType,
    ModelType,
    RoleType,
    TaskType,
)


def main(image_paths: list[str]) -> list[str]:
    sys_msg = PromptTemplateGenerator().get_prompt_from_key(
        TaskType.MULTI_CONDITION_IMAGE_CRAFT, RoleType.ASSISTANT
    )
    print("=" * 20 + " SYS MSG " + "=" * 20)
    print(sys_msg)
    print("=" * 49)

    model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
    )

    dalle_agent = ChatAgent(
        system_message=sys_msg,
        model=model,
        tools=DalleToolkit().get_tools(),
    )

    image_list = [Image.open(image_path) for image_path in image_paths]

    user_msg = BaseMessage.make_user_message(
        role_name="User",
        content='''Please generate an image based on the provided images and 
        text, make the backgroup of this image is in the morning''',
        image_list=image_list,
    )

    response = dalle_agent.step(user_msg)

    print("=" * 20 + " RESULT " + "=" * 20)
    print(response.msg.content)
    print("=" * 48)


if __name__ == "__main__":
    main()

"""
===============================================================================
==================== SYS MSG ====================
You are tasked with creating an image based on
        the provided text and images conditions. Please use your
        imagination and artistic capabilities to visualize and
        draw the images and explain what you are thinking about.
=================================================
==================== RESULT ====================
Here is the generated image of a serene desert scene in the morning:

![Morning Desert Scene](img/3d8310e8-9f14-48be-94db-c66dd0461cd0.png)

The scene features a camel standing on a sand dune, palm trees, and an oasis 
in the background. The sun is rising, casting a soft golden light over the 
landscape with clear skies and a few scattered clouds.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: vision\multi_turn_image_refining.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import re
from copy import deepcopy

from colorama import Fore
from PIL import Image

from camel.agents import ChatAgent
from camel.messages import BaseMessage
from camel.models import ModelFactory
from camel.prompts import PromptTemplateGenerator
from camel.responses import ChatAgentResponse
from camel.toolkits import DalleToolkit
from camel.types import (
    ModelPlatformType,
    ModelType,
    RoleType,
    TaskType,
)
from camel.utils import print_text_animated


class MMChat:
    r"""The class of multimodal chat session.

    NOTE: Currently this example doesn't work properly, since the generated
    image is not included in the response message. Need to add support to
    include Image in response message.
    """

    def __init__(
        self,
    ) -> None:
        self.critic = None
        self.artist = None
        critic_sys = """You need to describe what you see in the figure
and improve the prompt of it.
Reply with the following format:

CRITICS: the image needs to improve...
PROMPT: here is the updated prompt!
        """
        self.critic_sys_msg = BaseMessage.make_assistant_message(
            role_name='Critic', content=critic_sys
        )

        self.artist_sys_msg = BaseMessage.make_assistant_message(
            role_name="Artist",
            content=PromptTemplateGenerator().get_prompt_from_key(
                TaskType.MULTI_CONDITION_IMAGE_CRAFT, RoleType.ASSISTANT
            ),
        )

        self.init_agents()

    def init_agents(self):
        r"""Initialize artist and critic agents with their system messages."""
        model = ModelFactory.create(
            model_platform=ModelPlatformType.DEFAULT,
            model_type=ModelType.DEFAULT,
        )

        self.artist = ChatAgent(
            system_message=self.artist_sys_msg,
            model=model,
            tools=DalleToolkit().get_tools(),
        )

        self.artist.reset()

        self.critic = ChatAgent(
            system_message=self.critic_sys_msg, model=model
        )
        self.critic.reset()

    def step(self, initialPrompt: str, iter_num=2) -> ChatAgentResponse:
        r"""Process of the drawing and criticising.

        Returns:
            ChatAgentResponse: it contains the response message of
            the artist agent in the last iteration.

        """

        artist_user_msg = BaseMessage.make_user_message(
            role_name="User", content=initialPrompt
        )
        print(
            Fore.MAGENTA
            + "=" * 10
            + "ARTIST SYS"
            + "=" * 10
            + "\n"
            + self.artist_sys_msg.content
        )
        print(
            Fore.YELLOW
            + "=" * 10
            + "ARTIST USR"
            + "=" * 10
            + "\n"
            + artist_user_msg.content
        )

        pattern = r'''\(.*?/([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-
        [0-9a-fA-F]{4}-[0-9a-fA-F]{12})(\.jpg|\.png)\)'''
        response = self.artist.step(artist_user_msg)
        matches = re.findall(pattern, response.msg.content)

        image_paths = [f"./img/{uuid}{ext}" for uuid, ext in matches]
        tmp_paths = deepcopy(image_paths)
        response_msg = re.sub(
            pattern,
            lambda x: "(" + image_paths.pop(0) + ")",
            response.msg.content,
        )
        image_paths = deepcopy(tmp_paths)

        print_text_animated(
            Fore.BLUE
            + "=" * 10
            + "ARTIST RES"
            + "=" * 10
            + "\n"
            + response_msg
        )
        print(response_msg)

        i = 0
        while i < iter_num:
            i += 1
            # Resize the image to 128x128
            resized_imgs = [
                Image.open(image_path).resize(
                    (128, 128), Image.Resampling.LANCZOS
                )
                for image_path in image_paths
            ]
            # Save for maintaining the image format
            [
                img.save(f"tmp_{i}.png", "PNG")
                for i, img in enumerate(resized_imgs)
            ]
            saved = [f"tmp_{i}.png" for i in range(len(resized_imgs))]
            image_list = [Image.open(image) for image in saved]

            critic_user_msg = BaseMessage.make_user_message(
                role_name="User",
                content="image:",
                image_list=image_list,
                image_detail="low",
            )
            print(
                Fore.GREEN
                + "=" * 10
                + "CRITIC SYS"
                + "=" * 10
                + "\n"
                + self.critic_sys_msg.content
            )
            print(
                Fore.RED
                + "=" * 10
                + "CRITIC USR"
                + "=" * 10
                + "\n"
                + critic_user_msg.content
            )
            prompt = self.critic.step(critic_user_msg).msg.content
            print_text_animated(
                Fore.CYAN
                + "=" * 10
                + "CRITIC RES"
                + "=" * 10
                + "\n"
                + prompt
                + Fore.RESET
            )

            artist_user_msg = BaseMessage.make_user_message(
                role_name="User",
                content='''Please generate a image based on
                the following prompt: \n'''
                + prompt,
            )
            response = self.artist.step(artist_user_msg)

            matches = re.findall(pattern, response.msg.content)
            image_paths = [f"./img/{uuid}{ext}" for uuid, ext in matches]
            tmp_paths = deepcopy(image_paths)
            response_msg = re.sub(
                pattern,
                lambda x, image_paths=image_paths: "("
                + image_paths.pop(0)
                + ")",
                response.msg.content,
            )
            image_paths = deepcopy(tmp_paths)
            print_text_animated(
                Fore.BLUE
                + "=" * 10
                + "ARTIST RES"
                + "=" * 10
                + "\n"
                + response_msg
            )
            print(response_msg)

        return response


if __name__ == "__main__":
    session = MMChat()
    res = session.step(
        initialPrompt='''Create an image with pink background,
        a dog is showing a sign with 'I Love Camel'.''',
        iter_num=1,
    )



--------------------------------------------------------------------------------
# File: vision\object_recognition.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import argparse

from PIL import Image

from camel.agents import ChatAgent
from camel.generators import PromptTemplateGenerator
from camel.messages import BaseMessage
from camel.models import ModelFactory
from camel.types import (
    ModelPlatformType,
    ModelType,
    RoleType,
    TaskType,
)

parser = argparse.ArgumentParser(description="Arguments for object detection.")
parser.add_argument(
    "--image_paths",
    metavar='N',
    type=str,
    nargs='+',
    help="Path to the images for object detection.",
    default=None,
    required=True,
)


def detect_image_obj(image_paths: str) -> None:
    sys_msg = PromptTemplateGenerator().get_prompt_from_key(
        TaskType.OBJECT_RECOGNITION, RoleType.ASSISTANT
    )
    print("=" * 20 + " SYS MSG " + "=" * 20)
    print(sys_msg)
    print("=" * 49)

    model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
    )
    agent = ChatAgent(
        sys_msg,
        model=model,
    )
    image_list = [Image.open(image_path) for image_path in image_paths]

    user_msg = BaseMessage.make_user_message(
        role_name="User",
        content="Please start the object detection for following image!",
        image_list=image_list,
        image_detail="high",
    )
    assistant_response = agent.step(user_msg)
    print("=" * 20 + " RESULT " + "=" * 20)
    print(assistant_response.msgs[0].content)
    print("=" * 48)


def main(args: argparse.Namespace) -> None:
    detect_image_obj(args.image_paths)


if __name__ == "__main__":
    args = parser.parse_args()
    main(args=args)



--------------------------------------------------------------------------------
# File: vision\video_description.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents import ChatAgent
from camel.messages import BaseMessage
from camel.models import ModelFactory
from camel.prompts.prompt_templates import PromptTemplateGenerator
from camel.types import ModelPlatformType, ModelType
from camel.types.enums import RoleType, TaskType

# Define system message
sys_msg_prompt = PromptTemplateGenerator().get_prompt_from_key(
    TaskType.VIDEO_DESCRIPTION, RoleType.ASSISTANT
)

model = ModelFactory.create(
    model_platform=ModelPlatformType.DEFAULT,
    model_type=ModelType.DEFAULT,
)

# Set agent
camel_agent = ChatAgent(sys_msg_prompt, model=model)

# The video from YouTube can be found at the following link:
# https://www.youtube.com/watch?v=kQ_7GtE529M
video_path = "bison.mp4"
with open(video_path, "rb") as video_file:
    video_bytes = video_file.read()
user_msg = BaseMessage.make_user_message(
    role_name="User",
    content="These are frames from a video that I want to upload. Generate a"
    "compelling description that I can upload along with the video.",
    video_bytes=video_bytes,
)

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
"""
===============================================================================
Title: "Survival in the Snow: A Bison's Battle Against Wolves" 
Description:
Witness the raw power of nature in this gripping video showcasing a dramatic 
encounter between a lone bison and a pack of wolves in a snowy wilderness. As 
the harsh winter blankets the landscape, the struggle for survival 
intensifies. Watch as the bison, isolated from its herd, faces the relentless
pursuit of hungry wolves. The tension escalates as the wolves coordinate 
their attack, attempting to overcome the bison with their numbers and 
strategic movements. Experience the breathtaking and brutal moments of this 
wildlife interaction, where every second is a fight for survival. This video 
captures the fierce beauty and the stark realities of life in the wild. Join 
us in observing these incredible animals and the instinctual battles that 
unfold in the heart of winter's grasp.
===============================================================================
"""



--------------------------------------------------------------------------------
# File: vision\web_video_description_extractor.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
from camel.agents import ChatAgent
from camel.messages import BaseMessage
from camel.prompts import PromptTemplateGenerator
from camel.toolkits import VideoDownloaderToolkit
from camel.types import RoleType, TaskType

video_url = (
    "https://sample-videos.com/video321/mp4/720/big_buck_bunny_720p_1mb.mp4"
)
downloader = VideoDownloaderToolkit()

# Get the video bytes
video_bytes = downloader.get_video_bytes(video_url)

sys_msg = PromptTemplateGenerator().get_prompt_from_key(
    TaskType.VIDEO_DESCRIPTION, RoleType.ASSISTANT
)

camel_agent = ChatAgent(sys_msg)

# Create user message with video bytes
user_msg = BaseMessage.make_user_message(
    role_name="User",
    content="These are frames from a video that I want to upload. Generate a"
    " compelling description that I can upload along with the video.",
    video_bytes=video_bytes,
)

# Get response information
response = camel_agent.step(user_msg)
print(response.msgs[0].content)
"""
===============================================================================
Join the delightful adventure of a lovable, chubby bunny as he emerges from
 his cozy burrow to greet the day! Watch as he stretches and yawns, ready to
explore the vibrant, lush world around him. This heartwarming and beautifully 
animated scene is sure to bring a smile to your face and brighten your day. 
Don't miss out on this charming moment of pure joy and wonder! 🌿🐰✨
===============================================================================
"""



--------------------------------------------------------------------------------
# File: vision\web_video_object_recognition.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import argparse

from camel.agents import ChatAgent
from camel.generators import PromptTemplateGenerator
from camel.messages import BaseMessage
from camel.toolkits.video_toolkit import VideoDownloaderToolkit
from camel.types import (
    RoleType,
    TaskType,
)

parser = argparse.ArgumentParser(description="Arguments for object detection.")
parser.add_argument(
    "--video_url",
    type=str,
    help="URL of the video for screenshot extraction.",
    required=True,
)
parser.add_argument(
    "--timestamps",
    type=int,
    help="Number of screenshots to capture.",
    default=3,
)


def detect_image_obj(image_list) -> None:
    sys_msg = PromptTemplateGenerator().get_prompt_from_key(
        TaskType.OBJECT_RECOGNITION, RoleType.ASSISTANT
    )
    print("=" * 20 + " SYS MSG " + "=" * 20)
    print(sys_msg)
    print("=" * 49)

    agent = ChatAgent(sys_msg)

    user_msg = BaseMessage.make_user_message(
        role_name="User",
        content="Please start the object detection for the following images!",
        image_list=image_list,
        image_detail="high",
    )
    assistant_response = agent.step(user_msg)
    print("=" * 20 + " RESULT " + "=" * 20)
    print(assistant_response.msgs[0].content)
    print("=" * 48)


def main() -> None:
    video_url = 'https://sample-videos.com/video321/mp4/720/big_buck_bunny_720p_1mb.mp4'
    downloader = VideoDownloaderToolkit()

    image_list = downloader.get_video_screenshots(video_url, 3)

    detect_image_obj(image_list)


if __name__ == "__main__":
    main()
"""
===============================================================================
==================== SYS MSG ====================
You have been assigned an object recognition task.
Your mission is to list all detected objects in following image.
Your output should always be a list of strings starting with `1.`, `2.` etc.
Do not explain yourself or output anything else.
=================================================
==================== RESULT ====================
1. Rabbit
2. Grass
3. Rocks
4. Tree roots
5. Background trees
6. Hill
7. Sky
8. Stone structure
================================================
===============================================================================
"""



--------------------------------------------------------------------------------
# File: workforce\hackathon_judges.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
import textwrap

from camel.agents import ChatAgent
from camel.messages import BaseMessage
from camel.models import ModelFactory
from camel.societies.workforce import Workforce
from camel.tasks import Task
from camel.toolkits import FunctionTool, SearchToolkit
from camel.types import ModelPlatformType, ModelType


def make_judge(
    persona: str,
    example_feedback: str,
    criteria: str,
) -> ChatAgent:
    msg_content = textwrap.dedent(
        f"""\
        You are a judge in a hackathon.
        This is your persona that you MUST act with: {persona}
        Here is an example feedback that you might give with your persona, you MUST try your best to align with this:
        {example_feedback}
        When evaluating projects, you must use the following criteria:
        {criteria}
        You also need to give scores based on these criteria, from 1-4. The score given should be like 3/4, 2/4, etc.
        """  # noqa: E501
    )

    sys_msg = BaseMessage.make_assistant_message(
        role_name="Hackathon Judge",
        content=msg_content,
    )

    model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
    )

    agent = ChatAgent(
        system_message=sys_msg,
        model=model,
    )

    return agent


def main():
    proj_content = textwrap.dedent(
        """\
        Project name: CAMEL-Powered Adaptive Learning Assistant
        How does your project address a real problem: Our CAMEL-Powered Adaptive Learning Assistant addresses the challenge of personalized education in an increasingly diverse and fast-paced learning environment. Traditional one-size-fits-all approaches to education often fail to meet the unique needs of individual learners, leading to gaps in understanding and reduced engagement. Our project leverages CAMEL-AI's advanced capabilities to create a highly adaptive, intelligent tutoring system that can understand and respond to each student's learning style, pace, and knowledge gaps in real-time.
        Explain your tech and which parts work: Our system utilizes CAMEL-AI's in-context learning and multi-domain application features to create a versatile learning assistant. The core components include:
        1. Learner Profile Analysis: Uses natural language processing to assess the student's current knowledge, learning preferences, and goals.
        2. Dynamic Content Generation: Leverages CAMEL-AI to create personalized learning materials, explanations, and practice questions tailored to each student's needs.
        3. Adaptive Feedback Loop: Continuously analyzes student responses and adjusts the difficulty and style of content in real-time.
        4. Multi-Modal Integration: Incorporates text, images, and interactive elements to cater to different learning styles.
        5. Progress Tracking: Provides detailed insights into the student's learning journey, identifying strengths and areas for improvement.
        Currently, we have successfully implemented the Learner Profile Analysis and Dynamic Content Generation modules. The Adaptive Feedback Loop is partially functional, while the Multi-Modal Integration and Progress Tracking features are still in development.
        """  # noqa: E501
    )

    search_toolkit = SearchToolkit()
    search_tools = [
        FunctionTool(search_toolkit.search_google),
        FunctionTool(search_toolkit.search_duckduckgo),
    ]

    researcher_model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
    )

    researcher_agent = ChatAgent(
        system_message=BaseMessage.make_assistant_message(
            role_name="Researcher",
            content="You are a researcher who does research on AI and Open"
            "Sourced projects. You use web search to stay updated on the "
            "latest innovations and trends.",
        ),
        model=researcher_model,
        tools=search_tools,
    )

    vc_persona = (
        'You are a venture capitalist who is obsessed with how projects can '
        'be scaled into "unicorn" companies. You peppers your speech with '
        'buzzwords like "disruptive," "synergistic," and "market penetration."'
        ' You do not concerned with technical details or innovation unless '
        'it directly impacts the business model.'
    )

    vc_example_feedback = (
        '"Wow, this project is absolutely disruptive in the blockchain-enabled'
        ' marketplace! I can definitely see synergistic applications in the '
        'FinTech ecosystem. The scalability is through the roof--this is '
        'revolutionary!'
    )

    vc_criteria = textwrap.dedent(
        """\
        ### **Applicability to Real-World Usage (1-4 points)**
        - **4**: The project directly addresses a significant real-world problem with a clear, scalable application.
        - **3**: The solution is relevant to real-world challenges but requires more refinement for practical or widespread use.
        - **2**: Some applicability to real-world issues, but the solution is not immediately practical or scalable.
        - **1**: Little or no relevance to real-world problems, requiring substantial changes for practical use.
        """  # noqa: E501
    )

    vc_agent = make_judge(
        vc_persona,
        vc_example_feedback,
        vc_criteria,
    )

    eng_persona = (
        'You are an experienced engineer and a perfectionist. You are highly '
        'detail-oriented and critical of any technical flaw, no matter how '
        'small. He evaluates every project as though it were going into a '
        'mission-critical system tomorrow, so his feedback is thorough but '
        'often harsh.'
    )

    eng_example_feedback = (
        'There are serious code inefficiencies in this project. The '
        'architecture is unstable, and the memory management is suboptimal. '
        'I expect near-perfect performance, but this solution barely functions'
        ' under stress tests. It has potential, but it is nowhere near '
        'deployment-ready.'
    )

    eng_criteria = textwrap.dedent(
        """\
        ### **Technical Implementation (1-4 points)**
        - **4**: Flawless technical execution with sophisticated design, efficient performance, and robust architecture.
        - **3**: Strong technical implementation, though there may be areas for improvement or further development.
        - **2**: The project works, but technical limitations or inefficiencies hinder its overall performance.
        - **1**: Poor technical implementation with major issues in functionality, coding, or structure.
        """  # noqa: E501
    )

    eng_agent = make_judge(
        eng_persona,
        eng_example_feedback,
        eng_criteria,
    )

    founder_persona = (
        'You are a well-known AI startup founder who is always looking for the'
        ' "next big thing" in AI. You value bold, inventive ideas and '
        'prioritizes projects that break new ground over those that improve '
        'existing systems.'
    )

    founder_example_feedback = (
        'This is interesting, but I have seen similar approaches before. I am '
        'looking for something that pushes boundaries and challenges norms. '
        'What is the most revolutionary part of this project? Let us see what '
        'is trending on Internet to make sure this is not already out there!'
    )

    founder_criteria = textwrap.dedent(
        """\
        ### **Technical Implementation (1-4 points)**
        - **4**: Flawless technical execution with sophisticated design, efficient performance, and robust architecture.
        - **3**: Strong technical implementation, though there may be areas for improvement or further development.
        - **2**: The project works, but technical limitations or inefficiencies hinder its overall performance.
        - **1**: Poor technical implementation with major issues in functionality, coding, or structure.
        """  # noqa: E501
    )

    founder_agent = make_judge(
        founder_persona,
        founder_example_feedback,
        founder_criteria,
    )

    contributor_persona = (
        'You are a contributor to the CAMEL-AI project and is always excited '
        'to see how people are using it. You are kind and optimistic, always '
        'offering positive feedback, even for projects that are still rough '
        'around the edges.'
    )

    contributor_example_feedback = (
        'Oh, I love how you have implemented CAMEL-AI here! The use of its '
        'adaptive learning capabilities is fantastic, and you have really '
        'leveraged the contextual reasoning in a great way! Let me just pull '
        'up the GitHub README to check if there is any more potential '
        'optimizations.'
    )

    contributor_criteria = textwrap.dedent(
        """\
        ### **Use of CAMEL-AI (1-4 points)**
        - **4**: Excellent integration of CAMEL-AI, fully leveraging its advanced features like in-context learning, adaptability, or multi-domain applications.
        - **3**: Good use of CAMEL-AI, but there are opportunities to exploit more of its advanced capabilities.
        - **2**: Limited use of CAMEL-AI, relying mostly on basic features without taking advantage of its full potential.
        - **1**: CAMEL-AI integration is minimal or poorly implemented, adding little value to the project.
        """  # noqa: E501
    )

    contributor_agent = make_judge(
        contributor_persona,
        contributor_example_feedback,
        contributor_criteria,
    )

    workforce = Workforce('Hackathon Judges')
    task = Task(
        content="Evaluate the hackathon project. First, do some research on "
        "the information related to the project, then each judge should give a"
        " score accordingly. Finally, list the opinions from each judge while"
        " preserving the judge's unique identity, along with the score and"
        " judge name, and also give a final summary of the opinions.",
        additional_info=proj_content,
        id="0",
    )

    workforce.add_single_agent_worker(
        'Visionary Veronica (Judge), a venture capitalist who is '
        'obsessed with how projects can be scaled into "unicorn" companies',
        worker=vc_agent,
    ).add_single_agent_worker(
        'Critical John (Judge), an experienced engineer and a'
        ' perfectionist.',
        worker=eng_agent,
    ).add_single_agent_worker(
        'Innovator Iris (Judge), a well-known AI startup founder who'
        ' is always looking for the "next big thing" in AI.',
        worker=founder_agent,
    ).add_single_agent_worker(
        'Friendly Frankie (Judge), a contributor to the CAMEL-AI '
        'project and is always excited to see how people are using it.',
        worker=contributor_agent,
    ).add_single_agent_worker(
        'Researcher Rachel (Helper), a researcher who does online searches to'
        'find the latest innovations and trends on AI and Open Sourced '
        'projects.',
        worker=researcher_agent,
    )

    task = workforce.process_task(task)
    print(task.result)


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: workforce\multiple_single_agents.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents.chat_agent import ChatAgent
from camel.messages.base import BaseMessage
from camel.models import ModelFactory
from camel.societies.workforce import Workforce
from camel.tasks.task import Task
from camel.toolkits import (
    FunctionTool,
    GoogleMapsToolkit,
    SearchToolkit,
    WeatherToolkit,
)
from camel.types import ModelPlatformType, ModelType


def main():
    search_toolkit = SearchToolkit()
    search_tools = [
        FunctionTool(search_toolkit.search_google),
        FunctionTool(search_toolkit.search_duckduckgo),
    ]

    # Set up web searching agent
    search_agent_model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
    )
    search_agent = ChatAgent(
        system_message=BaseMessage.make_assistant_message(
            role_name="Web searching agent",
            content="You can search online for information",
        ),
        model=search_agent_model,
        tools=[*search_tools, *WeatherToolkit().get_tools()],
    )

    # Set up tour guide agent
    tour_guide_agent_model = ModelFactory.create(
        model_platform=ModelPlatformType.DEFAULT,
        model_type=ModelType.DEFAULT,
    )

    tour_guide_agent = ChatAgent(
        BaseMessage.make_assistant_message(
            role_name="Tour guide",
            content="You are a tour guide",
        ),
        model=tour_guide_agent_model,
        tools=GoogleMapsToolkit().get_tools(),
    )

    # Set up traveler agent
    traveler_agent = ChatAgent(
        BaseMessage.make_assistant_message(
            role_name="Traveler",
            content="You can ask questions about your travel plans",
        ),
        model=ModelFactory.create(
            model_platform=ModelPlatformType.DEFAULT,
            model_type=ModelType.DEFAULT,
        ),
    )

    workforce = Workforce('A travel group')

    workforce.add_single_agent_worker(
        "A tour guide",
        worker=tour_guide_agent,
    ).add_single_agent_worker(
        "A traveler", worker=traveler_agent
    ).add_single_agent_worker(
        "An agent who can do online searches", worker=search_agent
    )

    # specify the task to be solved
    human_task = Task(
        content=(
            "Plan a one-week trip to Paris, considering some historical places"
            " to visit and weather conditions."
        ),
        id='0',
    )

    task = workforce.process_task(human_task)

    print('Final Result of Original task:\n', task.result)


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# File: workforce\role_playing_with_agents.py
--------------------------------------------------------------------------------

# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========

from camel.agents.chat_agent import ChatAgent
from camel.messages.base import BaseMessage
from camel.models import ModelFactory
from camel.societies.workforce import Workforce
from camel.tasks.task import Task
from camel.toolkits import GoogleMapsToolkit, SearchToolkit, WeatherToolkit
from camel.types import ModelPlatformType, ModelType


def main():
    guide_sysmsg = BaseMessage.make_assistant_message(
        role_name="tour guide",
        content="You have to lead everyone to have fun",
    )

    planner_sysmsg = BaseMessage.make_assistant_message(
        role_name="planner",
        content="good at tour plan.",
    )

    guide_agent = ChatAgent(guide_sysmsg)
    planner_agent = ChatAgent(planner_sysmsg)

    function_list = [
        *SearchToolkit().get_tools(),
        *WeatherToolkit().get_tools(),
        *GoogleMapsToolkit().get_tools(),
    ]

    model_platform = ModelPlatformType.DEFAULT
    model_type = ModelType.DEFAULT
    assistant_role_name = "Searcher"
    user_role_name = "Professor"
    assistant_agent_kwargs = dict(
        model=ModelFactory.create(
            model_platform=model_platform,
            model_type=model_type,
        ),
        tools=function_list,
    )
    user_agent_kwargs = dict(
        model=ModelFactory.create(
            model_platform=model_platform,
            model_type=model_type,
        ),
    )

    workforce = Workforce('a travel group')
    workforce.add_role_playing_worker(
        'research Group',
        assistant_role_name,
        user_role_name,
        assistant_agent_kwargs,
        user_agent_kwargs,
        1,
    ).add_single_agent_worker(
        'tour guide', guide_agent
    ).add_single_agent_worker('planner', planner_agent)

    human_task = Task(
        content="research history of Paris and plan a tour.",
        id='0',
    )
    task = workforce.process_task(human_task)

    print('Final result of original task:\n', task.result)


if __name__ == "__main__":
    main()


